













































Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics


Proceedings of the ACL 2007 Demo and Poster Sessions, pages 1–4,
Prague, June 2007. c©2007 Association for Computational Linguistics

MIMUS: A Multimodal and Multilingual Dialogue System for the Home
Domain

J. Gabriel Amores
Julietta Research Group
Universidad de Sevilla
jgabriel@us.es

Guillermo Pérez
Julietta Research Group
Universidad de Sevilla
gperez@us.es

Pilar Manchón
Julietta Research Group
Universidad de Sevilla
pmanchon@us.es

Abstract

This paper describes MIMUS, a multimodal
and multilingual dialogue system for the in–
home scenario, which allows users to con-
trol some home devices by voice and/or
clicks. Its design relies on Wizard of Oz ex-
periments and is targeted at disabled users.
MIMUS follows the Information State Up-
date approach to dialogue management, and
supports English, German and Spanish, with
the possibility of changing language on–the–
fly. MIMUS includes a gestures–enabled
talking head which endows the system with
a human–like personality.

1 Introduction

This paper describes MIMUS, a multimodal and
multilingual dialogue system for the in–home sce-
nario, which allows users to control some home de-
vices by voice and/or clicks. The architecture of
MIMUS was first described in (Pérez et al., 2006c).
This work updates the description and includes a
life demo. MIMUS follows the Information State
Update approach to dialogue management, and has
been developed under the EU–funded TALK project
(Talk Project, 2004). Its architecture consists of a
set of OAA agents (Cheyer and Martin, 1972) linked
through a central Facilitator, as shown in figure 1:

The main agents in MIMUS are briefly described
hereafter:

• The system core is the Dialogue Manager,
which processes the information coming from
the different input modality agents by means of
a natural language understanding module and
provides output in the appropriate modality.

• The main input modality agent is the ASR
Manager, which is obtained through an OAA

Figure 1: MIMUS Architecture

wrapper for Nuance. Currently, the system sup-
ports English, Spanish and German, with the
possibility of changing languages on–the–fly
without affecting the dialogue history.

• The HomeSetup agent displays the house lay-
out, with all the devices and their state. When-
ever a device changes its state, the HomeSetup
is notified and the graphical layout is updated.

• The Device Manager controls the physical de-
vices. When a command is sent, the Device
Manager notifies it to the HomeSetup and the
Knowledge Manager, guaranteeing coherence
in all the elements in MIMUS.

• The GUI Agents control each of the device–
specific GUIs. Thus, clicking on the telephone
icon, a telephone GUI will be displayed, and so
on for each type of service.

• The Knowledge Manager connects all the
agents to the common knowledge resource by

1



means of an OWL Ontology.

• The Talking Head. MIMUS virtual charac-
ter is synchronized with Loquendo’s TTS, and
has the ability to express emotions and play
some animations such as nodding or shaking
the head.

2 WoZ Experiments

MIMUS has been developed taking into account
wheel–chair bound users. In order to collect first–
hand information about the users’ natural behavior
in this scenario, several WoZ experiments were first
conducted. A rather sophisticated multilingual WoZ
experimental platform was built for this purpose.

The set of WoZ experiments conducted was de-
signed in order to collect data. In turn, these
data helped determine the relevant factors to con-
figure multimodal dialogue systems in general, and
MIMUS in particular.

A detailed description of the results obtained after
the analysis of the experiments and their impact on
the overall design of the system may be found in
(Manchón et al., 2007).

3 ISU–based Dialogue Management in
MIMUS

As pointed out above, MIMUS follows the ISU
approach to dialogue management (Larsson and
Traum, 2000). The main element of the ISU ap-
proach in MIMUS is the dialogue history, repre-
sented formally as a list of dialogue states. Dia-
logue rules update this information structure either
by producing new dialogue states or by supplying
arguments to existing ones.

3.1 Multimodal DTAC structure

The information state in MIMUS is represented as a
feature structure with four main attributes: Dialogue
Move, Type, Arguments and Contents.

• DMOVE: Identifies the kind of dialogue move.

• TYPE: This feature identifies the specific dia-
logue move in the particular domain at hand.

• ARGS: The ARGS feature specifies the argu-
ment structure of the DMOVE/TYPE pair.

Modality and Time features have been added in
order to implement fusion strategies at dialogue
level.

3.2 Updating the Information State in MIMUS

This section provides an example of how the In-
formation State Update approach is implemented
in MIMUS. Update rules are triggered by dialogue
moves (any dialogue move whose DTAC structure
unifies with the Attribute–Value pairs defined in the
TriggeringCondition field) and may require addi-
tional information, defined as dialogue expectations
(again, those dialogue moves whose DTAC structure
unify with the Attribute–Value pairs defined in the
DeclareExpectations field).

Consider the following DTAC, which represents
the information state returned by the NLU module
for the sentence switch on:



DMOVE specifyCommand
TYPE SwitchOn

ARGS
[

Location, DeviceType
]

META INFO



MODALITY VOICE

TIME INIT 00:00:00

TIME END 00:00:30

CONFIDENCE 700







Consider now the (simplified) dialogue rule
“ON”, defined as follows:

RuleID: ON;
TriggeringCondition:
(DMOVE:specifyCommand,
TYPE:SwitchOn);

DeclareExpectations: {
Location,
DeviceType }

ActionsExpectations: {
[DeviceType] =>

{NLG(DeviceType);} }
PostActions: {

ExecuteAction(@is-ON); }

The DTAC obtained for switch on triggers the
dialogue rule ON. However, since two declared
expectations are still missing (Location and De-
viceType), the dialogue manager will activate the
ActionExpectations and prompt the user for the
kind of device she wants to switch on, by means
of a call to the natural language generation mod-
ule NLG(DeviceType). Once all expectations have

2



been fulfilled, the PostActions can be executed over
the desired device(s).

4 Integrating OWL in MIMUS

Initially, OWL Ontologies were integrated in
MIMUS in order to improve its knowledge manage-
ment module. This functionality implied the imple-
mentation of a new OAA wrapper capable of query-
ing OWL ontologies, see (Pérez et al., 2006b) for
details.

4.1 From Ontologies to Grammars: OWL2Gra

OWL ontologies play a central role in MIMUS. This
role is limited, though, to the input side of the sys-
tem. The domain–dependent part of multimodal and
multilingual production rules for context–free gram-
mars is semi–automatically generated from an OWL
ontology.

This approach has achieved several goals: it lever-
ages the manual work of the linguist, and ensures
coherence and completeness between the Domain
Knowledge (Knowledge Manager Module) and the
Linguistic Knowledge (Natural Language Under-
standing Module) in the application. A detailed ex-
planation of the algorithm and the results obtained
can be found in (Pérez et al., 2006a)

4.2 From OWL to the House Layout

MIMUS home layout does not consist of a pre–
defined static structure only usable for demonstra-
tion purposes. Instead, it is dynamically loaded at
execution time from the OWL ontology where all
the domain knowledge is stored, assuring the coher-
ence of the layout with the rest of the system.

This is achieved by means of an OWL–RDQL
wrapper. It is through this agent that the Home Setup
enquires for the location of the walls, the label of the
rooms, the location and type of devices per room and
so forth, building the 3D graphical image from these
data.

5 Multimodal Fusion Strategies

MIMUS approach to multimodal fusion involves
combining inputs coming from different multimodal
channels at dialogue level (Pérez et al., 2005). The
idea is to check the multimodal input pool before
launching the actions expectations while waiting for

an “inter–modality” time. This strategy assumes
that each individual input can be considered as an
independent dialogue move. In this approach, the
multimodal input pool receives and stores all in-
puts including information such as time and modal-
ity. The Dialogue Manager checks the input pool
regularly to retrieve the corresponding input. If
more than one input is received during a certain time
frame, they are considered simultaneous or pseudo–
simultaneous. In this case, further analysis is needed
in order to determine whether those independent
multimodal inputs are truly related or not. Another,
improved strategy has been proposed at (Manchón
et al., 2006), which combines the advantages of this
one, and those proposed for unification–based gram-
mars (Johnston et al., 1997; Johnston, 1998).

6 Multimodal Presentation in MIMUS

MIMUS offers graphical and voice output to the
users through an elaborate architecture composed of
a TTS Manager, a HomeSetup and GUI agents. The
multimodal presentation architecture in MIMUS
consists of three sequential modules. The current
version is a simple implementation that may be ex-
tended to allow for more complex theoretical issues
hereby proposed. The main three modules are:

• Content Planner (CP): This module decides
on the information to be provided to the user.
As pointed out by (Wahlster et al., 1993), the
CP cannot determine the content independently
from the presentation planner (PP). In MIMUS,
the CP generates a set of possibilities, from
which the PP will select one, depending on
their feasibility.

• Presentation Planner (PP): The PP receives the
set of possible content representations and se-
lects the “best” one.

• Realization Module (RM): This module takes
the presentation generated and selected by
the CP–PP, divides the final DTAC structure
and sends each substructure to the appropriate
agent for rendering.

7 The MIMUS Talking Head

MIMUS virtual character is known as Ambrosio.
Endowing the character with a name results in per-

3



sonalization, personification, and voice activation.
Ambrosio will remain inactive until called for duty
(voice activation); each user may name their per-
sonal assistant as they wish (Personalization); and
they will address the system at personal level, re-
inforcing the sense of human–like communication
(Personification). The virtual head has been imple-
mented in 3D to allow for more natural and realis-
tic gestures and movements. The graphical engine
used is OGRE (OGRE, 2006), a powerful, free and
easy to use tool. The current talking head is inte-
grated with Loquendo, a high quality commercial
synthesizer that launches the information about the
phonemes as asynchronous events, which allows for
lip synchronization. The dialogue manager controls
the talking head, and sends the appropriate com-
mands depending of the dialogue needs. Through-
out the dialogue, the dialogue manager may see it
fit to reinforce the communication channel with ges-
tures and expressions, which may or may not imply
synthesized utterances. For instance, the head may
just nod to acknowledge a command, without utter-
ing words.

8 Conclusions and Future Work

In this paper, an overall description of the MIMUS
system has been provided.

MIMUS is a fully multimodal and multilingual di-
alogue system within the Information State Update
approach. A number of theoretical and practical is-
sues have been addressed successfully, resulting in a
user–friendly, collaborative and humanized system.

We concluded from the experiments that a
human–like talking head would have a significant
positive impact on the subjects’ perception and will-
ingness to use the system.

Although no formal evaluation of the system has
taken place, MIMUS has already been presented
successfully in different forums, and as expected,
“Ambrosio” has always made quite an impression,
making the system more appealing to use and ap-
proachable.

References
Adam Cheyer and David Martin. 2001. The open

agent architecture. Journal of Autonomous Agents and
Multi–Agent Systems, 4(12):143–148.

Michael Johnston, Philip R. Cohen, David McGee,
Sharon L. Oviatt, James A. Pitman and Ira A. Smith.
1997. Unification–based Multimodal Integration ACL
281–288.

Michael Johnston. 1998. Unification–based Multimodal
Parsing Coling–ACL 624–630.

Staffan Larsson and David Traum. 2000. Information
State and dialogue management in the TRINDI Dia-
logue Move Engine Toolkit. Natural Language Engi-
neering, 6(34): 323-340.

Pilar Manchón, Guillermo Pérez and Gabriel Amores.
2006. Multimodal Fusion: A New Hybrid Strategy
for Dialogue Systems. Proceedings of International
Congress of Multimodal Interfaces (ICMI06), 357–
363. ACM, New York, USA.

Pilar Manchón, Carmen Del Solar, Gabriel Amores and
Guillermo Pérez. 2007. Multimodal Event Analysis
in the MIMUS Corpus. Multimodal Corpora: Special
Issue of the International Journal JLRE, submitted.

OGRE. 2006. Open Source Graphics Engine.
www.ogre3d.org

Guillermo Pérez, Gabriel Amores and Pilar Manchón.
2005. Two Strategies for multimodal fusion. E.V.
Zudilova–Sainstra and T. Adriaansen (eds.) Proceed-
ings of Multimodal Interaction for the Visualization
and Exploration of Scientific Data, 26–32. Trento,
Italy.

Guillermo Pérez, Gabriel Amores, Pilar Manchón and
David González Maline. 2006. Generating Multilin-
gual Grammars from OWL Ontologies. Research in
Computing Science, 18:3–14.

Guillermo Pérez, Gabriel Amores, Pilar Manchón, Fer-
nando Gómez and Jesús González. 2006. Integrating
OWL Ontologies with a Dialogue Manager. Proce-
samiento del Lenguaje Natural 37:153–160.

Guillermo Pérez, Gabriel Amores and Pilar Manchón.
2006. A Multimodal Architecture For Home Con-
trol By Disabled Users. Proceedings of the IEEE/ACL
2006 Workshop on Spoken Language Technology,
134–137. IEEE, New York, USA.

Talk Project. Talk and Look: Linguistic Tools for Am-
bient Linguistic Knowledge. 2004. 6th Framework
Programme. www.talk-project.org

Wolfgang Wahlster, Elisabeth André, Wolfgang Finkler,
Hans–Jürgen Profitlich and Thomas Rist. 1993. Plan–
Based integration of natural language and graphics
generation. Artificial intelligence, 63:287–247.

4


