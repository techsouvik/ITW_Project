













































Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics


Proceedings of the ACL 2007 Demo and Poster Sessions, pages 65–68,
Prague, June 2007. c©2007 Association for Computational Linguistics

An Approximate Approach for Training Polynomial Kernel SVMs in 
Linear Time 

Yu-Chieh Wu Jie-Chi Yang Yue-Shi Lee 
Dept. of Computer Science and 

Information Engineering 
Graduate Institute of Net-

work Learning Technology
Dept. of Computer Science and 

Information Engineering 
National Central University National Central University Ming Chuan University 

Taoyuan, Taiwan Taoyuan, Taiwan Taoyuan, Taiwan 
bcbb@db.csie.ncu.edu.tw yang@cl.ncu.edu.tw lees@mcu.edu.tw 

 

Abstract 

Kernel methods such as support vector ma-
chines (SVMs) have attracted a great deal 
of popularity in the machine learning and 
natural language processing (NLP) com-
munities. Polynomial kernel SVMs showed 
very competitive accuracy in many NLP 
problems, like part-of-speech tagging and 
chunking. However, these methods are 
usually too inefficient to be applied to large 
dataset and real time purpose. In this paper, 
we propose an approximate method to 
analogy polynomial kernel with efficient 
data mining approaches. To prevent expo-
nential-scaled testing time complexity, we 
also present a new method for speeding up 
SVM classifying which does independent 
to the polynomial degree d. The experi-
mental results showed that our method is 
16.94 and 450 times faster than traditional 
polynomial kernel in terms of training and 
testing respectively. 

1 Introduction 
Kernel methods, for example support vector 
machines (SVM) (Vapnik, 1995) are successfully 
applied to many natural language processing (NLP) 
problems. They yielded very competitive and 
satisfactory performance in many classification 
tasks, such as part-of-speech (POS) tagging 
(Gimenez and Marquez, 2003), shallow parsing 
(Kudo and Matsumoto, 2001, 2004; Lee and Wu, 
2007), named entity recognition (Isozaki and 
Kazawa, 2002), and parsing (Nivre et al., 2006). 

In particular, the use of polynomial kernel SVM 
implicitly takes the feature combinations into ac-

count instead of explicitly combines features. By 
setting with polynomial kernel degree (i.e., d), dif-
ferent number of feature conjunctions can be im-
plicitly computed. In this way, polynomial kernel 
SVM is often better than linear kernel which did 
not use feature conjunctions. However, the training 
and testing time costs for polynomial kernel SVM 
is far slow than the linear kernel. For example, it 
took one day to train the CoNLL-2000 task with 
polynomial kernel SVM, while the testing speed is 
merely 20-30 words per second (Kudo and Ma-
tsumoto, 2001). Although the author provided the 
solution for fast classifying with polynomial kernel 
(Kudo and Matsumoto, 2004), the training time is 
still inefficient. Nevertheless, the testing time of 
their method exponentially scales with polynomial 
kernel degree d, i.e., O(|X|d) where |X| denotes as 
the length of example X.  

On the contrary, even the linear kernel SVM 
simply disregards the effect of feature combina-
tions during training and testing, it performs not 
only more efficient than polynomial kernel, but 
also can be improved through directly appending 
features derived from the set of feature combina-
tions. Examples include bigram, trigram, etc. Nev-
ertheless, selecting the feature conjunctions was 
manually and heuristically encoded and should 
perform amount of validation trials to discover 
which is useful or not. In recent years, several 
studies had reported that the training time of linear 
kernel SVM can be reduced to linear time 
(Joachims, 2006; Keerthi and DeCoste, 2005). But 
they did not and difficult to be extent to polyno-
mial kernels.  

In this paper, we propose an approximate ap-
proach to extend the linear kernel SVM toward 
polynomial. By introducing the well-known se-
quential pattern mining approach (Pei et al., 2004), 

65



frequent feature conjunctions, namely patterns 
could be discovered and also kept as expand fea-
ture space. We then adopt the mined patterns to re-
represent the training/testing examples. Subse-
quently, we use the off-the-shelf linear kernel 
SVM algorithm to perform training and testing. 
Besides, to exponential-scaled testing time com-
plexity, we propose a new classification method 
for speeding up the SVM testing. Rather than 
enumerating all patterns for each example, our 
method requires O(Favg*Navg) which is independent 
to the polynomial kernel degree. Favg is the average 
number of frequent features per example, while the 
Navg is the average number of patterns per feature.  

2 SVM and Kernel Methods 
Suppose we have the training instance set for bi-
nary classification problem: 

}1 ,1{ ,  ),,(),...,,(),,( 2211 −+∈ℜ∈ iDinn yxyxyxyx  
where xi is a feature vector in D-dimension 

space of the i-th example, and yi is the label of xi 
either positive or negative. The training of SVMs 
involves in minimize the following object (primal 
form, soft-margin) (Vapnik, 1995): 

∑
=

+⋅=
n

i
ii yxWLossCWWW

1
),(

2
1

)(  :minimize α             (1) 

The loss function indicates the loss of training 
error. Usually, the hinge-loss is used (Keerthi and 
DeCoste, 2005). The factor C in (1) is a parameter 
that allows one to trade off training error and mar-
gin. A small value for C will increase the number 
of training errors. 

To determine the class (+1 or -1) of an example 
x can be judged by computing the following equa-
tion. 

))),(((sign)( ∑
∈

+=
SVsx

iii
i

bxxKyxy α
                              (2) 

αi is the weight of training example xi (αi>0), 
and b denotes as a threshold. Here the xi should be 
the support vectors (SVs), and are representative of 
training examples. The kernel function K is the 
kernel mapping function, which might map from 

Dℜ  to 'Dℜ  (usually D<<D’). The natural linear ker-
nel simply uses the dot-product as (3). 

),(),( ii xxdotxxK =                                             (3) 
A polynomial kernel of degree d is given by (4). 

d
ii xxdotxxK )),(1(),( +=                                      (4) 

One can design or employ off-the-shelf kernel 
types for particular applications. In particular to the 

use of polynomial kernel-based SVM, it was 
shown to be the most successful kernels for many 
natural language processing (NLP) problems 
(Kudo and Matsumoto, 2001; Isozaki and Kazawa, 
2002; Nivre et al., 2006).  

It is known that the dot-product (linear form) 
represents the most efficient kernel computing 
which can produce the output value by linearly 
combining all support vectors such as 

∑
∈

=+=
SVsx

iii
i

xywbwxdotxy α  ere        wh)),((sign)(
     (5) 

By combining (2) and (4), the determination of 
an example of x using the polynomial kernel can 
be shown as follows. 

)))1),((((sign)( bxxdotyxy di
SVsx

ii
i

++= ∑
∈

α
                   (6) 

Usually, degree d is set more than 1. When d is 
set as 1, the polynomial kernel backs-off to linear 
kernel. Although the effectiveness of polynomial 
kernel, it can not be shown to linearly combine all 
support vectors into one weight vector whereas it 
requires computing the kernel function (4) for each 
support vector xi. The situation is even worse when 
the number of support vectors become huge (Kudo 
and Matsumoto, 2004). Therefore, whether in 
training or testing phrase, the cost of kernel com-
putations is far more expensive than linear kernel. 

3 Approximate Polynomial Kernel 
In 2004, Kudo and Matsumoto (2004) derived both 
implicitly (6) and explicitly form of polynomial 
kernel. They indicated that the use of explicitly 
enumerate the feature combinations is equivalent 
to the polynomial kernel (see Lemma 1 and Exam-
ple 1, Kudo and Matsumoto, 2004) which shared 
the same view of (Cumby and Roth, 2003).  

We follow the similar idea of the above studies 
that requires explicitly enumerated all feature com-
binations. To meet with our problem, we employ 
the well-known sequential pattern mining algo-
rithm, namely PrefixSpan (Pei et al., 2004) to effi-
cient mine the frequent patterns. However, directly 
adopt the algorithm is not a good idea. To fit with 
SVM, we modify the original PrefixSpan algo-
rithm according to the following constraints.  

Given a set features, the PrefixSpan mines the 
frequent patterns which occurs more than prede-
fined minimum support in the training set and lim-
ited in the length of predefined d, which is equiva-
lent to the polynomial kernel degree d. For exam-

66



ple, if the minimum support is 5, and d=2, then a 
feature combination (fi, fj) must appear more than 5 
times in set of x.  
Definition 1 (Frequent single-item sequence):  
Given a set of feature vectors x, minimum support, 
and d, mining the frequent patterns (feature combi-
nations) is to mine the patterns in the single-item 
sequence database. 
Lemma 2 (Ordered feature vector):  
For each example, the feature vector could be 
transformed into an ordered item (feature) list, i.e., 
f1<f2<…<fmax where fmax is the highest dimension of 
the example. 
Proof. It is very easy to sort an unordered feature 
vector into the ordered list with conventional sort-
ing algorithm. 
Definition 3 (Uniqueness of the features per ex-
ample):  
Given the set of mined patterns, for any feature fi, 
it is impossible to appear more than once in the 
same pattern. 

Different from conventional sequential pattern 
mining method, in feature combination mining for 
SVM only contains a set of feature vectors each of 
which is independently treated. In other words, no 
compound features in the vector. If it exists, one 
can simply expand the compound features as an-
other new feature. 

By means of the above constraints, mining the 
frequent patterns can be reduced to mining the lim-
ited length of frequent patterns in the single-item 
database (set of ordered vectors). Furthermore, 
during each phase, we need only focus on finding 
the “frequent single features” to expand previous 
phase. More detail implementation issues can refer 
(Pei et al., 2004). 

3.1 Speed-up Testing 
To efficiently expand new features for the original 
feature vectors, we propose a new method to fast 
discovery patterns. Essentially, the PrefixSpan al-
gorithm gradually expands one item from previous 
result which can be viewed as a tree growing. An 
example can be found in Figure 1.  

Each node in Figure 1 is the associate feature of 
root. The whole patterns expanded by fj can be rep-
resented as the path from root to each node. For 
example, pattern (fj, fk, fm, fr) can be found via trav-
ersing the tree starting from fj. In this way, we can 
re-expand the original feature vector via visiting 
corresponding trees for each feature.  

 
Figure 1: The tree representation of feature fj  

 
Table 1: Encoding frequent patterns with DFS array 

representation 
Level 0 1 2 3 2 1 2 1 2 2
Label Root k m r p m p o p q
Item fj fk fm fr fp fm fp fo fp fq

 
However, traversing arrays is much more effi-

cient than visiting trees. Therefore, we adopt the l2-
sequences encoding method based on the DFS 
(depth-first-search) sequence as (Wang et al., 2004) 
to represent the trees. An l2-sequence does not only 
store the label information but also take the node 
level into account. Examples can be found in Table 
1.  

 
Theorem 4 (Uniqueness of l2-sequence): Given 
trees T1, and T2, their l2-sequences are identical if 
and only if T1 and T2 are isomorphic, i.e., there 
exists a one-to-one mapping for set of nodes, node 
labels, edges, and root nodes. 
Proof. see theorem 1 in (Wang et al., 2004). 
Definition 5 (Ascend-descend relation):  
Given a node k of feature fk in l2-sequence, all of 
the descendant of k that rooted by k have the 
greater feature numbers than fk.  
Definition 6 (Limited visiting space):  
Given the highest feature fmax of vector X, and fk 
rooted l2-sequence, if fmax<fk, then we can not find 
any pattern that prefix by fk. 

 
Both definitions 5 and 6 strictly follow lemma 2 

that kept the ordered relations among features. For 
example, once node k could be found in X, it is 
unnecessary to visit its children. More specifically, 
to determine whether a frequent pattern is in X, we 
need to compare feature vector of X and l2-
sequence database. It is clearly that the time com-
plexity of our method is O(Favg*Navg) where Favg is 
the average number of frequent features per exam-
ple, while the Navg is the average length of l2-
sequence. In other words, our method does not de-
pendent on the polynomial kernel degree.  

67



4 Experiments 
To evaluate our method, we examine the well-
known shallow parsing task which is the task of 
CoNLL-20001. We also adopted the released perl-
evaluator to measure the recall/precision/f1 rates. 
The used feature consists of word, POS, ortho-
graphic, affix(2-4 prefix/suffix letters), and previ-
ous chunk tags in the two words context window 
size (the same as (Lee and Wu, 2007)). We limited 
the features should at least appear more than twice 
in the training set.  

For the learning algorithm, we replicate the 
modified finite Newton SVM as learner which can 
be trained in linear time (Keerthi and DeCoste, 
2005). We also compare our method with the stan-
dard linear and polynomial kernels with SVMlight 2.  

4.1 Results 
Table 2 lists the experimental results on the 
CoNLL-2000 shallow parsing task. Table 3 com-
pares the testing speed of different feature expan-
sion techniques, namely, array visiting (our method) 
and enumeration. 
Table 2: Experimental results for CoNLL-2000 shal-
low parsing task 

CoNLL-2000 F1 Mining Time 
Training 

Time 
Testing 
Time 

Linear Kernel 93.15 N/A 0.53hr 2.57s
Polynomial(d=2) 94.19 N/A 11.52hr 3189.62s
Polynomial(d=3) 93.95 N/A 19.43hr 6539.75s
Our Method 
(d=2,sup=0.01) 

93.71 <10s 0.68hr 6.54s

Our Method 
(d=3,sup=0.01) 

93.46 <15s 0.79hr 9.95s

Table 3: Classification time performance of enu-
meration and array visiting techniques 

Array visiting Enumeration CoNLL-2000 
d=2 d=3 d=2 d=3 

Testing time 6.54s 9.95s 4.79s 11.73s
Chunking speed 
(words/sec) 7244.19 4761.50 9890.81 4038.95

It is not surprising that the best performance was 
obtained by the classical polynomial kernel. But 
the limitation is that the slow in training and test-
ing time costs. The most efficient method is linear 
kernel SVM but it does not as accurate as polyno-
mial kernel. However, our method stands for both 
efficiency and accuracy in this experiment. In 
terms of training time, it slightly slower than the 
linear kernel, while it is 16.94 and ~450 times 
faster than polynomial kernel in training and test-

                                                 
1 http://www.cnts.ua.ac.be/conll2000/chunking/ 2 http://svmlight.joachims.org/ 

ing. Besides, the pattern mining time is far smaller 
than SVM training.  

As listed in Table 3, we can see that our method 
provide a more efficient solution to feature expan-
sion when d is set more than two. Also it demon-
strates that when d is small, the enumerate-based 
method is a better choice (see PKE in (Kudo and 
Matsumoto, 2004)).  

5 Conclusion 
This paper presents an approximate method for 
extending linear kernel SVM to analogy polyno-
mial-like computing. The advantage of this method 
is that it does not require maintaining the cost of 
support vectors in training, while achieves satisfac-
tory result. On the other hand, we also propose a 
new method for speeding up classification which is 
independent to the polynomial kernel degree. The 
experimental results showed that our method close 
to the performance of polynomial kernel SVM and 
better than the linear kernel. In terms of efficiency, 
our method did not only improve 16.94 times 
faster in training and 450 times in testing, but also 
faster than previous similar studies.  

References 
Chad Cumby and Dan Roth. 2003. Kernel methods for rela-

tional learning. International Conference on Machine 
Learning, pages 104-114. 

Hideki Isozaki and Hideto Kazawa. 2002. Efficient support 
vector classifiers for named entity recognition. Interna-
tional Conference on Computational Linguistics, pages 1-7. 

Jian Pei, Jiawei Han, Behzad Mortazavi-Asl, Jianyong Wang, 
Helen Pinto, Qiming Chen, Umeshwar Dayal and Mei-
Chun Hsu. 2004. Mining Sequential Patterns by Pattern-
Growth: The Prefix Span Approach. IEEE Trans. on 
Knowledge and Data Engineering, 16(11): 1424-1440. 

Sathiya Keerthi and Dennis DeCoste. 2005. A modified finite 
Newton method for fast solution of large scale linear SVMs. 
Journal of Machine Learning Research. 6: 341-361. 

Taku Kudo and Yuji Matsumoto. 2001. Fast methods for 
kernel-based text analysis. Annual Meeting of the Associa-
tion for Computational Linguistics, pages 24-31. 

Taku Kudo and Yuji Matsumoto. 2001. Chunking with sup-
port vector machines. Annual Meetings of the North 
American Chapter and the Association for the Computa-
tional Linguistics. 

Yue-Shi Lee and Yu-Chieh Wu. 2007. A Robust Multilingual 
Portable Phrase Chunking System. Expert Systems with 
Applications, 33(3): 1-26. 

Vladimir N. Vapnik. 1995. The Nature of Statistical Learn-
ing Theory. Springer. 

Chen Wang, Mingsheng Hong, Jian Pei, Haofeng Zhou, Wei 
Wang and Baile Shi. 2004. Efficient Pattern-Growth 
Methods for Frequent Tree Pattern Mining. Pacific knowl-
edge discovery in database (PAKDD). 

68


