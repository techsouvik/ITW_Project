













































Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics


Proceedings of the ACL 2007 Student Research Workshop, pages 43–48,
Prague, June 2007. c©2007 Association for Computational Linguistics

Limitations of Current Grammar Induction Algorithms

Bart Cramer
School of Behavioral and Cognitive Neurosciences

University of Groningen
Groningen, the Netherlands

bart.cramer@gmail.com

Abstract

I review a number of grammar induction
algorithms (ABL, Emile, Adios), and test
them on the Eindhoven corpus, resulting in
disappointing results, compared to the usu-
ally tested corpora (ATIS, OVIS). Also, I
show that using neither POS-tags induced
from Biemann’s unsupervised POS-tagging
algorithm nor hand-corrected POS-tags as
input improves this situation. Last, I argue
for the development of entirely incremental
grammar induction algorithms instead of the
approaches of the systems discussed before.

1 Introduction

Grammar induction is a task within the field of nat-
ural language processing that attempts to construct a
grammar of a given language solely on the basis of
positive examples of this language. If a successful
method is found, this will have both practical appli-
cations and considerable theoretical implications.

Concerning the practical side, this will make the
engineering of NLP systems easier, especially for
less widely studied languages. One can conceive
successful GI algorithms as an inspiration for sta-
tistical machine translation systems.

Theoretically, grammar induction is important as
well. One of the main assertions in the nativist’s
position is the Poverty of the Stimulus argument,
which means that the child does not perceive enough
positive examples of language throughout his early
youth to have learned the grammar from his parents,
without the help of innate knowledge (or: Universal

Grammar), that severely constrains the number of
hypotheses (i.e. grammars) that he can learn. Proved
more strictly for formal grammars, Gold’s (1967)
work showed that one cannot learn any type of su-
perfinite grammar (e.g. regular languages, context-
free languages), if one only perceives (an unlim-
ited amount of) positive examples. After, say, n ex-
amples, there is always more than 1 grammar that
would be able to explain the seen examples, thus
these grammar might give different judgments on an
n + 1th example, of which it is impossible to say in
advance which judgment is the correct one.

But, given this is true, isn’t the grammar induction
pursuit deemed to fail? Not really. First, there are
hints that children do receive negative information,
and that they use it for grammar acquisition. Also,
the strictness required by Gold is not needed, and an
approximation in the framework of PAC (Probably
Approximately Correct) or VC (Vapnis and Cher-
vonenkis) could then suffice. This, and other argu-
ments favouring the use of machine learning tech-
niques in linguistic theory testing, are very well re-
viewed in Lappin and Shieber (2007).

Several attempts have been made to create such
systems. The authors of these systems reported
promising results on the ATIS and OVIS treebanks. I
tried to replicate these findings on the more compli-
cated Eindhoven treebank, which turned out to yield
disappointing results, even inferior to very simple
baselines. As an attempt to ameliorate this, and as
an attempt to confirm Klein and Manning’s (2002)
and Bod’s (2006) thesis that good enough unsuper-
vised POS-taggers exist to justify using POS-tags
instead of words in evaluating GI systems, I pre-

43



sented the algorithms with both POS-tags that were
induced from Biemann’s unsupervised POS-tagging
algorithm and hand-corrected POS-tags. This did
not lead to improvement.

2 Current Grammar Induction Models

2.1 Algorithms

Grammar induction models can be split up into two
types: tag-based and word-based grammar induc-
tion. The key feature that distinguishes between
these two is the type of input. Tag-based systems
receive part-of-speech tags as their input (i.e. the
words are already labelled), and only induce rules
using the given tags. This kind of work is done
by, for instance, Klein and Manning (2005). On the
other hand, word-based models accept plain text as
its input, and have to extract both the categories and
the syntactic rules from given input.

Recently, several word-based grammar induction
algorithms have been developed: Alignment-Based
Learning (van Zaanen, 2002), Adios (Solan et al.,
2005), Emile (Adriaans, 1992; Adriaans and Ver-
voort, 2002) and GraSp1 (Henrichsen, 2002). Al-
though the means of computation and underlying
aims differ, they all rely to a certain extent on Har-
ris’ principle (1951): if two word groups constitute
the same category, then they can be interchanged in
any sentence, without damaging the grammaticality
of that sentence. Hence, these GI system depend on
the inverse: if two word groups appear to occur in
the same contexts, they probably possess the same
syntactic characteristics.

The most prominent example of this principle is
Alignment-Based Learning, or ABL, (van Zaanen,
2002). This algorithm consists of two stages. First,
all sentences are aligned such that it finds a shared
and a distinct part of all pairs of sentences, sug-
gesting that the distinct parts have the same type.
For example, consider the pair ‘I saw the man’ and
‘I saw John’. Here, ’John’ and ’the man’ are cor-
rectly identified as examples of the same type (NP’s
in this case). The second step, that takes the same
corpus as input, tries to identify the constituents in
that sentence. Because the generated constituents
found in the previous step might overlap, the correct

1As there was no current working version of this system, I
did not include it in this project.

John
(.)

Pat
(.)

Jim
(.)

walks x x
talks x x
smiles x x

Table 1: An example of some context/expression
pairs to show the workings of EMILE. Note that, un-
der standard settings, a rule covering this entire table
will be inferred, causing a phrase like ‘John talks’ to
be accepted, although there was no such input sen-
tence.

ones have to be selected. Simple heuristics are used
to achieve this, for example to take the constituent
that was generated first (ABL-first) or to take the
constituent with the highest score on some proba-
bilistic function (ABL-leaf). For details, I refer to
van Zaanen (2000). Because ABL compares all sen-
tences in the corpus with all other sentences, the al-
gorithm is quadratic in the number of sentences, but
has low memory demands. Interestingly, ABL does
not come up with an explicit grammar, but generates
just a bracketed version of the corpus instead.

Adios (Solan et al., 2005) uses Harris’ principle
as well, although it attempts to create a grammar
(either context-free or context-sensitive) more ex-
plicitly. The algorithm represents language as a di-
rected pseudograph2 , with equivalence classes (ini-
tially single words) as nodes. Input sentences can
be regarded as ‘snakes’ over the nodes in the graph.
If enough support is found, words are merged into
equivalence classes, or frequently occurring edges
are put in a path (a rule in usual grammatical terms).
This generalisation process is done iteratively, until
convergence is reached.

Emile (Adriaans, 1992; Adriaans and Vervoort,
2002) is the system that to a greater extent tries to
pinpoint its reasons to accept a linguistic hypothe-
sis. Each rule is divided into expressions and types,
where types should be the interchangeable part of
two sentences. Instead of explicitly comparing each
sentence with all other sentences, it incrementally
builds up a table of type/expression pairs, and on the
basis of this table rules are extracted. An example is
given in table 1. This incrementality has two major

2This is a graph that allows for loops and multiple edges.

44



consequences: it makes the system vastly more effi-
cient in terms of time, at the cost of rising memory
demands, and it models time linearly, in contrast to
ABL and Adios.

2.2 Evaluation

Different methods of evaluation are used in GI. One
of them is visual inspection (Henrichsen, 2002).
This is not a reproducible and independent evalua-
tion measure, and it does certainly not suffice as an
assessment of the quality of the results. However,
Roberts and Atwell (2003) argue that this evaluation
should still be included in GI discussions.

A second evaluation method is shown by Solan
et al. (2005), in which Adios had to carry out a test
that is available on the Internet: English as a Second
Language (ESL). This test shows three sentences, of
which the examinee has to say which sentence is the
grammatical one. Adios answers around 60% cor-
rect on these questions, which is considered as inter-
mediate for a person who has had 6 years of English
lessons. Although this sounds impressive, no exam-
ples of test sentences are given, and the website is
not available anymore, so we are not able to assess
this result.

A third option is to have sentences generated by
the induced grammar judged on their naturalness,
and compare this average with the average of the
sentences of the original corpus. Solan et al. (2005)
showed that the judgments of Adios generated sen-
tences were comparable to the sentences in their cor-
pus. However, the algorithm might just generates
overly simple utterances, and will receive relatively
high scores that it doesn’t deserve.

The last option for evaluation is to compare the
parses with hand-annotated treebanks. This gives
the most quantifiable and detailed view on the per-
formance of a GI system. An interesting compara-
tive study between Emile and ABL using this eval-
uation method is available in van Zaanen and Adri-
aans (2001) where F-scores of 41.4% (Emile) and
61.7% (ABL) are reported on the OVIS (Openbaar
Vervoer Informatie Systeem3; Dutch) corpus, and
25.4% and 39.2% on the ATIS (Air Traffic Informa-
tion System; English) corpus.

3This acronym means Public Transport Information System.

3 Experiment 1

3.1 Motivation

A major choice in evaluating GI systems is to decide
which corpus to train the algorithm on. The cre-
ators of ABL and Emile chose to test on the ATIS
and OVIS corpus, which is, I believe, an unfortu-
nate choice. These corpora contain sentences that
are spoken to a computer, and represent a very lim-
ited subset of language. Deep recursion, one of the
aspects that is hard to catch in grammar induction,
does not occur often. The average sentence lengths
are 7.5 (ATIS) and 4.4 (OVIS). If we want to know
whether a system is truly capable of bootstrapping
knowledge about language, there is only one way to
test it: by using natural language that is unlimited
in its expressive power. Therefore, I will test ABL,
Adios and Emile on the Eindhoven corpus, that con-
tains 7K sentences, with an average length of ap-
proximately 20 tokens. This is, as far as I know, the
first attempt to train and test word-based GI algo-
rithms on such a complicated corpus.

3.2 Method

The Eindhoven corpus has been automatically anno-
tated by Alpino (Bouma et al., 2000; van der Beek
et al., 2002), a wide-coverage hand-written parser
for Dutch, with around 90% dependency triple ac-
curacy. Afterwards, this treebank has been manu-
ally corrected. The treebank does not literally con-
tain trees, but graphs: some nodes can be copied, so
that linguistic structure can be analyzed in more de-
tail. However, by removing all double nodes it is still
possible to retrieve a list of bracket-tuples from these
graphs. The graphs are also non-concatenative,
meaning that a constituent can span word groups that
are not contiguous. Therefore, if a sentence contains
a constituent wi...wjwk...wl, with k − j > 1, three
bracket-tuples are generated: (i, j), (k, l) and (i, l).

Evaluation of the algorithm is done according to
PARSEVAL, except for a few changes that are also
proposed by Klein and Manning (2002). The set of
bracket-pairs that is found in the Alpino treebank
are called facts, and those from a grammar induc-
tion algorithm predictions. The intersection of the
facts and predictions are called hits. From these we
can compute the unlabeled precision, recall and F-
score. The subtleties adopted from Klein and Man-

45



ning are the following: constituents of length 0 or 1,
constituents that span the whole sentence and con-
stituents just excluding punctuation are not taken
into account, as these are obvious predictions.

Three baselines were created: an algorithm that
always branches left4, idem for right-branching and
an algorithm that performs binary branching on ran-
dom points in the sentence. Note that left-branching
and right-branching yield the maximum number of
predictions.

3.3 Results

From the results in table 2, it can be seen that ABL
scores best: it is the only one that is able to slightly
outperform the random baseline. This is surpris-
ing, because it is the least complicated system of the
three. Adios and Emile performed poorly. It ap-
pears that, with larger sentences, the search space
become too sparse to actually induce any meaning-
ful structure. This is expressed in the low number of
predictions per sentence that Adios (1.5) and Emile
(0.7) make. Adjusting support parameters, to make
the algorithm accept more hypotheses, did not have
the intended effect. Still, notice that Emile has a rel-
atively high precision.

In sum, none of the systems is convincingly able
to outperform the very simple baselines. Neither
did visual inspection give the impression that mean-
ingful information was derived. Therefore, it can
be concluded that current word-based GI algorithms
are not equipped to derive syntactic structure from
corpora as complicated as the Eindhoven corpus.

4 Experiment 2

4.1 Motivation

The second experiment deals with the difference
between tag-based and word-based systems. Intu-
itively, the latter task seems to be more challenging.
Still, Klein and Manning (2002) and Bod (2006)
stick to tag-based models. Their argumentation is
twofold.

First, Bod assumes that unsupervised POS-
tagging can be done successfully, without explic-
itly showing results that can confirm this. Klein
and Manning did tag their text using a simple un-
supervised POS-tagging algorithm, and this mod-

4For example: [ [ [ I saw ] the ] large ] house.

erately harmed their performance: their Context-
Constituent Model’s F-score on Wall Street Journal
text fell from 71.1% to 63.2%.

Second, Klein and Manning created context vec-
tors for a number of non-terminals (NP, VP, PP), and
extracted the two principal components from these
vectors. They did the same with contexts of con-
stituents and distituents. The distribution of these
vectors suggest that the non-terminals were easier
to distinguish from each other than the constituents
from the distituents, suggesting that POS-tagging is
easier than finding syntactic rules. However, this
result would be more convincing if this is true for
POS-tags as well.

4.2 Method

In order to test the argument above, and as an at-
tempt to improve the results from the previous ex-
periment, POS-tags were induced using Biemann’s
unsupervised POS-tagger (Biemann, 2006). Be-
cause that algorithm needs at least 50M words to
work reliably, it was trained on the concatenation of
the Eindhoven corpus and the CLEF corpus (70M
words, also newspaper text). The tags of the Eind-
hoven corpus are then used as input for the GI al-
gorithms, both under same settings as experiment 1.
The evaluation was done the same way as in experi-
ment 1.

The same method was carried out using hand-
corrected tags. Large and equal improvements will
imply the justification for tag-based grammar in-
duction. If the models only improve on the hand-
corrected tags, this will suggest the opposite.

4.3 Results

The results can be found in table 3. Generally, more
predictions were made with respect to experiment 1,
due to the denser search space. Only a convergence
to the baseline was achieved, especially by Adios
and Emile, that were very low in predictions in the
first experiment. Again, none of the tested systems
was able to clearly outperform the baselines.

Because using neither induced nor hand-corrected
made the systems work more reliably, there seems to
be no strong evidence in favor or against Bod’s and
Klein and Manning’s conjecture. Therefore, there is
no sound justification for tag-based grammar induc-
tion yet.

46



Method Hits/Predictions Precision Recall F-score
Left 5.8K / 119K 4.9% 9.2% 6.4%
Right 4.4K / 119K 3.6% 6.9% 4.8%
Random 11K / 93K 11.7% 17.3% 14.0%
ABL-leaf 4.0K / 24K 16.9% 6.4% 9.3%
ABL-first 13K / 113K 11.6% 20.8% 14.9%
Adios 319 / 11K 2.8% 0.5% 0.9%
Emile 912 / 5.2K 17.3% 1.5% 2.7%

Table 2: This table shows the results of experiment 1. Left, Right and Random are baseline scores. The two
variants of ABL differ in the selection phase. 62.9K facts were found in the Alpino treebank.

Induced tags Hand-corrected tags
Method Hits/Pred.’s Precision Recall F-score Hits/Pred.’s Precision Recall F-score
ABL-leaf 5K / 30K 16.8% 8.1% 10.9% 7.0K / 34K 21.0% 11.2% 14.6%
ABL-first 11K / 125K 9.2% 18.2% 12.2% 12.6K / 123K 10.3% 20.0% 13.6%
Adios 2.7K / 24K 11.2% 4.3% 6.3% 2.2K / 20K 11.0% 3.5% 5.3%
Emile 1.8K / 16K 11.2% 2.9% 4.6% 1.7K / 19K 8.9% 2.7% 4.1%

Table 3: This table shows the results of experiment 2. The baseline scores are identical to the ones in
experiment 1.

5 Discussion

The results from experiment 1 and 2 clearly show
that ABL, Adios and Emile have severe shortcom-
ings, and that they cannot derive meaningful struc-
ture from language as complicated as the Eindhoven
corpus. An important reason for this is that a cor-
pus with only 7K sentences is not able to sufficiently
cover the search space. This can be seen from the
very low number of predictions made by Adios and
Emile: there was not enough support to accept hy-
potheses.

But how should we proceed? Any algorithm
based solely on Harris’ principle can be either incre-
mental (Emile) or non-incremental (ABL, Adios).
The previous experiments show that very large cor-
pora are needed to mitigate the very sparse search
space, leading me to conclude that non-incremental
systems are not suitable for the problem of gram-
mar induction. Also, incremental systems have the
advantage of an intuitive notion of time: it is al-
ways clear which working hypothesis of a grammar
is maintained.

Emile retains a Boolean table with all combina-
tions of types and expressions it has encountered up
until a given moment. This means that very infre-

quent words demand a disproportionally large part
of the memory. Therefore, all found words and rules
should be divided into three groups: pivotal, nor-
mal and infrequent. Initially, all encountered words
are infrequent. Transitions to the normal and piv-
otal stage occur when an estimator of the relative
frequency is high enough, for example by taking the
lower bound of the confidence interval (Mikheev,
1997). Ultimately, the number of words in the nor-
mal and pivotal stage will converge to a constant.
For example, if the relative frequency of a word
should be larger than 0.01 to become pivotal, there
can only be 100 of these words. Because one can
define upper limits for pivotal and normal words,
the size of the bookkeeping table is limited as well.
Also, when the system starts inducing syntactic cate-
gories of words, very infrequent words should not be
parsed as a separate category initially, but as a mem-
ber of another open-class category. This connects to
the cross-linguistic tendency that infrequent words
generally have simple complementation patterns.

One very important question remains: what in-
tuitions should this imaginary system use to induce
rules? First, all sentences should be sorted by length.
Then, for each sentence, the following steps are
taken:

47



• Update the bookkeeping tables.

• Parse the sentence as deeply as possible.

• If the sentence cannot be parsed completely,
induce all possible rules that would make the
parse complete. Add all these rules to the book-
keeping tables.

The last step deserves some extra attention. If
the algorithm encounters the sentence ‘he is such a
(.)’, we can safely infer that the unknown word at
(.) is a noun. Inducing complementation patterns
should be possible as well. Imagine that the algo-
rithm understands NP’s and transitive verbs. Then
consider the following: ‘John gave Tim a book’.
It will parse ‘John gave Tim’ as a sentence, and ‘a
book’ as a noun phrase. Because these two should
be connected, a number of hypotheses are generated,
for example: ‘a book’ is a complement of ‘Tim’; ‘a
book’ is a complement of ‘John gave Tim’; ‘a book’
is a second complement of ‘gave’. Naturally, only
the last hypothesis is correct. All three inductions
are included, but only the last is likely to be repro-
duced in later sentences in the corpus, because sen-
tences of the form ‘(.) gave (.) (.)’ are more likely
than ‘John gave Tim (.)’ and ‘Tim (.)’.

6 Acknowledgments

I would like to thank Jennifer Spenader, Gertjan van
Noord and the anonymous reviewers for providing
me their invaluable comments.

References

Pieter W. Adriaans and Mark R. Vervoort. 2002. The
EMILE 4.1 grammar induction toolbox. In Proceed-
ings of the 6th International Colloquium on Gram-
mar Induction (ICGI), pages 293–295, Amsterdam,
the Netherlands.

Pieter W. Adriaans. 1992. Language learning from a cat-
egorial perspective. Ph.D. thesis, University of Ams-
terdam, NL.

Chris Biemann. 2006. Unsupervised part-of-speech tag-
ging employing efficient graph clustering. In Proceed-
ings of ACL/COLING-2006 Students Research Work-
shop, pages 7–12, Sydney, Australia.

Rens Bod. 2006. An all-subtrees approach to unsuper-
vised parsing. In Proceedings of ACL/COLING-2006,
pages 865–872, Sydney, Australia.

Gosse Bouma, Gertjan van Noord, and Robert Malouf.
2000. Alpino: wide-coverage computational analysis
of Dutch. In Proceedings of Computational Linguis-
tics in the Netherlands (CLIN), pages 45–59, Tilburg,
the Netherlands.

E. Mark Gold. 1967. Language identification in the
limit. Information and Control, 16:447–474.

Zellig S. Harris. 1951. Methods in Structural Linguis-
tics. University of Chicago Press, Chicago.

Peter J. Henrichsen. 2002. GraSp: Grammar learning
from unlabelled speech corpora. In Proceedings of
CoNLL-2002, pages 22–28, Pennsylvania, PA, USA.

Dan Klein and Christopher D. Manning. 2002. A gener-
ative Constituent-Context Model for improved gram-
mar induction. In Proceedings of ACL-2001, pages
128–135, Toulouse, France.

Dan Klein and Christopher D. Manning. 2005. Nat-
ural language grammar induction with a genera-
tive constituent-context model. Pattern Recognition,
9(38):1407–1419.

Shalom Lappin and Stuart M. Shieber. 2007. Machine
learning theory and practice as a source of insight into
universal grammar. Computational Linguistics, 43:1–
34.

Andrei Mikheev. 1997. Automatic rule induction for
unknown-word guessing. Computational Linguistics,
23(3):405–423.

Andrew Roberts and Eric Atwell. 2003. The use of cor-
pora for automatic evaluation of grammar inference
systems. In Proceedings of the Corpus Linguistics
2003 conference, pages 657–661, Lancaster, United
Kingdom.

Zach Solan, David Horn, Eytan Ruppin, and Shimon
Edelman. 2005. Unsupervised learning of natural lan-
guages. Proceedings of the National Academy of Sci-
ences, 102(33):11629–11634.

Leonoor van der Beek, Gosse Bouma, Robert Malouf,
and Gertjan van Noord. 2002. The Alpino depen-
dency treebank. In Proceedings of Computational Lin-
guistics in the Netherlands (CLIN) 2001, pages 8–22,
Enschede, the Netherlands.

Menno van Zaanen and Pieter W. Adriaans. 2001.
Alignment-Based Learning versus EMILE: A compar-
ison. In Proceedings of the 13th Dutch-Belgian Artifi-
cial Intelligence Conference (BNAIC), pages 315–322,
Amsterdam, the Netherlands.

Menno van Zaanen. 2002. Implementing Alignment-
Based Learning. In Proceedings of the 6th Interna-
tional Colloquium on Grammatical Inference (ICGI),
pages 312–314, Amsterdam, the Netherlands.

48


