













































Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics


Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 872–879,
Prague, Czech Republic, June 2007. c©2007 Association for Computational Linguistics

Benefits of the ‘Massively Parallel Rosetta Stone’:  
Cross-Language Information Retrieval with over 30 Languages 

Peter A. Chew 
Sandia National Laboratories 

P. O. Box 5800, MS 1012 
Albuquerque, NM 87185-1012, USA 

pchew@sandia.gov

Ahmed Abdelali 
New Mexico State University 

P.O. Box 30002, Mail Stop 3CRL 
Las Cruces, NM 88003-8001, USA 

ahmed@crl.nmsu.edu

Abstract 

In this paper, we describe our experiences 
in extending a standard cross-language in-
formation retrieval (CLIR) approach 
which uses parallel aligned corpora and 
Latent Semantic Indexing. Most, if not 
all, previous work which follows this ap-
proach has focused on bilingual retrieval; 
two examples involve the use of French-
English or English-Greek parallel cor-
pora. Our extension to the approach is 
‘massively parallel’ in two senses, one 
linguistic and the other computational. 
First, we make use of a parallel aligned 
corpus consisting of almost 50 parallel 
translations in over 30 distinct languages, 
each in over 30,000 documents. Given the 
size of this dataset, a ‘massively parallel’ 
approach was also necessitated in the 
more usual computational sense. Our re-
sults indicate that, far from adding more 
noise, more linguistic parallelism is better 
when it comes to cross-language retrieval 
precision, in addition to the self-evident 
benefit that CLIR can be performed on 
more languages. 

1 Introduction 

Approaches to cross-language information retrieval 
(CLIR) fall generally into one of two types, or 
some combination thereof: the ‘query translation’ 
approach or the ‘parallel corpus’ approach. The 
first of these, which is perhaps more common, in-

volves translation of the query into the target lan-
guage, for example using machine translation or 
on-line dictionaries.  The second makes use of par-
allel aligned corpora as training sets. One approach 
which uses parallel corpora does this in conjunc-
tion with Latent Semantic Indexing (LSI) (Lan-
dauer and Littman 1990, Young 1994). According 
to Berry et al. (1994:21), the use of LSI with paral-
lel corpora can be just as effective as the query 
translation approach, and avoids some of the draw-
backs of the latter, discussed in Nie et al. (1999). 

Generally, research in CLIR has not attempted 
to use very many languages at a time (see for ex-
ample Nie and Jin 2002). With query translation 
(although that is not the approach that Nie and Jin 
take), this is perhaps understandable, as for each 
new language, a new translation algorithm must be 
included. The effort involved in extending query 
translation to multiple languages, therefore, is 
likely to be in proportion to the number of lan-
guages. 

With parallel corpora, the reason that research 
has been limited to only a few languages at a time 
– and usually just two at a time, as in the LSI work 
cited above – is more likely to be rooted in the 
widespread perception that good parallel corpora 
are difficult to obtain (see for example Asker 
2004). However, recent work (Resnik et al. 1999, 
Chew et al. 2006) has challenged this idea. 

One advantage of a ‘massively parallel’ multi-
lingual corpus is perhaps self-evident: within the 
LSI framework, the more languages are mapped 
into the single conceptual space, the fewer restric-
tions there are on which languages documents can 
be selected from for cross-language retrieval. 
However, several questions were raised for us as 

872



we contemplated the use of a massively parallel 
corpus. Would the addition of languages not used 
in testing create ‘noise’ for a given language pair, 
reducing the precision of CLIR? Could partially 
parallel corpora be used? Our work appears to 
show both that more languages are generally bene-
ficial, and even incomplete parallel corpora can be 
used. In the remainder of this paper, we provide 
evidence for this claim. The paper is organized as 
follows: section 2 describes the work we undertook 
to build the parallel corpus and its characteristics. 
In section 3, we outline the mechanics behind the 
'Rosetta-Stone' type method we use for cross-
language comparison. In section 4, we present and 
discuss the results of the various tests we per-
formed. Finally, we conclude on our findings in 
section 5. 

2 The massively parallel corpus  

Following Chew et al. (2006), our parallel corpus 
was built up from translations of the Bible which 
are freely available on the World Wide Web. Al-
though reliable comparable statistics are hard to 
find, it appears to be generally agreed that the Bi-
ble is the world’s most widely translated book, 
with complete translations in 426 languages and 
partial translations in 2,403 as of December 31, 
2005 (Bible Society, 2006). Great care is taken 
over the translations, and they are alignable by 
chapter and verse. According to Resnik et al. 
(1999), the Bible’s coverage of modern vocabulary 
may be as high as 85%. The vast majority of the 
translations we used came from the ‘Unbound Bi-
ble’ website (Biola University, 2005-2006); from 
this website, the text of a large number of different 
translations of the Bible can – most importantly for 
our purposes – be downloaded in a tab-delimited 
format convenient for loading into a database and 
then indexing by chapter and verse in order to en-
sure ‘parallelism’ in the corpus. The number of 
translations available at the website is apparently 
being added to, based on our observations access-
ing the website on a number of different occasions. 

The languages we have included in our multilin-
gual parallel corpus include those both ancient and 
modern, and are as follows: 

Language No. of 
translations 

Used in 
tests 

Afrikaans 1 12+ 
Albanian 1 27+ 
Arabic 1 All 
Chinese (Simplified) 1 44+ 
Chinese (Traditional) 1 44+ 
Croatian 1 27+ 
Czech 2 12+ 
Danish 1 12+ 
Dutch 1 12+ 
English 7 All 
Finnish 3 27+ 
French 2 All 
German 4 8,27+ 
Greek (New Testament) 2 46+ 
Hebrew (Old Testament) 1 46+ 
Hebrew (Modern) 1 6,12+ 
Hungarian 1 6+ 
Italian 2 8,27+ 
Japanese* 1 9+ 
Korean 1 27+ 
Latin 1 8,9,28+ 
Maori 1 7,8,9,27+ 
Norwegian 1 27+ 
Polish* 1 27+ 
Portuguese 1 27+ 
Russian 1 All 
Spanish 2 All 
Swedish 1 27+ 
Tagalog 1 27+ 
Thai 1 27+ 
Vietnamese 1 27,44+ 

Table 1. Languages1

The languages above represent many of the ma-
jor language groups: Austronesian (Maori and 
Tagalog); Altaic (Japanese and Korean); Sino-
Tibetan (Chinese); Semitic (Arabic and Hebrew);  
Finno-Ugric (Finnish and Hungarian); Austro-
Asiatic (Vietnamese); Tai-Kadai (Thai); and Indo-
European (the remaining languages). The two New 
Testament Greek versions are the Byzan-
tine/Majority Text (2000), and the parsed version 
of the same text, in which we treated distinct mor-
phological elements (such as roots or inflectional 
endings) as distinct terms. Overall, the list includes 

 
1 Translations in languages marked with an asterisk above 
were obtained from websites other than the ‘Unbound Bible’ 
website. ‘Used in tests’ indicates in which tests in Table 2 
below the language was used as training data, and hence the 
order of addition of languages to the training data. 

873



47 versions in 31 distinct languages (assuming 
without further discussion here that each entry in 
the list represents a distinct language). 

We aligned the translations by verse, and, since 
there are some differences in versification between 
translations (for example, the Hebrew Old Testa-
ment includes the headings for the Psalms as sepa-
rate verses, unlike most translations), we spent 
some time cleaning the data to ensure the align-
ment was as good as possible, given available re-
sources and our knowledge of the languages. (Even 
after this process, the alignment was not perfect, 
and differences in how well the various transla-
tions were aligned may account for some of the 
variability in the outcome of our experiments, de-
pending on which translations were used.) The end 
result was that our parallel corpus consisted of 
31,226 ‘mini-documents’ – the total number of text 
chunks2 after the cleaning process, aligned across 
all 47 versions. The two New Testament Greek 
versions, and the one Old Testament Hebrew ver-
sion, were exceptions because these are only par-
tially complete; the former have text in only 7,953 
of the verses, and the latter has text in 23,266 of 
the verses. For some versions, a few of the verse 
translations are incomplete where a particular verse 
has been skipped in translation; this also explains 
the fact that the number of Hebrew and Greek text 
chunks together do not add up to 31,226. However, 
the number of such verses is negligible in compari-
son to the total. 

3 Framework 

The framework we used was the standard LSI 
framework described in Berry et al. (1994). Each 
aligned mini-document from the parallel corpus 
consists of the combination of text from all the 31 
languages. A document-by-term matrix is formed 
in which each cell represents a weighted frequency 
of a particular term t in a particular document k.
We used a standard log-entropy weighting scheme, 
where the weighted frequency W is given by: 

 
W = log2 (F) × (1 + Ht / log2 (N)) 

 
where F is the raw frequency of t in k, Ht is the 
standard ‘p log p’ measure of the entropy of the 
term across all documents, and N is the number of 
 
2 The text chunks generally had the same boundaries as the 
verses in the original text. 

documents in the corpus. The last term in the ex-
pression above, log2 (N), is the maximum entropy 
that any term can have in the corpus, and therefore 
(1 + Ht / log2 (N)) is 1 for the most distinctive 
terms in the corpus, 0 for those which are least dis-
tinctive. 

The sparse document-by-term matrix is sub-
jected to singular value decomposition (SVD), and 
a reduced non-sparse matrix is output. Generally, 
we used the output corresponding to the top 300 
singular values in our experiments. When we had a 
smaller number of languages in the mix, it was 
possible to use SVDPACK (Berry et al. 1996), 
which is an open-source non-parallel algorithm for 
computing the SVD, but for larger problems (in-
volving more than a couple of dozen parallel ver-
sions), use of a parallel algorithm (in a library 
called Trilinos) was necessitated. (This was run on 
a Linux cluster consisting of 4,096 dual CPU com-
pute nodes, running on Dell PowerEdge 1850 1U 
Servers with 6GB of RAM.) 

In order to test the precision versus recall of our 
framework, we used translations of the 114 suras 
of the Qu’ran into five languages, Arabic, English, 
French, Russian and Spanish. The number of 
documents used for testing is fairly small, but large 
enough to give comparative results for our pur-
poses which are still highly statistically significant. 
The test set was split into each of the 10 possible 
language-pair combinations: Arabic-English, Ara-
bic-French, English-French, and so on. 

For each language pair and test, 228 distinct 
‘queries’ were submitted – each query consisting 
of one of the 228 sura ‘documents’. If the highest-
ranking document in the other language of the pair 
was in fact the query’s translation, then the result 
was deemed ‘correct’. To assess the aggregate per-
formance of the framework, we used two meas-
ures: average precision at 0 (the maximum 
precision at any level of recall), and average preci-
sion at 1 document (1 if the ‘correct’ document 
ranked highest, zero otherwise). The second meas-
ure is a stricter one, but we generally found that 
there is a high rate of correlation between the two 
measures anyway. 

4 Results and Discussion 

The following tables show the results of our tests. 
First, we present in Table 2 the overall summary, 

874



with averages across all language pairs used in 
testing. 
 

Average precision No. of 
parallel 
versions

At 0 at 1 doc. 

2 0.706064 0.571491 
3 0.747620 0.649269 
4 0.617615 0.531873 
5 0.744951 0.656140 
6 0.811666 0.732602 
7 0.827246 0.753070 
8 0.824501 0.750000 
9 0.823430 0.746053 
12 0.827761 0.752632 
27 0.825577 0.751316 
28 0.823137 0.747807 
44 0.839346 0.765789 
46 0.839319 0.766667 
47 0.842936 0.774561 

Table 2. Summary results for all language pairs 
 

From the above, the following should be clear: 
as more parallel translations are added to the index, 
the average precision rises considerably at first, 
and then begins to level off after about the seventh 
parallel translation. The results will of course vary 
according to which combination of translations is 
selected for the index. The number of such combi-
nations is generally very large: for example, with 
47 translations available, there are 47! / (40! 7!), or 
62,891,499, possible ways of selecting 7 transla-
tions. Thus, for any particular number of parallel 
versions, we had to use some judgement in which 
parallel versions to select, since there was no way 
to achieve anything like exhaustive coverage of the 
possibilities. 

Further, with more than 7 parallel translations, 
there is certainly no justification for saying that 
adding more translations or languages increases the 
‘noise’ for languages in the test set, since beyond 7 
the average precision remains fairly level. If any-
thing, in fact, the precision still appears to rise 
slightly. For example, the average precision at 1 
document rises by more than 0.75 percentage 
points between 46 and 47 versions. Given that in 
each of these experiments, we are measuring preci-
sion 228 times per language pair, and therefore 
2,280 times in total, this small rise in precision is 
significant (p ≈ 0.034). Interestingly, the 47th ver-

sion to be added was parsed New Testament 
Greek. It appears, therefore, that the parsing helped 
in particular; we also have evidence from other 
experiments (not presented here) that overall preci-
sion is generally improved for all languages when 
Arabic wordforms are replaced by their respective 
citation forms (the bare root, or stem) – also a form 
of morphological parsing. Ancient Greek, like 
Arabic, is morphologically highly complex, so it 
would be understandable that parsing (or stem-
ming) would help when parsing of either language 
is used in training. 

One other point needs to be made here: the three 
versions added after the 44th version were the three 
incomplete versions (the two Greek versions cover 
just the New Testament, while Ancient Hebrew 
covers just the Old Testament). The above-
mentioned increase in precision which resulted 
from the addition of these three versions is clear 
evidence that even in the case where a parallel cor-
pus is defective for some language(s), including 
those languages can still result in the twofold bene-
fit that (1) those languages are now available for 
analysis, and (2) precision is maintained or in-
creased for the remaining languages. 

Finally, precision at 1 document, the stricter of 
the two measures, is by definition less than or 
equal to precision at 0. This taken into account, it 
is also interesting that the gap between the two 
measures seems to narrow as more parallel transla-
tions and parsing are added, as Figure 1 shows. 

For certain applications where it is important 
that the translation is ranked first, not just highly, 
among all retrieved documents, there is thus a par-
ticular benefit in using a ‘massively parallel’ 
aligned corpus. 

0%

2%

4%

6%

8%

10%

12%

14%

16%

2 3 4 5 6 7 8 9 12 27 28 44 46 47
Number of parallel translations

D
iff

er
en

tia
li

n
pe

rc
en

ta
ge

po
in

ts

Figure 1. Differential between precision at 0 and 
precision at 1 document, by number of languages 

875



Now we move on to look at more detailed re-
sults by language pair. Figure 2 below breaks 
down the results for precision at 1 document by 
language pair. In all tests, the two languages in 
each pair were (naturally) always included in the 
languages used for training. There is more volatil-
ity in the results by language pair than there is in 
the overall results, shown again at the right of the 
graph, which should come as no surprise since the 
averages are based on samples a tenth of the size. 
Generally, however, the pattern is the same for 
particular language pairs as it is overall; the more 

parallel versions are used in training, the better the 
average precision. 

There are some more detailed observations 
which should also be made from Figure 2. First, 
the average precision clearly varies quite widely 
between language pairs. The language pairs with 
the best average precision are those in which two 
of English, French and Spanish are present. Of the  
five languages used for testing, these three cluster 
together genetically, since all three are Western 
(Germanic or Romance) Indo-European languages. 
Moreover, these are the three languages of the five 
which are written in the Roman alphabet. 

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Spanish-
Arabic

Arabic-
French

Russian-
Arabic

English-
Arabic

Spanish-
Russian

Russian-
French

English-
Russian

English-
Spanish

Spanish-
French

English-
French

OVERALL

Language Pair

P
re

ci
si

on
at

1
do

cu
m

en
t

2 3 4 5 6 7 8 9 12 27 28 44 46 47Number of languages
 

Figure 2. Chart of precision at 1 doc. by language pair and number of parallel training versions 
 

However, we believe the explanation for the 
poorer results for language pairs involving either 
Arabic, Russian, or both, can be pinned down to 
something more specific. We have already par-
tially alluded to the obvious difference between 
Arabic and Russian on the one hand, and English, 
French and Spanish on the other: that Arabic and 
Russian are highly morphologically rich, while 
English, French and Spanish are generally analytic 
languages. This has a clear effect on the statistics 
for the languages in question, as can be seen in 

Table 3, which is based on selected translations of 
the Bible for each of the languages in question. 
 

Translation Types Tokens 
English (King James) 12,335 789,744 
Spanish (Reina Valera 1909) 28,456 704,004 
Russian (Synodal 1876) 47,226 560,524 
Arabic (Smith Van Dyke) 55,300 440,435 
French (Darby) 20,428 812,947 

Table 3. Statistics for Bible translations in 5 lan-
guages used in test data 

876



Assuming that the respective translations are 
faithful (and we have no reason to believe other-
wise), and based on the statistics in Table 3, it 
should be the case that Arabic contains the most 
‘information’ per term (in the information theoretic 
sense), followed by Russian, Spanish, English and 
French.3 Again, this corresponds to our intuition 
that much information is contained in Arabic pat-
terns and Russian inflectional morphemes, which 
in English, French and Spanish would be contained 
in separate terms (for example, prepositions). 

Without additional pre-processing, however, 
LSI cannot deal adequately with root-pattern or 
inflectional morphology. Moreover, it is clearly a 
weakness of LSI, or at least the standard log-
entropy weighting scheme as applied within this 
framework, that it makes no adjustment for differ-
ences in information content per word between 
languages. Even though we can assume near-
equivalency of information content between the 
different translations above, according to the stan-
dard log-entropy weighting scheme there are large 
differences between the total entropy of particular 
parallel documents; in general, languages such as 
English are overweighted while those such as Ara-
bic are underweighted. 

Now that this issue is in perspective, we should 
draw attention to another detail in Figure 2. Note 
that the language pairs which benefited most from 
the addition of Ancient Greek and Hebrew into the 
training data were those which included Russian, 
and Russian-Arabic saw the greatest increase in 
precision. Recall also that the 47th version to be 
added was the parsed Greek, so that essentially 
each Greek morpheme is represented by a distinct 
term. From Figure 2, it seems clear that the inclu-
sion of parsed Greek in particular boosted the pre-
cision for Russian (this is most visible at the right-
hand side of the set of columns for Russian-Arabic 
and English-Russian). There are, after all, notable 
similarities between modern Russian and Ancient 
Greek morphology (for example, the nominal case 
system). Essentially, the parsed Greek acts as a 
‘clue’ to LSI in associating inflected forms in Rus-

 
3 To clarify the meaning of ‘term’ here: for all languages ex-
cept Chinese, text is tokenized in our framework into terms 
using regular expressions; each non-word character (such as 
punctuation or white space) is assumed to mark the boundary 
of a word. For Chinese, we made the simplifying assumption 
that each character represented a separate term. 

sian with preposition/non-inflected combinations 
in other languages. These results seem to be further 
confirmation of the notion that parsing just one of 
the languages in the mix helps overall; the greatest 
boost is for those languages with morphology re-
lated to that of the parsed language, but there is at 
least a maintenance, and perhaps a small boost, in 
the precision for unrelated languages too. 

Finally, we turn to look at some effects of the 
particular languages selected for training. Included 
in the results above, there were three separate tests 
run in which there were 6 training versions. In all 
three, Arabic, English, French, Russian and Span-
ish were included. The only factor we varied in the 
three tests was the sixth version. In the three tests, 
we used Modern Hebrew (a Semitic language, 
along with Arabic), Hungarian (a Uralic language, 
not closely related to any of the other five lan-
guages), and a second English version respectively. 
The results of these tests are shown in Figure 3, 
with figures for the test in which only 5 versions 
were included for comparative purposes. 

From these results, it is apparent first of all that 
it was generally beneficial to add a sixth version, 
regardless of whether the version added was Eng-
lish, Hebrew or Hungarian. This is consistent with 
the results reported elsewhere in this paper. Sec-
ond, it is also apparent that the greatest benefit 
overall was had by using an additional English ver-
sion, rather than using Hebrew or Hungarian. 
Moreover, perhaps surprisingly, the use of Hebrew 
in training – even though Hebrew is related to 
Arabic – was of less benefit to Arabic than either 
Hungarian or an additional English version. It ap-
pears that the use of multiple versions in the same 
language is beneficial because it enables LSI to 
make use of the many different instantiations in the 
expression of a concept in a single language, and 
that this effect can be greater than the effect which 
obtains from using heterogeneous languages, even 
if there is a genetic relationship to existing lan-
guages. 

877



0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9

1

Russian-
Arabic

Spanish-
Arabic

Arabic-
French

English-
Arabic

Spanish-
Russian

Russian-
French

English-
Russian

English-
Spanish

Spanish-
French

English-
French

OVERALL

Language pair

P
re

ci
si

on
at

1
do

c.

Arabic, English, French, Russian, Spanish Arabic, English, French, Hebrew, Russian, Spanish
Arabic, English, French, Hungarian, Russian, Spanish Arabic, English x 2, French, Russian, Spanish

 

Figure 3. Precision at 1 document for 6 training versions, with results of using different mixes of lan-
guages for training 
 

Figure 3 may also shed some additional light on 
one other detail from Figure 2: a perceptible jump 
in precision between 28 and 44 training versions 
for Arabic-English and Arabic-French. It should be 
mentioned that among the 16 additional versions 
were five English versions (American Standard 
Version, Basic English Bible, Darby, Webster’s 
Bible, and Young’s Literal Translation), and one 
French version (Louis Segond 1910). It seems that 
Figure 2 and Figure 3 both point to the same thing: 
that the use of parallel versions or translations in a 
single language can be particularly beneficial to 
overall precision within the LSI framework – even 
to a greater extent than the use of parallel transla-
tions in different languages. 

5 Conclusion 

In this paper, we have shown how ‘massive paral-
lelism’ in an aligned corpus can be used to im-
prove the results of cross-language information 
retrieval. Apart from the obvious advantage (the 

ability to automate the processing of a greater vari-
ety of linguistic data within a single framework), 
we have shown that including more parallel trans-
lations in training improves the precision of CLIR 
across the board. This is true whether the addi-
tional translations are in the language of another 
translation already within the training set, whether 
they are in a related language, or whether they are 
in an unrelated language; although this is not to say 
that these choices do not lead to (generally minor) 
variations in the results. The improvement in preci-
sion also appears to hold whether the additional 
translations are complete or incomplete, and it ap-
pears that morphological pre-processing helps, not 
just for the languages pre-processed, but again 
across the board. 

Our work also offers further evidence that the 
supply of useful pre-existing parallel corpora is not 
perhaps as scarce as it is sometimes claimed to be. 
Compilation of the 47-version parallel corpus we 
used was not very time-consuming, especially if 
the time taken to clean the data is not taken into 

878



account, and all the textual material we used is 
publicly available on the World Wide Web. 

While the experiments we performed were on 
non-standard test collections (primarily because 
the Qu’ran was easy to obtain in multiple lan-
guages), it seems that there is no reason to believe 
our general observation – that more parallelism in 
the training data is beneficial for cross-language 
retrieval – would not hold for text from other do-
mains. Whether the genre of text used as training 
data affects the absolute rate of retrieval precision 
for text of a different genre (e.g. news articles, 
shopping websites) is a separate question, and one 
we intend to address more fully in future work. 

In summary, it appears that we are able to 
achieve the results we do partly because of the in-
herent properties of LSI. In essence, when the data 
from more and more parallel translations are sub-
jected to SVD, the LSI ‘concepts’ become more 
and more reinforced. The resulting trend for preci-
sion to increase, despite ‘blips’ for individual lan-
guages, can be seen for all languages. To put it in 
more prosaic terms, the more different ways the 
same things are said in, the more understandable 
they become – including in cross-language infor-
mation retrieval. 

Acknowledgement 

Sandia is a multiprogram laboratory operated by 
Sandia Corporation, a Lockheed Martin Company, 
for the United States Department of Energy’s Na-
tional Nuclear Security Administration under con-
tract DE-AC04-94AL85000. 

References  
Lars Asker. 2004. Building Resources: Experiences 

from Amharic Cross Language Information Re-
trieval. Paper presented at Cross-Language Informa-
tion Retrieval and Evaluation: Workshop of the 
Cross-Language Evaluation Forum, CLEF 2004.

Ricardo Baeza-Yates and Berthier Ribeiro-Neto. 1999. 
Modern Information Retrieval. New York: ACM 
Press. 

Michael Berry, Theresa Do, Gavin O’Brien, Vijay 
Krishna, and Sowmimi Varadhan. 1996. 
SVDPACKC (Version 1.0) User’s Guide. Knoxville, 
TN: University of Tennessee. 

Bible Society. 2006. A Statistical Summary of Lan-
guages with the Scriptures. Accessed at 

http://www.biblesociety.org/latestnews/latest341-
slr2005stats.html on Jan. 5, 2007. 

Biola University. 2005-2006. The Unbound Bible. Ac-
cessed at http://www.unboundbible.com/ on Jan. 5, 
2007. 

Peter Chew, Stephen Verzi, Travis Bauer and Jonathan 
McClain. 2006. Evaluation of the Bible as a Re-
source for Cross-Language Information Retrieval. In 
Proceedings of the Workshop on Multilingual Lan-
guage Resources and Interoperability, 68-74. Syd-
ney: Association for Computational Linguistics. 

Susan Dumais. 1991. Improving the Retrieval of Infor-
mation from External Sources. Behavior Research 
Methods, Instruments, and Computers 23(2):229-
236. 

Julio Gonzalo. 2001. Language Resources in Cross-
Language Text Retrieval: a CLEF Perspective. In 
Carol Peters (ed.). Cross-Language Information Re-
trieval and Evaluation: Workshop of the Cross-
Language Evaluation Forum, CLEF 2000: 36-47. 
Berlin: Springer-Verlag.  

Dragos Munteanu and Daniel Marcu. 2006. Improving 
Machine Translation Performance by Exploiting 
Non-Parallel Corpora. Computational Linguistics 
31(4):477-504. 

Jian-Yun Nie and Fuman Jin. 2002. A Multilingual Ap-
proach to Multilingual Information Retrieval. Pro-
ceedings of the Cross-Language Evaluation Forum,
101-110. Berlin: Springer-Verlag. 

Jian-Yun Nie, Michel Simard, Pierre Isabelle, and Rich-
ard Durand. 1999. Cross-Language Retrieval based 
on Parallel Texts and Automatic Mining of Parallel 
Texts from the Web. Proceedings of the 22nd Annual 
International ACM SIGIR Conference on Research 
and Development in Information Retrieval, 74-81, 
August 15-19, 1999, Berkeley, CA.  

Carol Peters (ed.). 2001. Cross-Language Information 
Retrieval and Evaluation: Workshop of the Cross-
Language Evaluation Forum, CLEF 2000. Berlin: 
Springer-Verlag. 

Recherche appliquée en linguistique informatique 
(RALI). 2006. Corpus aligné bilingue anglais-
français. Accessed at http://rali.iro.umontreal.ca/ on 
February 22, 2006. 

Philip Resnik, Mari Broman Olsen, and Mona Diab. 
1999. The Bible as a Parallel Corpus: Annotating the 
"Book of 2000 Tongues". Computers and the Hu-
manities, 33: 129-153.  

879


