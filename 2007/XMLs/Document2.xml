<document page-count="8">
<page number="1">
<table data-filename="file.pdf" data-page="1" data-table="1"><tr><td></td><td colspan="8">Guiding Statistical Word Alignment Models With Prior Knowledge</td></tr><tr><td></td><td></td><td></td><td colspan="6">Yonggang Deng and Yuqing Gao</td></tr><tr><td></td><td></td><td></td><td colspan="6">IBM T. J. Watson Research Center</td></tr><tr><td></td><td></td><td></td><td></td><td colspan="5">Yorktown Heights, NY 10598</td></tr><tr><td></td><td></td><td></td><td colspan="6">{ydeng,yuqing}@us.ibm.com</td></tr><tr><td></td><td></td><td>Abstract</td><td></td><td></td><td></td><td></td><td colspan="2">most relaxed IBM Model-1, which assumes that any</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="2">source word can be generated by any target word</td></tr><tr><td></td><td colspan="5">We present a general framework to incor-</td><td></td><td colspan="2">equally regardless of distance, can be improved by</td></tr><tr><td></td><td>porate</td><td>prior knowledge</td><td colspan="3">such as heuristics</td><td></td><td colspan="2">demanding a Markov process of alignments as in</td></tr><tr><td></td><td colspan="5">or linguistic features in statistical generative</td><td></td><td colspan="2">HMM-based models (Vogel et al., 1996), or imple-</td></tr><tr><td></td><td colspan="5">word alignment models. Prior knowledge</td><td></td><td colspan="2">menting a distribution of number of target words</td></tr><tr><td></td><td colspan="5">plays a role of probabilistic soft constraints</td><td></td><td colspan="2">linked to a source word as in IBM fertility-based</td></tr><tr><td></td><td colspan="5">between bilingual word pairs that shall be</td><td></td><td colspan="2">models (Brown et al., 1993).</td></tr><tr><td></td><td colspan="5">used to guide word alignment model train-</td><td></td><td></td><td></td></tr><tr><td></td><td colspan="5">ing. We investigate knowledge that can be</td><td></td><td colspan="2">Following the path, we shall put more constraints</td></tr><tr><td></td><td colspan="5">derived automatically from entropy princi-</td><td></td><td colspan="2">on word alignment models and investigate ways of</td></tr><tr><td></td><td colspan="5">ple and bilingual latent semantic analysis</td><td></td><td colspan="2">implementing them in a statistical framework. We</td></tr><tr><td></td><td colspan="5">and show how they can be applied to im-</td><td></td><td colspan="2">have seen examples showing that names tend to</td></tr><tr><td></td><td colspan="3">prove translation performance.</td><td></td><td></td><td></td><td colspan="2">align to names and function words are likely to be</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="2">linked to function words. These observations are</td></tr><tr><td style="text-align: right">1</td><td colspan="2">Introduction</td><td></td><td></td><td></td><td></td><td colspan="2">independent of language and can be understood by</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="2">common sense. Moreover, there are other linguis-</td></tr><tr><td colspan="2">Statistical</td><td>word alignment</td><td>models</td><td>learn</td><td>word</td><td>as-</td><td colspan="2">tically motivated constraints. For instance, words</td></tr><tr><td colspan="2">sociations</td><td>between parallel</td><td colspan="4">sentences from statis-</td><td colspan="2">aligned to each other presumably are semantically</td></tr><tr><td colspan="7">tics. Most models are trained from corpora in an</td><td colspan="2">consistent; and likely to be, they are syntactically</td></tr><tr><td colspan="7">unsupervised manner whose success is heavily de-</td><td colspan="2">agreeable. In these paper, we shall exploit some of</td></tr><tr><td colspan="7">pendent on the quality and quantity of the training</td><td colspan="2">these constraints in building better word alignments</td></tr><tr><td colspan="7">data. It has been shown that human knowledge,</td><td colspan="2">in the application of statistical machine translation.</td></tr><tr><td colspan="7">in the form of a small amount of manually anno-</td><td></td><td></td></tr><tr><td colspan="7">tated parallel data to be used to seed or guide model</td><td>We</td><td>propose a simple framework that can inte-</td></tr><tr><td colspan="7">training, can significantly improve word alignment</td><td>grate</td><td>prior knowledge into statistical word align-</td></tr><tr><td colspan="7">F-measure and translation performance (Ittycheriah</td><td colspan="2">ment model training. In the framework, prior knowl-</td></tr><tr><td colspan="6">and Roukos, 2005; Fraser and Marcu, 2006).</td><td></td><td colspan="2">edge serves as probabilistic soft constraints that will</td></tr><tr><td colspan="7">As formulated in the competitive linking algo-</td><td colspan="2">guide word alignment model training. We present</td></tr><tr><td colspan="7">rithm (Melamed, 2000), the problem of word align-</td><td colspan="2">two types of constraints that are derived in an un-</td></tr><tr><td colspan="7">ment can be regarded as a process of word link-</td><td colspan="2">supervised way: one is based on the entropy prin-</td></tr><tr><td colspan="7">age disambiguation, that is, choosing correct asso-</td><td colspan="2">ciple, the other comes from bilingual latent seman-</td></tr><tr><td colspan="7">ciations among all competing hypothesis. The more</td><td colspan="2">tic analysis. We investigate their impact on word</td></tr><tr><td colspan="7">reasonable constraints are imposed on this process,</td><td colspan="2">alignments and show their effectiveness in improv-</td></tr><tr><td colspan="7">the easier the task would become. For instance, the</td><td colspan="2">ing translation performance.</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td style="text-align: right">1</td><td></td><td></td></tr><tr><td colspan="9">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 1–8,</td></tr><tr><td colspan="5">Prague, Czech Republic, June 2007.</td><td colspan="4">c 2007 Association for Computational Linguistics</td></tr></table></page>
<page number="2">
<table data-filename="file.pdf" data-page="2" data-table="1"><tr><td colspan="10">2 Constrained Word Alignment Models</td><td>f</td><td>f</td></tr><tr><td colspan="10">The framework that we propose to incorporate sta-</td><td>F<br/>E</td><td></td></tr><tr><td colspan="10">tistical constraints into word alignment models is</td><td>e</td><td>e</td></tr><tr><td colspan="10">generic. It can be applied to complicated models</td><td>A</td><td>B</td></tr><tr><td colspan="10">such IBM Model-4 (Brown et al., 1993). We shall</td><td></td><td></td></tr><tr><td colspan="10">take HMM-based word alignment model (Vogel et</td><td colspan="2">Figure 1: A simple table lookup (A) vs. a con-</td></tr><tr><td colspan="10">al., 1996) as an example and follow the notation of</td><td colspan="2">strained procedure (B) of generating a target word</td></tr><tr><td colspan="10">(Brown et al., 1993). Let e = el1 represent a source</td><td colspan="2">f from a source word e.</td></tr><tr><td colspan="10">string and f = f1m a target string. The random vari-</td><td colspan="2"></td></tr><tr><td colspan="10">able a = a1mspecifies the indices of source words</td><td colspan="2"></td></tr><tr><td colspan="6">that target words are aligned to.</td><td></td><td></td><td></td><td></td><td colspan="2">We do not change the value of Con(f, e) during</td></tr><tr><td colspan="10">In an HMM-based word alignment model, source</td><td colspan="2">iterative model training but rather keep it constant as</td></tr><tr><td colspan="2">words are treated</td><td colspan="4">as Markov</td><td colspan="2">states while</td><td colspan="2">target</td><td colspan="2">an indicator of how strong the word pair should be</td></tr><tr><td colspan="4">words are observations</td><td>that</td><td>are</td><td colspan="2">generated</td><td colspan="2">when</td><td colspan="2">considered as a candidate. This information is de-</td></tr><tr><td colspan="2">jumping to states:</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="2">rived before word alignment model training and will</td></tr><tr><td></td><td></td><td></td><td>m</td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="2">act as soft constraints that need to be respected dur-</td></tr><tr><td>P (a, f |e)</td><td>=</td><td colspan="2">Y</td><td>P (a</td><td>|a</td><td>, e)t(f</td><td>|e</td><td style="text-align: right">)</td><td></td><td colspan="2">ing training and alignments. For a given word pair,</td></tr><tr><td></td><td></td><td colspan="2">j=1</td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="2">the soft constraint can have different assignment in</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="2">different sentence pairs since the word tags can be</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="2">context dependent.</td></tr><tr><td colspan="6">Notice that a target word f</td><td colspan="4">is generated from a</td><td colspan="2"></td></tr><tr><td colspan="10">source state e by a simple lookup of the translation</td><td colspan="2">To understand why we take the “detour” of gen-</td></tr><tr><td colspan="10">table, a.k.a., t-table t(f |e), as depicted in (A) of Fig-</td><td colspan="2">erating a target word rather than directly from a t-</td></tr><tr><td colspan="10">ure 1. To incorporate prior knowledge or impose</td><td colspan="2">table, consider the hidden tag as binary value in-</td></tr><tr><td colspan="10">constraints, we introduce two nodes E and F repre-</td><td colspan="2">dicating being a name or not. Without these con-</td></tr><tr><td colspan="10">senting the hidden tags of the source word e and the</td><td colspan="2">straints, t-table entries for names with low frequency</td></tr><tr><td colspan="10">target word f respectively, and organize the depen-</td><td colspan="2">tend to be flat and word alignments can be chosen</td></tr><tr><td colspan="10">dency structure as in (B) of Figure 1. Given this gen-</td><td colspan="2">randomly without sufficient statistics or strong lexi-</td></tr><tr><td colspan="10">erative procedure, f will also depend on its tag F ,</td><td colspan="2">cal preference under maximum likelihood criterion.</td></tr><tr><td colspan="10">which is determined probabilistically by the source</td><td colspan="2">If we assume that a name is produced by a name</td></tr><tr><td colspan="10">tag E. The dependency from E to F functions as a</td><td colspan="2">with a high probability but by a non-name with a</td></tr><tr><td colspan="10">soft constraint showing how the two hidden tags are</td><td colspan="2">low probability, i.e. P (F = E) &gt;&gt; P (F = E),</td></tr><tr><td colspan="10">agreeable to each other. Mathematically, the condi-</td><td colspan="2">proper names with low counts then are encouraged</td></tr><tr><td colspan="5">tional distribution follows:</td><td></td><td></td><td></td><td></td><td></td><td colspan="2">to link to proper names during training; and conse-</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="2">quently, conditional probability mass would be more</td></tr><tr><td>P (f |e)</td><td>=</td><td>X</td><td colspan="4">P (f, E, F |e)</td><td></td><td></td><td></td><td colspan="2">focused on correct name translations. On the other</td></tr><tr><td></td><td></td><td>E,F</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="2">hand, names are discouraged to produce non-names.</td></tr><tr><td></td><td>=</td><td>X</td><td colspan="6">P (E|e)P (F |E)P (f |e, F )</td><td></td><td colspan="2">This will potentially avoid incorrect word associa-</td></tr><tr><td></td><td></td><td>E,F</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="2">tions. We are able to apply this type of constraint</td></tr><tr><td></td><td>=</td><td colspan="5">t(f |e) · Con(f, e),</td><td></td><td></td><td style="text-align: right">(1)</td><td colspan="2">since usually there are many monolingual resources</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="2">available to build a high performance probabilistic</td></tr><tr><td>where</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="2">name tagger. The example suggests that putting rea-</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="2">sonable constraints learned from monolingual analy-</td></tr><tr><td>Con(f, e) =</td><td></td><td colspan="7">P (E|e)P (F |E)P (F |f )/P (F )</td><td style="text-align: right">(2)</td><td colspan="2">sis can alleviate data spareness problem in bilingual</td></tr><tr><td></td><td>E,F</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="2">applications.</td></tr><tr><td colspan="10">is the soft weight attached to the t-table entry. It con-</td><td colspan="2">The weights Con(f, e) are the prior knowledge</td></tr><tr><td colspan="10">siders all possible hidden tags of e and f and serves</td><td colspan="2">that shall be assigned with care but respected dur-</td></tr><tr><td colspan="6">as constraint between the link.</td><td></td><td></td><td></td><td></td><td colspan="2">ing training. The baseline is to set all these weights</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td style="text-align: right">2</td><td colspan="2"></td></tr></table></page>
<page number="3">
<table data-filename="file.pdf" data-page="3" data-table="1"><tr><td colspan="4">to 1, which is equivalent to placing no prior knowl-</td><td colspan="11">approximate the probability of a word as a function</td></tr><tr><td colspan="4">edge on model training. The introduction of these</td><td colspan="11">word with the relative uncertainty of its being ob-</td></tr><tr><td colspan="4">weights does not complicate parameter estimation</td><td colspan="5">served in a sentence.</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td colspan="4">procedure. Whenever a source word e is hypoth-</td><td colspan="7">More specifically, suppose</td><td>we</td><td colspan="3">have N parallel</td></tr><tr><td colspan="4">esized to generate a target word f , the translation</td><td colspan="11">sentences in the training corpus. For each word wi 1 ,</td></tr><tr><td colspan="4">probability t(f |e) should be weighted by Con(f, e).</td><td colspan="11">let cij be the number of word wi observed in the j-th</td></tr><tr><td colspan="4">We point out that the constraints between f and e</td><td colspan="11">sentence pair, and let cibe the total number of oc-</td></tr><tr><td colspan="4">through their hidden tags are in probabilities. There</td><td colspan="11">currences of wi in the corpus. We define the relative</td></tr><tr><td colspan="4">are no hard decisions made before training. A strong</td><td colspan="5">entropy of word wi as</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td colspan="4">preference between two words can be expressed by</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td colspan="3">assigning corresponding weights close to 1.</td><td>This</td><td></td><td></td><td></td><td></td><td style="text-align: right">1</td><td>X</td><td>cij</td><td></td><td>cij</td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td colspan="2">w</td><td>= −</td><td></td><td></td><td></td><td>log</td><td></td><td style="text-align: right">.</td><td></td></tr><tr><td colspan="3">will affect the final alignment model.</td><td></td><td></td><td></td><td>i</td><td></td><td colspan="2">log N</td><td>ci</td><td></td><td>ci</td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>j=1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td colspan="4">Depending on the hidden tags, there are many re-</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td colspan="4">alizations of reasonable constraints that can be put</td><td colspan="11">With the entropy of a word, the likelihood of word</td></tr><tr><td colspan="4">beforehand. They can be semantic classes, syntactic</td><td colspan="11">w being tagged as a function word is approximated</td></tr><tr><td colspan="4">annotations, or as simple as whether being a function</td><td colspan="11">with w (1)= wand being tagged as a content word</td></tr><tr><td colspan="4">word or content word. Moreover, the source side and</td><td colspan="5">with w (0) = 1 − w .</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td colspan="4">the target side do not have to share the same set of</td><td colspan="11">We ignore the denominator in Equ. (2) and find</td></tr><tr><td colspan="4">tags. The framework is also flexible to support mul-</td><td colspan="10">the constraint under the entropy principle:</td><td></td></tr><tr><td colspan="4">tiple types of constraints that can be implemented in</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td colspan="4">parallel or cascaded sequence. Moreover, the con-</td><td colspan="3">Con(f, e)</td><td>=</td><td colspan="4">α(e f  + e</td><td colspan="2">f  ) +</td><td></td></tr><tr><td colspan="4">straints between words can be dependent on context</td><td></td><td></td><td></td><td></td><td colspan="7">(1 − α)(e(1) f (0) + e(0) f (1) ).</td></tr><tr><td colspan="4">within parallel sentences. Next, we will describe</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td colspan="4">two types of constraints that we proposed. Both of</td><td>As</td><td>can</td><td>be</td><td colspan="6">seen, the connection</td><td>between</td><td>two</td></tr><tr><td colspan="4">them are derived from data in an unsupervised way.</td><td colspan="11">words is simulated with a binary symmetric chan-</td></tr><tr><td></td><td></td><td></td><td></td><td colspan="11">nel. An example distribution of the constraint func-</td></tr><tr><td colspan="3">2.1 Entropy Principle</td><td></td><td colspan="7">tion is illustrated in Figure 2.</td><td colspan="4">A high value of α</td></tr><tr><td colspan="4">It is assumed that generally speaking, a source func-</td><td colspan="3">encourages</td><td colspan="8">connecting word pairs with compara-</td></tr><tr><td colspan="4">tion word generates a target function word with a</td><td colspan="11">ble entropy; When α = 0.5, Con(f, e) is constant</td></tr><tr><td colspan="4">higher probability than generating a target content</td><td colspan="11">which corresponds to applying no prior constraint;</td></tr><tr><td colspan="4">word; similar assumption applies to a source con-</td><td colspan="11">When α is close to 0, the function plays opposite</td></tr><tr><td colspan="4">tent word as well. We capture this type of constraint</td><td colspan="11">role on word alignment training where a high fre-</td></tr><tr><td colspan="4">by defining the hidden tag E and F as binary labels</td><td colspan="11">quency word is pushed to associate with a low fre-</td></tr><tr><td colspan="4">indicating being a content word or not. Based on</td><td colspan="3">quency word.</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td colspan="4">the assumption, we design probabilistic relationship</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td style="text-align: right">2.2</td><td colspan="9">Bilingual Latent Semantic Analysis</td><td></td></tr><tr><td colspan="3">between the two hidden tags as:</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td colspan="4">Latent Semantic</td><td colspan="2">Analysis</td><td colspan="2">(LSA)</td><td>is</td><td>a theory</td><td>and</td></tr><tr><td colspan="3">P (E = F ) = 1 − P (E = F ) = α,</td><td></td><td colspan="11">method for extracting and representing the meaning</td></tr><tr><td></td><td></td><td></td><td></td><td colspan="11">of words by statistically analyzing word contextual</td></tr><tr><td colspan="4">where α is a scalar whose value is close to 1, say</td><td colspan="7">usages in a collection of text.</td><td colspan="4">It provides a method</td></tr><tr><td colspan="4">0.9. The bigger α is, the tighter constraint we put on</td><td colspan="11">by which to calculate the similarity of meaning of</td></tr><tr><td colspan="4">word pairs to be connected requiring the same type</td><td colspan="11">given words and documents. LSA has been success-</td></tr><tr><td>of label.</td><td></td><td></td><td></td><td colspan="4">fully applied to</td><td colspan="7">information retrieval (Deerwester</td></tr><tr><td>To determine</td><td>the</td><td>probability</td><td>of a word being</td><td colspan="11">et al., 1990), statistical langauge modeling (Belle-</td></tr><tr><td>a function word,</td><td>we</td><td>apply the</td><td>entropy principle.</td><td colspan="5">garda, 2000) and etc.</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td colspan="4">A function word, say “of”,“in” or “have”, appears</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td colspan="11">1We prefix ‘E ’ to source words and ‘F ’ to target words</td></tr><tr><td colspan="4">more frequently than a content word, say “journal”</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td colspan="11">to distinguish words that have the same spelling but are from</td></tr><tr><td colspan="4">or “chemistry”, in a document or sentence. We will</td><td colspan="4">different languages.</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td style="text-align: right">3</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table></page>
<page number="4">
<table data-filename="file.pdf" data-page="4" data-table="1"><tr><td></td><td></td><td></td><td></td><td>Con(f,e)</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>Con(f,e)</td><td></td><td></td><td></td><td colspan="2"></td><td></td><td colspan="3"></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td colspan="3">alpha=0.9</td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="2">alpha=0.1</td><td></td><td colspan="2"></td><td></td><td colspan="3"></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="6">R×R diagonal matrix of singular values s1</td><td>≥ s2</td><td>≥</td></tr><tr><td style="text-align: right">0.9</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td style="text-align: right">0.9</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="3">. . . ≥ sR  0, and V</td><td colspan="5">is N ×R a right singular ma-</td></tr><tr><td>0.8<br/>0.7</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>0.8<br/>0.7</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="2">trix with rows vj , j</td><td></td><td colspan="5">= 1, · · · , N . For each i, the</td></tr><tr><td style="text-align: right">0.5</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td style="text-align: right">0.6</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="8">scaled R-vector u i S may be viewed as representing</td></tr><tr><td style="text-align: right">00..43</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>0.5<br/>0.4</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="8">wi , the i-th word in the vocabulary, and similarly the</td></tr><tr><td style="text-align: right">00..21</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>0.18<br/>0.6 (0)</td><td style="text-align: right">00..32</td><td></td><td></td><td></td><td></td><td></td><td></td><td>0.18<br/>0.6 (0)</td><td colspan="8">scaled R-vector vj S as representing d j , j-th docu-</td></tr><tr><td style="text-align: right">0</td><td style="text-align: right">0</td><td style="text-align: right">0.2</td><td style="text-align: right">0.4</td><td style="text-align: right">0.6</td><td style="text-align: right">0.8</td><td style="text-align: right">1</td><td style="text-align: right">0</td><td>f<br/>0.02.4</td><td style="text-align: right">0.1</td><td style="text-align: right">0</td><td style="text-align: right">0.2</td><td style="text-align: right">0.4</td><td style="text-align: right">0.6</td><td style="text-align: right">0.8</td><td style="text-align: right">1</td><td>f<br/>0.02.4<br/>0</td><td colspan="2">ment in the corpus.</td><td colspan="6">Note that the u i S’s and vj S’s</td></tr><tr><td></td><td></td><td></td><td>e(0)</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>e(0)</td><td></td><td></td><td></td><td></td><td colspan="2"></td><td></td><td colspan="3"></td><td></td><td></td></tr><tr><td colspan="25">R</td></tr><tr><td colspan="12"></td><td></td><td colspan="3"></td><td colspan="3">both belong to IR</td><td colspan="6">, the so-called LSA-space. All</td></tr><tr><td colspan="16">Figure 2: Distribution of the constraint function</td><td colspan="9">target and source words are projected into the same</td></tr><tr><td colspan="12">based on entropy principle when α</td><td>=</td><td colspan="3">0.9 on the</td><td colspan="3">LSA-space too.</td><td></td><td></td><td></td><td></td><td colspan="2"></td></tr><tr><td colspan="12">left and α = 0.1 on the right.</td><td></td><td colspan="3"></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="2"></td></tr><tr><td colspan="12"></td><td></td><td colspan="3"></td><td></td><td>Documents</td><td></td><td colspan="3">R orthonormal vectors</td><td></td><td colspan="2"></td></tr><tr><td colspan="12"></td><td></td><td colspan="3"></td><td></td><td>d 1</td><td>dN</td><td></td><td></td><td></td><td></td><td colspan="2"></td></tr><tr><td colspan="12"></td><td></td><td colspan="3"></td><td>w1</td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="2"></td></tr><tr><td colspan="16">We explore LSA techniques in bilingual environ-</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="2"></td></tr><tr><td colspan="16">ment to derive semantic constraints as prior knowl-</td><td></td><td></td><td></td><td></td><td>R</td><td>R</td><td>R</td><td colspan="2">N</td></tr><tr><td colspan="16">edge for guiding a word alignment model train-</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="2"></td></tr><tr><td colspan="16">ing. The idea is to find semantic representation of</td><td>w M</td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="2"></td></tr><tr><td colspan="16">source words and target words in the so-called low-</td><td></td><td>M × N</td><td></td><td>M × R</td><td></td><td></td><td></td><td colspan="2"></td></tr><tr><td colspan="16">dimensional LSA-space, and then to use their sim-</td><td></td><td>W</td><td></td><td>U</td><td></td><td>S</td><td colspan="3">V</td></tr><tr><td colspan="16">ilarities to quantitatively establish semantic consis-</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="2"></td></tr><tr><td colspan="16">tencies. We propose two different approaches.</td><td></td><td colspan="8">Figure 3: SVD of the Sparse Matrix W .</td></tr><tr><td colspan="14">2.2.1 A Simple Bag-of-word Model</td><td colspan="2"></td><td colspan="9">As Equ. (2) suggested, to induce semantic con-</td></tr><tr><td colspan="16">One method we investigate is a simple bag-of-</td><td colspan="9">straints in a straightforward way, one would proceed</td></tr><tr><td colspan="16">word model as in monolingual LSA. We treat each</td><td colspan="9">as follows: firstly, perform word semantic cluster-</td></tr><tr><td colspan="3">sentence</td><td colspan="5">pair as a</td><td colspan="8">document and do not distin-</td><td colspan="9">ing with, say, their compact representations in the</td></tr><tr><td colspan="10">guish source words and target</td><td colspan="6">words as if they</td><td colspan="9">LSA-space; secondly, construct cluster generating</td></tr><tr><td colspan="16">are terms generated from the same vocabulary. A</td><td colspan="9">dependencies by specifying the conditional distribu-</td></tr><tr><td colspan="16">sparse matrix W characterizing word-document co-</td><td colspan="9">tion of P (F |E); and finally, for each word pair, in-</td></tr><tr><td colspan="16">occurrence is constructed. Following the notation in</td><td colspan="9">duce the semantic constraint by considering all pos-</td></tr><tr><td colspan="14">section 2.1, the ij-th entry of the matrix W</td><td colspan="2">is de-</td><td colspan="9">sible semantic labeling schemes. We approximate</td></tr><tr><td colspan="10">fined as in (Bellegarda, 2000)</td><td></td><td colspan="3"></td><td colspan="2"></td><td colspan="9">this long process with simply finding word similar-</td></tr><tr><td colspan="3"></td><td colspan="5"></td><td colspan="2"></td><td></td><td colspan="3"></td><td colspan="2"></td><td colspan="9">ities defined by their cosine distance in the low di-</td></tr><tr><td colspan="3"></td><td colspan="5"></td><td colspan="2"></td><td>ij</td><td colspan="3"></td><td colspan="2"></td><td colspan="2">mension space:</td><td></td><td colspan="4"></td><td colspan="2"></td></tr><tr><td colspan="3"></td><td colspan="5">W ij</td><td colspan="2">= (1 − w i )</td><td></td><td style="text-align: right" colspan="3">,</td><td colspan="2"></td><td colspan="2"></td><td></td><td colspan="4"></td><td colspan="2"></td></tr><tr><td colspan="3"></td><td colspan="5"></td><td colspan="2"></td><td>cj</td><td colspan="3"></td><td colspan="2"></td><td colspan="2"></td><td></td><td colspan="4"></td><td colspan="2"></td></tr><tr><td colspan="3"></td><td colspan="5"></td><td colspan="2"></td><td></td><td colspan="3"></td><td colspan="2"></td><td colspan="2"></td><td style="text-align: right">1</td><td colspan="4"></td><td colspan="2"></td></tr><tr><td colspan="3">where cj</td><td colspan="13">is the total number of words in the j-th</td><td colspan="2">Con(f, e) =</td><td></td><td colspan="4">(cos(u f S, u e S) + 1)</td><td style="text-align: right" colspan="2">(3)</td></tr><tr><td colspan="16">sentence pair. This construction considers the im-</td><td colspan="2"></td><td style="text-align: right">2</td><td colspan="4"></td><td colspan="2"></td></tr><tr><td colspan="16">portance of words globally (corpus wide) and locally</td><td colspan="9">The linear mapping above is introduced to avoid</td></tr><tr><td colspan="16">(within sentence pairs). Alternative constructions of</td><td colspan="9">negative constraints and to set the maximum con-</td></tr><tr><td colspan="16">the matrix are possible using raw counts or TF-IDF</td><td colspan="2">straint value as 1.</td><td></td><td colspan="4"></td><td colspan="2"></td></tr><tr><td colspan="10">(Deerwester et al., 1990).</td><td></td><td colspan="3"></td><td colspan="2"></td><td colspan="3">In building word</td><td colspan="6">alignment models, a special</td></tr><tr><td colspan="8">W is a M × N</td><td colspan="6">sparse matrix, where M</td><td colspan="2">is the</td><td colspan="9">“NULL” word is usually introduced to address tar-</td></tr><tr><td colspan="16">size of vocabulary including both source and target</td><td colspan="9">get words that align to no source words. Since this</td></tr><tr><td colspan="16">words. To obtain a compact representation, singular</td><td colspan="9">physically non-existing word is not in the vocabu-</td></tr><tr><td colspan="16">value decomposition (SVD) is employed (cf. Berry</td><td colspan="9">lary of the bilingual LSA, we use the centroid of all</td></tr><tr><td colspan="10">et al (1993)) to yield W ≈ Ŵ</td><td colspan="6">= U × S × V T</td><td colspan="9">source words as its vector representation in the LSA-</td></tr><tr><td colspan="16">as Figure 3 shows, where, for some order R</td><td colspan="9">space. The semantic constraints between “NULL”</td></tr><tr><td colspan="16">min(M, N ) of the decomposition, U is a M ×R left</td><td colspan="9">and any target words can be derived in the same way.</td></tr><tr><td colspan="16">singular matrix with rows u i , i = 1, · · · , M , S is a</td><td colspan="9">However, this is chosen for mostly computational</td></tr><tr><td colspan="3"></td><td colspan="5"></td><td colspan="2"></td><td></td><td colspan="3"></td><td style="text-align: right" colspan="2">4</td><td colspan="2"></td><td></td><td colspan="4"></td><td colspan="2"></td></tr></table></page>
<page number="5">
<table data-filename="file.pdf" data-page="5" data-table="1"><tr><td colspan="6">convenience, and is not the only way to address the</td><td colspan="7">etc. To avoid impacts of speech recognition errors,</td></tr><tr><td colspan="3">empty word issue.</td><td></td><td></td><td></td><td colspan="7">we only report experiments from text to text transla-</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td>tion.</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td style="text-align: right">2.2.2</td><td colspan="4">Utilizing Word Alignment Statistics</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td>The</td><td>training</td><td>corpus</td><td>consists</td><td>of</td><td>390K</td><td>sentence</td></tr><tr><td colspan="5">While the simple bag-of-word model puts</td><td>all</td><td colspan="7">pairs, with total 2.43M Arabic words and 3.38M En-</td></tr><tr><td colspan="6">source words and target words as rows in the ma-</td><td colspan="7">glish words. These sentences are in typical spoken</td></tr><tr><td colspan="6">trix, another method of deriving semantic constraint</td><td colspan="7">transcription form, i.e., spelling errors, disfluencies,</td></tr><tr><td colspan="6">constructs the sparse matrix by taking source words</td><td colspan="7">such as word or phrase repetition, and ungrammat-</td></tr><tr><td colspan="6">as rows and target words as columns and uses statis-</td><td colspan="7">ical utterances are commonly observed. Arabic ut-</td></tr><tr><td colspan="6">tics from word alignment training to form word pair</td><td colspan="7">terance length ranges from 3 to 70 words with the</td></tr><tr><td colspan="3">co-occurrence association.</td><td></td><td></td><td></td><td colspan="3">average of 6 words.</td><td></td><td></td><td></td><td></td></tr><tr><td colspan="6">More specifically, we regard each target word f as</td><td colspan="7">There are 25K entries in the English vocabulary</td></tr><tr><td colspan="6">a “document” and each source word e as a “term”.</td><td colspan="7">and 90K in Arabic side. Data sparseness severely</td></tr><tr><td colspan="6">The number of occurrences of the source word e in</td><td colspan="7">challenges word alignment model and consequently</td></tr><tr><td>the document f</td><td colspan="5">is defined as the expected number</td><td colspan="7">automatic phrase translation induction. There are</td></tr><tr><td>of times that f</td><td colspan="5">generates e in the parallel corpus</td><td colspan="7">42K singletons in Arabic vocabulary, and 14K Ara-</td></tr><tr><td colspan="6">under the word alignment model. This method re-</td><td colspan="7">bic words with occurrence of twice each in the cor-</td></tr><tr><td colspan="6">quires training the baseline word alignment model</td><td colspan="7">pus. Since Arabic is a morphologically rich lan-</td></tr><tr><td colspan="6">in another direction by taking f s as source words</td><td colspan="7">guage where affixes are attached to stem words to</td></tr><tr><td colspan="2">and es as target</td><td colspan="3">words, which is often done</td><td>for</td><td colspan="7">indicate gender, tense, case and etc, in order to re-</td></tr><tr><td colspan="6">symmetric alignments, and then dumping out the</td><td colspan="7">duce vocabulary size and address out-of-vocabulary</td></tr><tr><td colspan="4">soft counts when model converges.</td><td colspan="2">We threshold</td><td colspan="7">words, we split Arabic words into affix and root ac-</td></tr><tr><td colspan="6">the minimum word-to-word translation probability</td><td colspan="7">cording to a rule-based segmentation scheme (Xiang</td></tr><tr><td colspan="6">to remove word pairs that have low co-occurrence</td><td colspan="7">et al., 2006) with the help from the Buckwalter ana-</td></tr><tr><td>counts.</td><td></td><td></td><td></td><td></td><td></td><td colspan="3">lyzer (LDC, 2002) output.</td><td colspan="4">This reduces the size of</td></tr><tr><td colspan="6">Following the similarity induced semantic con-</td><td colspan="3">Arabic vocabulary to 52K.</td><td></td><td></td><td></td><td></td></tr><tr><td colspan="6">straints in section 2.2.1, we need to find the distance</td><td>Our</td><td colspan="2">test data consists</td><td colspan="2">of 1294</td><td colspan="2">sentence pairs.</td></tr><tr><td colspan="6">between a term and a document. Let vfbe the pro-</td><td colspan="7">They are split into two parts: half of them is used as</td></tr><tr><td colspan="6">jection of the document representing the target word</td><td colspan="7">the development set, on which training parameters</td></tr><tr><td colspan="6">f and u e the projection of the term representing the</td><td colspan="7">and decoding feature weights are tuned, the other</td></tr><tr><td colspan="6">source word e after performing SVD on the sparse</td><td colspan="2">half is for test.</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td colspan="6">matrix, we calculate the similarity between (f, e)</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td colspan="5">and then find their semantic constraint to be</td><td></td><td colspan="6">3.1 Training and Translation Setup</td><td></td></tr><tr><td></td><td style="text-align: right">1</td><td></td><td></td><td></td><td></td><td colspan="7">Starting from the collection of parallel training sen-</td></tr><tr><td>Con(f, e) =</td><td style="text-align: right">2</td><td>(cos(vf S</td><td>, u e S</td><td style="text-align: right">) + 1)</td><td style="text-align: right">(4)</td><td colspan="7">tences, we train word alignment models in two trans-</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="7">lation directions, from English to Iraqi Arabic and</td></tr><tr><td colspan="6">Unlike the method in section 2.2.1, there is no</td><td colspan="7">from Iraqi Arabic to English, and derive two sets</td></tr><tr><td colspan="6">empty word issue here since we do have statistics</td><td colspan="3">of Viterbi alignments.</td><td colspan="4">By combining word align-</td></tr><tr><td colspan="6">of the “NULL” word as a source word generating e</td><td colspan="7">ments in two directions using heuristics (Och and</td></tr><tr><td colspan="6">words and therefore there is a “document” assigned</td><td colspan="7">Ney, 2003), a single set of static word alignments</td></tr><tr><td>to it.</td><td></td><td></td><td></td><td></td><td></td><td colspan="2">is then formed.</td><td colspan="5">All phrase pairs which respect to</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="7">the word alignment boundary constraint are iden-</td></tr><tr><td style="text-align: right">3</td><td colspan="3">Experimental Results</td><td></td><td></td><td colspan="7">tified and pooled to build phrase translation tables</td></tr><tr><td colspan="6">We test our framework on the task of large vocab-</td><td colspan="7">with the Maximum Likelihood criterion. We prune</td></tr><tr><td colspan="6">ulary translation from dialectical (Iraqi) Arabic ut-</td><td colspan="7">phrase translation entries by their probabilities. The</td></tr><tr><td colspan="6">terances into English. The task covers multiple do-</td><td colspan="7">maximum number of tokens in Arabic phrases is set</td></tr><tr><td colspan="6">mains including travel, emergency medical diagno-</td><td colspan="3">to 5 for all conditions.</td><td></td><td></td><td></td><td></td></tr><tr><td colspan="6">sis, defense-oriented force protection, security and</td><td colspan="7">Our decoder is a phrase-based multi-stack imple-</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td style="text-align: right">5</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table></page>
<page number="6">
<table data-filename="file.pdf" data-page="6" data-table="1"><tr><td>mentation of the log-linear model similar to Pharaoh</td><td colspan="4">ing. The semantic constraints are used to guide word</td></tr><tr><td>(Koehn et al., 2003). Like other log-linear model</td><td colspan="4">alignment model training for each iteration. The</td></tr><tr><td>based decoders, active features in our translation en-</td><td colspan="4">BLEU score and TER with this constraint are shown</td></tr><tr><td>gine include translation models in two directions,</td><td colspan="3">in the line “BiLSA-1” of Table 1.</td><td></td></tr><tr><td>lexicon weights in two directions, language model,</td><td colspan="4">To exploit word alignment statistics in bilingual</td></tr><tr><td>distortion model, and sentence length penalty. These</td><td colspan="4">LSA as described in Section 2.2.2, we dump out the</td></tr><tr><td>feature weights are tuned on the dev set to achieve</td><td colspan="4">statistics of the baseline word alignment model and</td></tr><tr><td>optimal translation performance using downhill sim-</td><td colspan="4">use them to construct the sparse matrix. We find</td></tr><tr><td>plex method (Och and Ney, 2002). The language</td><td colspan="4">low-dimensional representation (R = 67) of English</td></tr><tr><td>model is a statistical trigram model estimated with</td><td colspan="4">words and Arabic words and use their similarity to</td></tr><tr><td>Modified Kneser-Ney smoothing (Chen and Good-</td><td colspan="4">establish semantic constraints as in Equ. 4. The</td></tr><tr><td>man, 1996) using all English sentences in the paral-</td><td colspan="4">training procedure is the same as the baseline and</td></tr><tr><td>lel training data.</td><td colspan="4">“BiLSA-1”. The translation results with these word</td></tr><tr><td>We measure translation performance by the</td><td colspan="4">alignments are shown as “BiLSA-2” in Table 1.</td></tr><tr><td>BLEU score (Papineni et al., 2002) and Translation</td><td colspan="4">As Table 1 shows, when the entropy based con-</td></tr><tr><td>Error Rate (TER) (Snover et al., 2006) with one ref-</td><td colspan="4">straints are applied, BLEU score improves 0.5 point</td></tr><tr><td>erence for each hypothesis. Word alignment mod-</td><td>on the test set.</td><td colspan="3">Clearly, when bilingual LSA con-</td></tr><tr><td>els trained with different constraints are compared</td><td colspan="4">straints are applied, translation performance can be</td></tr><tr><td>to show their effects on the resulting phrase transla-</td><td colspan="4">improved up to 1.6 BLEU points. We also observe</td></tr><tr><td>tion tables and the final translation performance.</td><td colspan="4">that TER can drop 2.1 points with the “BiLSA-1”</td></tr><tr><td></td><td>constraint.</td><td></td><td></td><td></td></tr><tr><td>3.2 Translation Results</td><td colspan="4">While “BiLSA-1” constraint performs better on</td></tr><tr><td>Our baseline word alignment model is the word-to-</td><td colspan="4">the test set, “BiLSA-2” constraint achieves slightly</td></tr><tr><td>word Hidden Markov Model (Vogel et al., 1996).</td><td>higher BLEU</td><td>score on the</td><td>dev set.</td><td>We then</td></tr><tr><td>Basic models in two translation directions are</td><td>try a simple</td><td>combination</td><td>of these</td><td>two types</td></tr><tr><td>trained simultaneously where statistics of two direc-</td><td>of constraints,</td><td>that is the</td><td>geometric</td><td>mean of</td></tr><tr><td>tions are shared to learn symmetric translation lexi-</td><td colspan="4">Con BiLSA−1 (f, e) and Con BiLSA−2 (f, e), and find</td></tr><tr><td>con and word alignments with high precision moti-</td><td colspan="4">out that BLEU score can be improved a little bit fur-</td></tr><tr><td>vated by (Zens et al., 2004) and (Liang et al., 2006).</td><td colspan="4">ther on both sets as the line “Mix” shows.</td></tr><tr><td>The baseline translation results (BLEU and TER) on</td><td colspan="4">We notice that the relatively simpler HMM model</td></tr><tr><td>the dev and test set are presented in the line “HMM”</td><td colspan="4">can perform comparable or better than the sophis-</td></tr><tr><td>of Table 1. We also compare with results of IBM</td><td colspan="4">ticated Model-4 when proper constraints are active</td></tr><tr><td>Model-4 word alignments implemented in GIZA++</td><td colspan="4">in guiding word alignment model training. We also</td></tr><tr><td>toolkit (Och and Ney, 2003).</td><td colspan="4">try to put constraints in Model-4. As the Equation</td></tr><tr><td>We study and compare two types of constraint and</td><td colspan="4">1 implies, when a word-to-word generative proba-</td></tr><tr><td>see how they affect word alignments and translation</td><td colspan="4">bility is needed, one should multiply corresponding</td></tr><tr><td>output. One is based on the entropy principle as de-</td><td colspan="4">lexicon entry in the t-table with the word pair con-</td></tr><tr><td>scribed in Section 2.1, where α is set to 0.9; The</td><td colspan="4">straint. We simply modify the GIZA++ toolkit (Och</td></tr><tr><td>other is based on bilingual latent semantic analysis.</td><td colspan="4">and Ney, 2003) by always weighting lexicon proba-</td></tr><tr><td>For the simple bag-of-word bilingual LSA as de-</td><td colspan="4">bilities with soft constraints during iterative model</td></tr><tr><td>scribed in Section 2.2.1, after SVD on the sparse ma-</td><td colspan="4">training, and obtain 0.7% TER reduction on both</td></tr><tr><td>trix using the toolkit SVDPACK (Berry et al., 1993),</td><td colspan="4">sets and 0.4% BLEU improvement on the test set.</td></tr><tr><td>all source and target words are projected into a low-</td><td></td><td></td><td></td><td></td></tr><tr><td>dimensional (R = 88) LSA-space. Word pair se-</td><td>3.3 Analysis</td><td></td><td></td><td></td></tr><tr><td>mantic constrains are calculated based on their sim-</td><td colspan="4">To understand how prior knowledge encoded as soft</td></tr><tr><td>ilarity as in Equ. 3 before word alignment training.</td><td colspan="4">constraints plays a role in guiding word alignment</td></tr><tr><td>Like the baseline, we perform 6 iterations of IBM</td><td colspan="4">training, we compare statistics of different word</td></tr><tr><td>Model-1 training and then 4 iteration of HMM train-</td><td colspan="4">alignment models. We find that our baseline HMM</td></tr><tr><td style="text-align: right">6</td><td></td><td></td><td></td><td></td></tr></table></page>
<page number="7">
<table data-filename="file.pdf" data-page="7" data-table="1"><tr><td>Table</td><td>1:</td><td>Translation</td><td>Results</td><td>with</td><td>different</td><td>word</td><td colspan="6">Table 2: Word pair constraint values</td></tr><tr><td colspan="3">alignments.</td><td></td><td></td><td></td><td></td><td>English e</td><td>Arabic f</td><td colspan="4">Con BiLSA−1 (f, e)</td></tr><tr><td colspan="2">Alignments</td><td colspan="2">BLEU</td><td></td><td>TER</td><td></td><td colspan="2">esophagus</td><td>mrM</td><td style="text-align: right" colspan="3">0.6424</td></tr><tr><td colspan="2"></td><td>dev</td><td>test</td><td>dev</td><td></td><td>test</td><td></td><td></td><td>mAl</td><td style="text-align: right" colspan="3">0.1819</td></tr><tr><td colspan="2">Model-4</td><td style="text-align: right">0.310</td><td style="text-align: right">0.296</td><td style="text-align: right">0.528</td><td style="text-align: right" colspan="2">0.530</td><td></td><td></td><td>tk</td><td style="text-align: right" colspan="3">0.2897</td></tr><tr><td colspan="2">+Mix</td><td style="text-align: right">0.306</td><td style="text-align: right">0.300</td><td style="text-align: right">0.521</td><td style="text-align: right" colspan="2">0.523</td><td></td><td>your</td><td>mrM</td><td style="text-align: right" colspan="3">0.6319</td></tr><tr><td colspan="2">HMM<br/>+Entropy</td><td>0.289<br/>0.289</td><td>0.288<br/>0.293</td><td>0.543<br/>0.534</td><td colspan="2">0.542<br/>0.536</td><td></td><td></td><td>mAl<br/>tk</td><td colspan="3">0.4930<br/>0.9672</td></tr><tr><td colspan="2">+BiLSA-1<br/>+BiLSA-2<br/>+Mix</td><td>0.294<br/>0.298<br/>0.302</td><td>0.300<br/>0.292<br/>0.304</td><td>0.531<br/>0.530<br/>0.532</td><td colspan="2">0.521<br/>0.528<br/>0.524</td><td></td><td></td><td></td><td colspan="3"></td></tr><tr><td colspan="2"></td><td></td><td></td><td></td><td></td><td></td><td>“mrM”.</td><td></td><td></td><td colspan="3"></td></tr><tr><td colspan="2"></td><td></td><td></td><td></td><td></td><td></td><td colspan="6">4 Related Work</td></tr><tr><td colspan="7">generates 2.6% less number of total word links than</td><td colspan="6"></td></tr><tr><td colspan="7">that of Model-4. Part of the reason is that mod-</td><td colspan="6">Heuristics based on co-occurrence analysis, such as</td></tr><tr><td colspan="7">els of two directions in the baseline are trained si-</td><td colspan="6">point-wise mutual information or Dice coefficients</td></tr><tr><td colspan="7">multaneously. The requirement of bi-directional ev-</td><td colspan="6">, have been shown to be indicative for word align-</td></tr><tr><td colspan="5">idence places a certain constraint</td><td colspan="2">on word align-</td><td colspan="6">ments (Zhang and Vogel, 2005; Melamed, 2000).</td></tr><tr><td colspan="7">ments. When “BiLSA-1” constraints are applied in</td><td colspan="6">The framework presented in this paper demonstrates</td></tr><tr><td colspan="7">the baseline model, 2.7% less number of total word</td><td colspan="6">the possibility of taking heuristics as constraints</td></tr><tr><td colspan="7">links are hypothesized, and consequently, less num-</td><td colspan="6">guiding statistical generative word alignment model</td></tr><tr><td colspan="7">ber of Arabic n-gram translations in the final phrase</td><td colspan="6">training. Their effectiveness can be expected espe-</td></tr><tr><td colspan="4">translation table are induced.</td><td colspan="3">The observation sug-</td><td colspan="6">cially when data sparseness is severe.</td></tr><tr><td colspan="7">gests that the constraints improve word alignment</td><td colspan="6">Discriminative word alignment models, such as</td></tr><tr><td colspan="7">precision and accuracy of phrase translation tables</td><td colspan="6">Ittycheriah and Roukos (2005); Moore (2005);</td></tr><tr><td colspan="2">as well.</td><td></td><td></td><td></td><td></td><td></td><td colspan="6">Blunsom and Cohn (2006), have received great</td></tr><tr><td colspan="2"></td><td></td><td></td><td></td><td></td><td></td><td colspan="6">amount of study recently. They have proven that lin-</td></tr><tr><td colspan="2">gloss</td><td>(in)</td><td>(esophagus)</td><td colspan="2">(ownership)</td><td>(yours)</td><td colspan="6"></td></tr><tr><td colspan="2"></td><td>bAl_</td><td>mrM</td><td>mAl</td><td></td><td>_tk</td><td colspan="6">guistic knowledge is useful in modeling word align-</td></tr><tr><td colspan="2">HMM</td><td></td><td></td><td></td><td></td><td></td><td colspan="6">ments under log-linear distributions as morphologi-</td></tr><tr><td colspan="2"></td><td>in</td><td>your</td><td></td><td colspan="2">esophagus</td><td colspan="6"></td></tr><tr><td colspan="2"></td><td></td><td></td><td></td><td></td><td></td><td colspan="6">cal, semantic or syntactic features. Our framework</td></tr><tr><td colspan="2"></td><td>bAl_</td><td>mrM</td><td>mAl</td><td>_tk</td><td colspan="7">proposes to exploit these features differently by tak-</td></tr><tr><td colspan="2">+BiLSA-1</td><td></td><td></td><td></td><td></td><td colspan="7"></td></tr><tr><td colspan="2"></td><td></td><td></td><td></td><td></td><td colspan="7">ing them as soft constraints of translation lexicon un-</td></tr><tr><td colspan="2"></td><td>in</td><td>your</td><td></td><td>esophagus</td><td colspan="7"></td></tr><tr><td colspan="13">der a generative model.</td></tr><tr><td colspan="2"></td><td>bAl_</td><td>mrM</td><td>mAl</td><td>_tk</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td colspan="2">Model-4</td><td></td><td></td><td></td><td></td><td colspan="7">While word alignments can help identifying se-</td></tr><tr><td colspan="2"></td><td>in</td><td>your</td><td></td><td>esophagus</td><td>mantic</td><td>relations</td><td>(van</td><td>der</td><td>Plas</td><td>and</td><td>Tiedemann,</td></tr><tr><td colspan="7"></td><td colspan="6">2006), we proceed in the reverse direction. We in-</td></tr><tr><td colspan="7">Figure 4: An example of word alignments under dif-</td><td colspan="6">vestigate the impact of semantic constraints on sta-</td></tr><tr><td colspan="7">ferent models</td><td colspan="6">tistical word alignment models as prior knowledge.</td></tr><tr><td colspan="7"></td><td colspan="6">In (Ma et al., 2004), bilingual semantic maps are</td></tr><tr><td colspan="7">Figure 4 shows example word alignments of a par-</td><td colspan="6">constructed to guide word alignment. The frame-</td></tr><tr><td colspan="7">tial sentence pair. The complete English sentence is</td><td colspan="6">work we proposed seamlessly integrates derived se-</td></tr><tr><td colspan="7">“have you ever had like any reflux diseases in your</td><td colspan="6">mantic similarities into a statistical word alignment</td></tr><tr><td colspan="7">esophagus”. We notice that the Arabic word “mrM”</td><td colspan="6">model. And we extended monolingual latent seman-</td></tr><tr><td colspan="7">(means esophagus) appears only once in the corpus.</td><td colspan="6">tic analysis in bilingual applications.</td></tr><tr><td colspan="7">Some of the word pair constraints are listed in Ta-</td><td colspan="6">Toutanova et al. (2002) augmented bilingual sen-</td></tr><tr><td colspan="7">ble 2. The example demos that due to reasonable</td><td colspan="6">tence pairs with part-of-speech tags as linguistic</td></tr><tr><td colspan="7">constraints placed in word alignment training, the</td><td colspan="6">constraints for HMM-based word alignments. The</td></tr><tr><td colspan="7">link to “ tK” is corrected and consequently we have</td><td colspan="6">constraints between tags are automatically learned</td></tr><tr><td colspan="7">accurate word translation for the Arabic singleton</td><td colspan="6">in a parallel generative procedure along with lex-</td></tr><tr><td style="text-align: right" colspan="7">7</td><td colspan="6"></td></tr></table></page>
<page number="8">
<table data-filename="file.pdf" data-page="8" data-table="1"><tr><td colspan="4">icon. We have introduced hidden tags between a</td><td colspan="6">A. Fraser and D. Marcu. 2006. Semi-supervised training for</td></tr><tr><td colspan="4">word pair to specialize their soft constraints, which</td><td></td><td colspan="5">statistical word alignment. In Proc. of COLING/ACL, pages</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td>769–776.</td><td></td><td></td><td></td><td></td></tr><tr><td colspan="4">serve as prior knowledge that will be used in guiding</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td colspan="4">word alignment model training. Constraint between</td><td colspan="6">A. Ittycheriah and S. Roukos. 2005. A maximum entropy word</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td colspan="3">aligner for arabic-english machine translation.</td><td colspan="2">In Proc. of</td></tr><tr><td colspan="4">tags are embedded into the word to word generative</td><td></td><td colspan="3">HLT/EMNLP, pages 89–96.</td><td></td><td></td></tr><tr><td>process.</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td colspan="6">P. Koehn, F. Och, and D. Marcu. 2003. Statistical phrase-based</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td colspan="3">translation. In Proc. of HLT-NAACL.</td><td></td><td></td></tr><tr><td style="text-align: right">5</td><td colspan="2">Conclusions and Future Work</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td colspan="6">LDC, 2002. Buckwalter Arabic Morphological Analyzer Ver-</td></tr><tr><td colspan="4">We have presented a simple and effective framework</td><td></td><td colspan="3">sion 1.0. LDC Catalog Number LDC2002L49.</td><td></td><td></td></tr><tr><td colspan="4">to incorporate prior knowledge such as heuristics</td><td colspan="6">P. Liang, B. Taskar, and D. Klein. 2006. Alignment by agree-</td></tr><tr><td colspan="4">or linguistic features into statistical generative word</td><td></td><td colspan="3">ment. In Proc. of HLT/NAACL, pages 104–111.</td><td></td><td></td></tr><tr><td colspan="4">alignment models. Prior knowledge serves as soft</td><td>Q.</td><td>Ma, K. Kanzaki,</td><td>Y.</td><td>Zhang, M. Murata, and</td><td>H.</td><td>Isahara.</td></tr><tr><td colspan="4">constraints that shall be placed on translation lexi-</td><td></td><td colspan="5">2004. Self-organizing semantic maps and its application to</td></tr><tr><td colspan="4">con to guide word alignment model training and dis-</td><td></td><td colspan="5">word alignment in japanese-chinese parallel corpora. Neural</td></tr><tr><td colspan="4">ambiguation during Viterbi alignment process. We</td><td></td><td colspan="3">Netw., 17(8-9):1241–1253.</td><td></td><td></td></tr><tr><td colspan="4">studied two types of constraints that can be obtained</td><td colspan="6">I. Dan. Melamed. 2000. Models of translational equivalence</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td colspan="5">among words. Computational Linguistics, 26(2):221–249.</td></tr><tr><td colspan="4">automatically from data and showed improved per-</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td colspan="4">formance (up to 1.6% absolute BLEU increase or</td><td colspan="6">R. C. Moore. 2005. A discriminative framework for bilingual</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td colspan="4">word alignment. In Proc. of HLT/EMNLP, pages 81–88.</td><td></td></tr><tr><td colspan="4">2.1% absolute TER reduction) in translating dialec-</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td colspan="4">tical Arabic into English. Future work includes im-</td><td colspan="2">F. J. Och and H. Ney.</td><td style="text-align: right">2002.</td><td colspan="3">Discriminative training and max-</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td colspan="4">imum entropy models for statistical machine translation.</td><td>In</td></tr><tr><td colspan="4">plementing the idea in alternative alignment mod-</td><td></td><td colspan="3">Proc. of ACL, pages 295–302.</td><td></td><td></td></tr><tr><td colspan="4">els and also exploiting prior knowledge derived from</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td colspan="2">F. J. Och and H. Ney.</td><td style="text-align: right">2003.</td><td colspan="3">A systematic comparison of vari-</td></tr><tr><td colspan="4">such as manually-aligned data and pre-existing lin-</td><td></td><td colspan="5">ous statistical alignment models. Computational Linguistics,</td></tr><tr><td colspan="2">guistic resources.</td><td></td><td></td><td></td><td>29(1):19–51.</td><td></td><td></td><td></td><td></td></tr><tr><td colspan="4">Acknowledgement We thank Mohamed Afify for</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td colspan="4">K. Papineni, S. Roukos, T. Ward, and W. Zhu.</td><td style="text-align: right">2002.</td><td>Bleu: a</td></tr><tr><td colspan="4">discussions and the anonymous reviewers for sug-</td><td></td><td colspan="4">method for automatic evaluation of machine translation.</td><td>In</td></tr><tr><td>gestions.</td><td></td><td></td><td></td><td></td><td colspan="3">Proc. of ACL, pages 311–318.</td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td colspan="6">M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and J. Makhoul.</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td colspan="5">2006. A study of translation edit rate with targeted human</td></tr><tr><td>References</td><td></td><td></td><td></td><td></td><td colspan="3">annotation. In Proc. of AMTA.</td><td></td><td></td></tr><tr><td>J. R. Bellegarda.</td><td style="text-align: right">2000.</td><td colspan="2">Exploiting latent semantic informa-</td><td colspan="4">K. Toutanova, H. T. Ilhan, and C. Manning.</td><td colspan="2">2002. Extentions</td></tr><tr><td colspan="3">tion in statistical language modeling.</td><td>Proc. of the IEEE,</td><td></td><td colspan="4">to HMM-based statistical word alignment models.</td><td>In Proc.</td></tr><tr><td colspan="2">88(8):1279–1296, August.</td><td></td><td></td><td></td><td>of EMNLP.</td><td></td><td></td><td></td><td></td></tr><tr><td colspan="3">M. Berry, T. Do, and S. Varadhan. 1993.</td><td>Svdpackc (version</td><td colspan="6">Lonneke van der Plas and Jörg Tiedemann. 2006. Finding syn-</td></tr><tr><td colspan="4">1.0) user’s guide. Tech. report cs-93-194, University of Ten-</td><td></td><td colspan="5">onyms using automatic word alignment and measures of dis-</td></tr><tr><td colspan="2">nessee, Knoxville, TN.</td><td></td><td></td><td></td><td colspan="2">tributional similarity.</td><td colspan="3">In Proc. of the COLING/ACL 2006</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td colspan="4">Main Conference Poster Sessions, pages 866–873.</td><td></td></tr><tr><td colspan="4">P. Blunsom and T. Cohn. 2006. Discriminative word alignment</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td colspan="4">with conditional random fields. In Proc. of COLING/ACL,</td><td colspan="4">S. Vogel, H. Ney, and C. Tillmann. 1996.</td><td colspan="2">HMM based word</td></tr><tr><td>pages 65–72.</td><td></td><td></td><td></td><td></td><td colspan="4">alignment in statistical translation. In Proc. of COLING.</td><td></td></tr><tr><td colspan="4">P. Brown, S. Della Pietra, V. Della Pietra, and R. Mercer. 1993.</td><td colspan="6">B. Xiang, K. Nguyen, L. Nguyen, R. Schwartz, and J. Makhoul.</td></tr><tr><td colspan="4">The mathematics of machine translation: Parameter estima-</td><td></td><td colspan="5">2006. Morphological decomposition for arabic broadcast</td></tr><tr><td colspan="3">tion. Computational Linguistics, 19:263–312.</td><td></td><td></td><td colspan="5">news transcription. In Proc. of ICASSP, pages 1089–1092.</td></tr><tr><td></td><td></td><td></td><td></td><td colspan="6">R. Zens, E. Matusov, and H. Ney. 2004. Improved word align-</td></tr><tr><td colspan="2">S. F. Chen and J. Goodman.</td><td style="text-align: right">1996.</td><td>An empirical study of</td><td></td><td colspan="3">ment using a symmetric lexicon model.</td><td colspan="2">In Proc. of COL-</td></tr><tr><td colspan="3">smoothing techniques for language modeling.</td><td>In Proc. of</td><td></td><td>ING, pages 36–42.</td><td></td><td></td><td></td><td></td></tr><tr><td colspan="2">ACL, pages 310–318.</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td colspan="3">Y. Zhang and S. Vogel.</td><td style="text-align: right">2005.</td><td colspan="2">Competitive grouping in inte-</td></tr><tr><td colspan="4">S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas,</td><td></td><td colspan="5">grated phrase segmentation and alignment model. In Proc.</td></tr><tr><td colspan="2">and R. A. Harshman.</td><td colspan="2">1990. Indexing by latent semantic</td><td></td><td colspan="5">of the ACL Workshop on Building and Using Parallel Texts,</td></tr><tr><td colspan="4">analysis. Journal of the American Society of Information</td><td></td><td>pages 159–162.</td><td></td><td></td><td></td><td></td></tr><tr><td colspan="2">Science, 41(6):391–407.</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td style="text-align: right">8</td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table></page>
</document>
