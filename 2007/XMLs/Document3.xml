<document page-count="8">
<page number="1">
<table data-filename="file.pdf" data-page="1" data-table="1"><tr><td></td><td colspan="4">A Discriminative Syntactic Word Order Model for Machine Translation</td></tr><tr><td></td><td>Pi-Chuan Chang∗</td><td>Kristina Toutanova</td><td></td><td></td></tr><tr><td></td><td>Computer Science Department</td><td>Microsoft Research</td><td></td><td></td></tr><tr><td></td><td>Stanford University</td><td>Redmond, WA</td><td></td><td></td></tr><tr><td></td><td>Stanford, CA 94305</td><td>kristout@microsoft.com</td><td></td><td></td></tr><tr><td></td><td>pichuan@stanford.edu</td><td></td><td></td><td></td></tr><tr><td></td><td>Abstract</td><td>The advantages of modeling how a</td><td>target</td><td>lan-</td></tr><tr><td></td><td></td><td colspan="3">guage syntax tree moves with respect to a source lan-</td></tr><tr><td></td><td>We present a global discriminative statistical</td><td colspan="3">guage syntax tree are that (i) we can capture the fact</td></tr><tr><td></td><td>word order model for machine translation.</td><td colspan="3">that constituents move as a whole and generally re-</td></tr><tr><td></td><td>Our model combines syntactic movement</td><td colspan="3">spect the phrasal cohesion constraints (Fox, 2002),</td></tr><tr><td></td><td>and surface movement information, and is</td><td colspan="3">and (ii) we can model broad syntactic reordering</td></tr><tr><td></td><td>discriminatively trained to choose among</td><td colspan="3">phenomena, such as subject-verb-object construc-</td></tr><tr><td></td><td>possible word orders. We show that com-</td><td colspan="3">tions translating into subject-object-verb ones, as is</td></tr><tr><td></td><td>bining discriminative training with features</td><td colspan="2">generally the case for English and Japanese.</td><td></td></tr><tr><td></td><td>to detect these two different kinds of move-</td><td colspan="3">On the other hand, there is also significant amount</td></tr><tr><td></td><td>ment phenomena leads to substantial im-</td><td colspan="3">of information in the surface strings of the source</td></tr><tr><td></td><td>provements in word ordering performance</td><td colspan="3">and target and their alignment. Many state-of-the-art</td></tr><tr><td></td><td>over strong baselines. Integrating this word</td><td colspan="3">SMT systems do not use trees and base the ordering</td></tr><tr><td></td><td>order model in a baseline MT system results</td><td colspan="3">decisions on surface phrases (Och and Ney, 2004;</td></tr><tr><td></td><td>in a 2.4 points improvement in BLEU for</td><td colspan="3">Al-Onaizan and Papineni, 2006; Kuhn et al., 2006).</td></tr><tr><td></td><td>English to Japanese translation.</td><td colspan="3">In this paper we develop an order model for machine</td></tr><tr><td></td><td></td><td colspan="3">translation which makes use of both syntactic and</td></tr><tr><td style="text-align: right">1</td><td>Introduction</td><td>surface information.</td><td></td><td></td></tr><tr><td colspan="2">The machine translation task can be viewed as con-</td><td colspan="3">The framework for our statistical model is as fol-</td></tr><tr><td colspan="2">sisting of two subtasks: predicting the collection of</td><td colspan="3">lows. We assume the existence of a dependency tree</td></tr><tr><td colspan="2">words in a translation, and deciding the order of the</td><td colspan="3">for the source sentence, an unordered dependency</td></tr><tr><td colspan="2">predicted words. For some language pairs, such as</td><td colspan="3">tree for the target sentence, and a word alignment</td></tr><tr><td colspan="2">English and Japanese, the ordering problem is es-</td><td colspan="3">between the target and source sentences. Figure 1</td></tr><tr><td colspan="2">pecially hard, because the target word order differs</td><td colspan="3">(a) shows an example of aligned source and target</td></tr><tr><td colspan="2">significantly from the source word order.</td><td colspan="3">dependency trees. Our task is to order the target de-</td></tr><tr><td></td><td>Previous work has shown that it is useful to model</td><td>pendency tree.</td><td></td><td></td></tr><tr><td colspan="2">target language order in terms of movement of syn-</td><td colspan="3">We train a statistical model to select the best or-</td></tr><tr><td colspan="2">tactic constituents in constituency trees (Yamada</td><td colspan="3">der of the unordered target dependency tree. An im-</td></tr><tr><td colspan="2">and Knight, 2001; Galley et al., 2006) or depen-</td><td colspan="3">portant advantage of our model is that it is global,</td></tr><tr><td colspan="2">dency trees (Quirk et al., 2005), which are obtained</td><td colspan="3">and does not decompose the task of ordering a tar-</td></tr><tr><td colspan="2">using a parser trained to determine linguistic con-</td><td colspan="3">get sentence into a series of local decisions, as in the</td></tr><tr><td colspan="2">stituency. Alternatively, order is modelled in terms</td><td colspan="3">recently proposed order models for Machine Transi-</td></tr><tr><td colspan="2">of movement of automatically induced hierarchical</td><td colspan="3">tion (Al-Onaizan and Papineni, 2006; Xiong et al.,</td></tr><tr><td colspan="2">structure of sentences (Chiang, 2005; Wu, 1997).</td><td colspan="3">2006; Kuhn et al., 2006). Thus we are able to define</td></tr><tr><td></td><td>∗This research was conducted during the author’s intern-</td><td colspan="3">features over complete target sentence orders, and</td></tr><tr><td colspan="2">ship at Microsoft Research.</td><td colspan="3">avoid the independence assumptions made by these</td></tr><tr><td></td><td style="text-align: right">9</td><td></td><td></td><td></td></tr><tr><td colspan="5">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 9–16,</td></tr><tr><td>Prague, Czech Republic, June 2007.</td><td colspan="4">c 2007 Association for Computational Linguistics</td></tr></table></page>
<page number="2">
<table data-filename="file.pdf" data-page="2" data-table="1"><tr><td>all</td><td colspan="2">constraints</td><td colspan="2">are</td><td>satisfied</td><td>d e c f</td><td>g h</td><td colspan="2">on the target sentence words. The dependency tree<br/>constrains the possible orders of the target sentence</td></tr><tr><td colspan="6">[ࠫપ] [㦕ٙ] [圹] [圣坃地][㻫圩土] [坖坈圡圩]<br/>“restriction”“condition” TOPIC “all” “satisfy” PASSIVE-PRES</td><td>h d e c</td><td>f g</td><td colspan="2">only to the ones that are projective with respect to</td></tr><tr><td>c</td><td>d</td><td>e</td><td>f</td><td>g</td><td>h</td><td colspan="2">g h d e c f</td><td colspan="2">the tree. An order of the sentence is projective with<br/>respect to the tree if each word and its descendants</td></tr><tr><td></td><td></td><td colspan="5">(a) (b)</td><td colspan="3">form a contiguous subsequence in the ordered sen-</td></tr><tr><td></td><td></td><td colspan="5"></td><td colspan="3">tence. Figure 1(b) shows several orders of the sen-</td></tr><tr><td colspan="2">Figure 1:</td><td colspan="5">(a) A sentence pair with source depen-</td><td colspan="2">tence which violate this constraint. 1</td><td></td></tr><tr><td colspan="7">dency tree, projected target dependency tree, and</td><td colspan="2">Previous studies have shown that if both</td><td>the</td></tr><tr><td colspan="7">word alignments. (b) Example orders violating the</td><td colspan="2">source and target dependency trees represent</td><td>lin-</td></tr><tr><td colspan="7">target tree projectivity constraints.</td><td colspan="3">guistic constituency, the alignment between subtrees</td></tr><tr><td></td><td></td><td colspan="5"></td><td colspan="3">in the two languages is very complex (Wellington et</td></tr><tr><td colspan="7">models. Our model is discriminatively trained to se-</td><td colspan="3">al., 2006). Thus such parallel trees would be difficult</td></tr><tr><td colspan="7">lect the best order (according to the BLEU measure)</td><td colspan="3">for MT systems to construct in translation. In this</td></tr><tr><td colspan="7">(Papineni et al., 2001) of an unordered target depen-</td><td colspan="3">work only the source dependency trees are linguisti-</td></tr><tr><td colspan="7">dency tree from the space of possible orders.</td><td colspan="3">cally motivated and constructed by a parser trained</td></tr><tr><td colspan="7">Since the space of all possible orders of an un-</td><td colspan="3">to determine linguistic structure. The target depen-</td></tr><tr><td colspan="7">ordered dependency tree is factorially large, we train</td><td colspan="3">dency trees are obtained through projection of the</td></tr><tr><td colspan="7">our model on N-best lists of possible orders. These</td><td colspan="3">source dependency trees, using the word alignment</td></tr><tr><td colspan="7">N-best lists are generated using approximate search</td><td colspan="3">(we use GIZA++ (Och and Ney, 2004)), ensuring</td></tr><tr><td colspan="7">and simpler models, as in the re-ranking approach of</td><td colspan="3">better parallelism of the source and target structures.</td></tr><tr><td colspan="7">(Collins, 2000).</td><td></td><td></td><td></td></tr><tr><td colspan="7">We first evaluate our model on the task of ordering</td><td style="text-align: right">2.1</td><td>Obtaining Target Dependency Trees</td><td></td></tr><tr><td colspan="7">target sentences, given correct (reference) unordered</td><td></td><td>Through Projection</td><td></td></tr><tr><td colspan="7">target dependency trees. Our results show that com-</td><td colspan="3">Our algorithm for obtaining target dependency trees</td></tr><tr><td colspan="7">bining features derived from the source and tar-</td><td colspan="3">by projection of the source trees via the word align-</td></tr><tr><td colspan="7">get dependency trees, distortion surface order-based</td><td colspan="3">ment is the one used in the MT system of (Quirk</td></tr><tr><td colspan="7">features (like the distortion used in Pharaoh (Koehn,</td><td colspan="3">et al., 2005). We describe the algorithm schemat-</td></tr><tr><td colspan="7">2004)) and language model-like features results in a</td><td colspan="3">ically using the example in Figure 1. Projection</td></tr><tr><td colspan="7">model which significantly outperforms models using</td><td colspan="3">of the dependency tree through alignments is not at</td></tr><tr><td colspan="7">only some of the information sources.</td><td colspan="3">all straightforward. One of the reasons of difficulty</td></tr><tr><td colspan="7">We also evaluate the contribution of our model</td><td colspan="3">is that the alignment does not represent an isomor-</td></tr><tr><td>to</td><td colspan="6">the performance of an MT system. We inte-</td><td colspan="3">phism between the sentences, i.e. it is very often</td></tr><tr><td colspan="7">grate our order model in the MT system, by simply</td><td colspan="3">not a one-to-one and onto mapping.2 If the align-</td></tr><tr><td colspan="7">re-ordering the target translation sentences output</td><td colspan="3">ment were one-to-one we could define the parent of</td></tr><tr><td colspan="7">by the system. The model resulted in an improve-</td><td colspan="3">a word wt in the target to be the target word aligned</td></tr><tr><td colspan="7">ment from 33.6 to 35.4 BLEU points in English-to-</td><td colspan="3">to the parent of the source word si aligned to wt . An</td></tr><tr><td colspan="7">Japanese translation on a computer domain.</td><td colspan="3">additional difficulty is that such a definition could re-</td></tr><tr><td></td><td></td><td colspan="5"></td><td colspan="3">sult in a non-projective target dependency tree. The</td></tr><tr><td style="text-align: right">2</td><td colspan="6">Task Setup</td><td colspan="3">projection algorithm of (Quirk et al., 2005) defines</td></tr><tr><td></td><td></td><td colspan="5"></td><td colspan="3">heuristics for each of these problems. In case of</td></tr><tr><td colspan="7">The ordering problem in MT can be formulated as</td><td></td><td></td><td></td></tr><tr><td></td><td></td><td colspan="5"></td><td colspan="3">one-to-many alignments, for example, the case of</td></tr><tr><td colspan="7">the task of ordering a target bag of words, given a</td><td></td><td></td><td></td></tr><tr><td></td><td></td><td colspan="5"></td><td colspan="3">“constraints” aligning to the Japanese words for “re-</td></tr><tr><td colspan="7">source sentence and word alignments between tar-</td><td></td><td></td><td></td></tr><tr><td></td><td></td><td colspan="5"></td><td colspan="3">striction” and “condition”, the algorithm creates a</td></tr><tr><td colspan="7">get and source words. In this work we also assume</td><td></td><td></td><td></td></tr><tr><td colspan="7">a source dependency tree and an unordered target</td><td colspan="3">1For example, in the first order shown, the descendants of</td></tr><tr><td colspan="7">dependency tree are given. Figure 1(a) shows an ex-</td><td colspan="3">word 6 are not contiguous and thus this order violates the con-</td></tr><tr><td></td><td></td><td colspan="5"></td><td colspan="2">straint.</td><td></td></tr><tr><td colspan="7">ample. We build a model that predicts an order of</td><td colspan="3">2In an onto mapping, every word on the target side is asso-</td></tr><tr><td colspan="7">the target dependency tree, which induces an order</td><td colspan="2">ciated with some word on the source side.</td><td></td></tr><tr><td></td><td></td><td style="text-align: right" colspan="5">10</td><td></td><td></td><td></td></tr></table></page>
<page number="3">
<table data-filename="file.pdf" data-page="3" data-table="1"><tr><td colspan="8">subtree in the target rooted at the rightmost of these</td><td style="text-align: right">3</td><td>Language Model with Syntactic</td></tr><tr><td colspan="8">words and attaches the other word(s) to it. In case of</td><td></td><td>Constraints: A Pilot Study</td></tr><tr><td colspan="8">non-projectivity, the dependency tree is modified by</td><td></td><td></td></tr><tr><td colspan="8">re-attaching nodes higher up in the tree. Such a step</td><td colspan="2">In this section we report the results of a pilot study to</td></tr><tr><td colspan="8">is necessary for our example sentence, because the</td><td colspan="2">evaluate the difficulty of ordering a target sentence if</td></tr><tr><td colspan="8">translations of the words “all” and “constraints” are</td><td colspan="2">we are given a target dependency tree as the one in</td></tr><tr><td colspan="8">not contiguous in the target even though they form a</td><td colspan="2">Figure 1, versus if we are just given an unordered</td></tr><tr><td colspan="4">constituent in the source.</td><td></td><td></td><td></td><td></td><td colspan="2">bag of target language words.</td></tr><tr><td colspan="8">An important characteristic of the projection algo-</td><td colspan="2">The difference between those two settings is that</td></tr><tr><td colspan="8">rithm is that all of its heuristics use the correct target</td><td colspan="2">when ordering a target dependency tree, many of the</td></tr><tr><td colspan="8">word order.3 Thus the target dependency trees en-</td><td colspan="2">orders of the sentence are not allowed, because they</td></tr><tr><td colspan="8">code more information than is present in the source</td><td colspan="2">would be non-projective with respect to the tree.</td></tr><tr><td colspan="5">dependency trees and alignment.</td><td></td><td></td><td></td><td colspan="2">Figure 1 (b) shows some orders which violate the</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="2">projectivity constraint. If the given target depen-</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="2">dency tree is projective with respect to the correct</td></tr><tr><td style="text-align: right">2.2</td><td colspan="7">Task Setup for Reference Sentences vs MT</td><td colspan="2">word order, constraining the possible orders to the</td></tr><tr><td></td><td>Output</td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="2">ones consistent with the tree can only help perfor-</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="2">mance. In our experiments on reference sentences,</td></tr><tr><td>Our</td><td>model uses</td><td>input</td><td>of</td><td>the</td><td>same</td><td>form</td><td>when</td><td colspan="2">the target dependency trees are projective by con-</td></tr><tr><td colspan="8">trained/tested on reference sentences and when used</td><td colspan="2">struction. If, however, the target dependency tree</td></tr><tr><td colspan="8">in machine translation: a source sentence with a de-</td><td colspan="2">provided is not necessarily projective with respect</td></tr><tr><td colspan="2">pendency tree,</td><td colspan="6">an unordered target sentence with</td><td colspan="2">to the best word order, the constraint may or may</td></tr><tr><td>and</td><td>unordered</td><td colspan="5">target dependency tree, and</td><td>word</td><td colspan="2">not be useful. This could happen in our experiments</td></tr><tr><td colspan="2">alignments.</td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="2">on ordering MT output sentences.</td></tr><tr><td colspan="8">We train our model on reference sentences. In this</td><td colspan="2">Thus in this section we aim to evaluate the use-</td></tr><tr><td colspan="8">setting, the given target dependency tree contains the</td><td colspan="2">fulness of the constraint in both settings: reference</td></tr><tr><td colspan="8">correct bag of target words according to a reference</td><td colspan="2">sentences with projective dependency trees, and MT</td></tr><tr><td colspan="8">translation, and is projective with respect to the cor-</td><td colspan="2">output sentences with possibly non-projective de-</td></tr><tr><td colspan="8">rect word order of the reference by construction. We</td><td colspan="2">pendency trees. We also seek to establish a baseline</td></tr><tr><td colspan="8">also evaluate our model in this setting; such an eval-</td><td colspan="2">for our task. Our methodology is to test a simple</td></tr><tr><td colspan="8">uation is useful because we can isolate the contribu-</td><td colspan="2">and effective order model, which is used by all state</td></tr><tr><td colspan="8">tion of an order model, and develop it independently</td><td colspan="2">of the art SMT systems – a trigram language model</td></tr><tr><td colspan="2">of an MT system.</td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="2">– in the two settings: ordering an unordered bag of</td></tr><tr><td colspan="8">When translating new sentences it is not possible</td><td colspan="2">words, and ordering a target dependency tree.</td></tr><tr><td colspan="8">to derive target dependency trees by the projection</td><td colspan="2">Our experimental design is as follows. Given an</td></tr><tr><td colspan="4">algorithm described above.</td><td colspan="4">In this setting, we use</td><td colspan="2">unordered sentence t and an unordered target de-</td></tr><tr><td colspan="8">target dependency trees constructed by our baseline</td><td colspan="2">pendency tree tree(t), we define two spaces of tar-</td></tr><tr><td colspan="8">MT system (described in detail in 6.1). The system</td><td colspan="2">get sentence orders. These are the unconstrained</td></tr><tr><td colspan="8">constructs dependency trees of the form shown in</td><td colspan="2">space of all permutations, denoted by Permutations(t)</td></tr><tr><td colspan="8">Figure 1 for each translation hypothesis. In this case</td><td colspan="2">and the space of all orders of t which are projec-</td></tr><tr><td colspan="8">the target dependency trees very often do not con-</td><td colspan="2">tive with respect to the target dependency tree, de-</td></tr><tr><td colspan="8">tain the correct target words and/or are not projective</td><td colspan="2">noted by TargetProjective (t,tree(t)). For both spaces</td></tr><tr><td colspan="6">with respect to the best possible order.</td><td></td><td></td><td>S,</td><td>we apply a standard trigram target language</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="2">model to select a most likely order from the space;</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="2">i.e., we find a target order order ∗ S (t) such that:</td></tr><tr><td colspan="8">3For example, checking which word is the rightmost for the</td><td colspan="2">order ∗ S (t) = argmax order  P r LM (order (t)).</td></tr><tr><td colspan="8">heuristic for one-to-many mappings and checking whether the</td><td></td><td>∗</td></tr><tr><td colspan="8">constructed tree is projective requires knowledge of the correct</td><td colspan="2">The operator which finds order  S (t) is difficult to</td></tr><tr><td colspan="3">word order of the target.</td><td></td><td></td><td></td><td></td><td></td><td colspan="2">implement since the task is NP-hard in both set-</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td style="text-align: right">11</td><td></td><td></td></tr></table></page>
<page number="4">
<table data-filename="file.pdf" data-page="4" data-table="1"><tr><td colspan="3">Reference SentencesAvg. Size</td><td colspan="2"></td></tr><tr><td>Space</td><td>BLEU</td><td></td><td colspan="2">The gain in BLEU due to the constraint was not</td></tr><tr><td>Permutations<br/>TargetProjective</td><td>58.8<br/>83.9</td><td>261<br/>229</td><td colspan="2">as large on MT output sentences, but was still con-</td></tr><tr><td colspan="2">MT Output Sentences</td><td></td><td colspan="2">siderable. The reduction in search space size due</td></tr><tr><td>Space</td><td>BLEU</td><td>Avg. Size</td><td colspan="2">to the constraint is enormous. There are about 230</td></tr><tr><td>Permutations<br/>TargetProjective</td><td>26.3<br/>31.7</td><td>256<br/>225</td><td colspan="2">times fewer orders to consider in the space of tar-</td></tr><tr><td colspan="3"></td><td colspan="2">get projective orders, compared to the space of all</td></tr><tr><td colspan="3">Table 1: Performance of a tri-gram language model</td><td></td><td></td></tr><tr><td colspan="3"></td><td colspan="2">permutations. From these experiments we conclude</td></tr><tr><td colspan="3">on ordering reference and MT output sentences: un-</td><td></td><td></td></tr><tr><td colspan="3"></td><td colspan="2">that the constraints imposed by a projective target</td></tr><tr><td colspan="3">constrained or subject to target tree projectivity con-</td><td></td><td></td></tr><tr><td colspan="3"></td><td colspan="2">dependency tree are extremely informative. We also</td></tr><tr><td colspan="3">straints.</td><td></td><td></td></tr><tr><td colspan="3"></td><td colspan="2">conclude that the constraints imposed by the target</td></tr><tr><td colspan="3">tings, even for a bi-gram language model (Eisner</td><td colspan="2">dependency trees constructed by our baseline MT</td></tr><tr><td colspan="3">and Tromble, 2006).4We implemented left-to-right</td><td></td><td></td></tr><tr><td colspan="3"></td><td colspan="2">system are very informative as well, even though</td></tr><tr><td colspan="3">beam A* search for the Permutationsspace, and a</td><td colspan="2">the trees are not necessarily projective with respect</td></tr><tr><td colspan="3">tree-based bottom up beam A* search for the Tar-</td><td colspan="2">to the best order. Thus the projectivity constraint</td></tr><tr><td colspan="3">getProjective space. To give an estimate of the search</td><td colspan="2">with respect to a reasonably good target dependency</td></tr><tr><td colspan="3">error in each case, we computed the number of times</td><td colspan="2">tree is useful for addressing the search and modeling</td></tr><tr><td colspan="3">the correct order had a better language model score</td><td colspan="2">problems for MT ordering.</td></tr><tr><td colspan="3">than the order returned by the search algorithm. 5</td><td></td><td></td></tr><tr><td colspan="3">The lower bounds on search error were 4% for Per-</td><td style="text-align: right">4</td><td>A Global Order Model for Target</td></tr><tr><td colspan="3">mutations and 2% for TargetProjective , computed on</td><td></td><td>Dependency Trees</td></tr><tr><td colspan="3">reference sentences.</td><td></td><td></td></tr><tr><td colspan="3">We compare the performance in BLEU of orders</td><td colspan="2">In the rest of the paper we present our new word or-</td></tr><tr><td colspan="3">selected from both spaces. We evaluate the perfor-</td><td colspan="2">der model and evaluate it on reference sentences and</td></tr><tr><td colspan="3">mance on reference sentences and on MT output</td><td colspan="2">in machine translation. In line with previous work</td></tr><tr><td colspan="3">sentences. Table 1 shows the results. In addition</td><td colspan="2">on NLP tasks such as parsing and recent work on</td></tr><tr><td colspan="3">to BLEU scores, the table shows the median number</td><td colspan="2">machine translation, we develop a discriminative or-</td></tr><tr><td colspan="3">of possible orders per sentence for the two spaces.</td><td colspan="2">der model. An advantage of such a model is that we</td></tr><tr><td colspan="3">The highest achievable BLEU on reference sen-</td><td colspan="2">can easily combine different kinds of features (such</td></tr><tr><td colspan="3">tences is 100, because we are given the correct bag</td><td colspan="2">as syntax-based and surface-based), and that we can</td></tr><tr><td colspan="3">of words. The highest achievable BLEU on MT out-</td><td colspan="2">optimize the parameters of our model directly for the</td></tr><tr><td colspan="3">put sentences is well below 100 (the BLEU score of</td><td colspan="2">evaluation measures of interest.</td></tr><tr><td colspan="3">the MT output sentences is 33). Table 3 describes</td><td colspan="2">Additionally, we develop a globally normalized</td></tr><tr><td colspan="3">the characteristics of the main data-sets used in the</td><td colspan="2">model, which avoids the independence assumptions</td></tr><tr><td colspan="3">experiments in this paper; the test sets we use in the</td><td colspan="2">in locally normalized conditional models.  We train</td></tr><tr><td colspan="3">present pilot study are the reference test set (Ref-</td><td colspan="2">a global log-linear model with a rich set of syntactic</td></tr><tr><td colspan="3">test) of 1K sentences and the MT test set (MT-test)</td><td colspan="2">and surface features. Because the space of possible</td></tr><tr><td colspan="3">of 1,000 sentences.</td><td colspan="2">orders of an unordered dependency tree is factori-</td></tr><tr><td colspan="3">The results from our experiment show that the tar-</td><td colspan="2">ally large, we use simpler models to generate N-best</td></tr><tr><td colspan="3">get tree projectivity constraint is extremely powerful</td><td colspan="2">orders, which we then re-rank with a global model.</td></tr><tr><td colspan="3">on reference sentences, where the tree given is in-</td><td></td><td></td></tr><tr><td colspan="3">deed projective. (Recall that in order to obtain the</td><td colspan="2">4.1 Generating N-best Orders</td></tr><tr><td colspan="3">target dependency tree in this setting we have used</td><td colspan="2">The simpler models which we use to generate N-best</td></tr><tr><td colspan="3">information from the true order, which explains in</td><td colspan="2">orders of the unordered target dependency trees are</td></tr><tr><td colspan="3">part the large performance gain.)</td><td colspan="2">the standard trigram language model used in Section</td></tr><tr><td colspan="3"></td><td colspan="2">3, and another statistical model, which we call a Lo-</td></tr><tr><td colspan="3">4Even though the dependency tree constrains the space, the</td><td></td><td></td></tr><tr><td colspan="3">number of children of a node is not bounded by a constant.</td><td colspan="2">cal Tree Order Model (LTOM). The LTOM model</td></tr><tr><td colspan="3">5This is an underestimate of search error, because we don’t</td><td></td><td></td></tr><tr><td colspan="3">know if there was another (non-reference) order which had a</td><td></td><td>6Those models often assume that current decisions are inde-</td></tr><tr><td colspan="3">better score, but was not found.</td><td colspan="2">pendent of future observations.</td></tr><tr><td style="text-align: right" colspan="3">12</td><td></td><td></td></tr></table></page>
<page number="5">
<table data-filename="file.pdf" data-page="5" data-table="1"><tr><td></td><td></td><td></td><td></td><td></td><td colspan="2">Det</td><td></td><td></td><td></td><td></td><td colspan="6">since it uses syntactic information from the source, it</td></tr><tr><td></td><td colspan="2">Pron</td><td colspan="2">Verb</td><td></td><td></td><td>Funcw</td><td colspan="2">Funcw</td><td>Noun</td><td colspan="6"></td></tr><tr><td></td><td colspan="2">this-1</td><td colspan="2">eliminates</td><td colspan="2">the</td><td>six</td><td colspan="2">minute</td><td>delay+1</td><td colspan="6">provides an alternative view compared to the trigram</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="6">language model. The example in Figure 2 shows</td></tr><tr><td colspan="2">[䬢 䭛  ]-2</td><td>[䬺 䭗 䭙 ]</td><td>[6]</td><td>[ಽ]</td><td>[㑆]</td><td>[䬽 ]</td><td>[ᇲ䭛  ]-1</td><td>[䬛 ]</td><td>[⸃ᶖ]</td><td>[䬤 䭛䭍 䬨 ]</td><td colspan="6"></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="6">that the head word “eliminates” takes a dependent</td></tr><tr><td colspan="2">[kore]</td><td>[niyori]</td><td>[roku]</td><td>[fun]</td><td>[kan]</td><td>[no]</td><td>[okure]</td><td>[ga]</td><td>[kaishou]</td><td>[saremasu]</td><td colspan="6"></td></tr><tr><td>Pron</td><td></td><td>Posp</td><td>Noun</td><td>Noun</td><td>Noun</td><td>Posp</td><td>Noun</td><td>Posp</td><td>Vn</td><td>Auxv</td><td colspan="6">“this” to the left (position −1), and on the Japanese</td></tr><tr><td>“this”</td><td></td><td>“by”</td><td style="text-align: right">6</td><td>“minute”</td><td colspan="2">“period” “of”</td><td>“delay”</td><td></td><td>“eliminate”</td><td>PASSIVE</td><td colspan="6"></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="6">side, the head word “kaishou” (corresponding to</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="5">“eliminates”) takes a dependent “kore” (correspond-</td></tr><tr><td colspan="12">Figure 2: Dependency parse on the source (English)</td><td colspan="5">ing to “this”) to the left (position −2). The trigram</td></tr><tr><td colspan="12">sentence, alignment and projected tree on the target</td><td colspan="5">language model would not capture the position of</td></tr><tr><td colspan="12">(Japanese) sentence. Notice that the projected tree</td><td colspan="5">“kore” with respect to “kaishou”, because the words</td></tr><tr><td colspan="12">is only partial and is used to show the head-relative</td><td colspan="5">are farther than three positions away.</td></tr><tr><td colspan="3">movement.</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="5">We use the language model and the local tree or-</td></tr><tr><td colspan="12">uses syntactic information from the source and tar-</td><td colspan="5">der model to create N-best target dependency tree</td></tr><tr><td colspan="12">get dependency trees, and orders each local tree of</td><td>orders.</td><td colspan="4">In particular, we generate the N-best lists</td></tr><tr><td colspan="12">the target dependency tree independently. It follows</td><td>from a</td><td>simple</td><td colspan="3">log-linear combination of the two</td></tr><tr><td colspan="11">the order model defined in (Quirk et al., 2005).</td><td></td><td>models:</td><td></td><td></td><td></td><td></td></tr><tr><td></td><td colspan="11">The model assigns a probability to the position</td><td colspan="2">P (o(t)|s, t)</td><td colspan="3">∝ PLM (o(t)|t)PLT OM (o(t)|s, t)λ</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="5">where o(t) denotes an order of the target.7We used</td></tr><tr><td colspan="12">of each target node (modifier) relative to its par-</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td colspan="12">ent (head), based on information in both the source</td><td colspan="5">a bottom-up beam A* search to generate N-best or-</td></tr><tr><td colspan="12">and target trees. The probability of an order of the</td><td colspan="5">ders. The performance of each of these two models</td></tr><tr><td colspan="12">complete target dependency tree decomposes into a</td><td colspan="5">and their combination, together with the 30-best or-</td></tr><tr><td colspan="12">product over probabilities of positions for each node</td><td colspan="5">acle performance on reference sentences is shown in</td></tr><tr><td colspan="5">in the tree as follows:</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="5">Table 2. As we can see, the 30-best oracle perfor-</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="5">mance of the combined model (98.0) is much higher</td></tr><tr><td colspan="4">P (order(t)|s, t) =</td><td>Y</td><td></td><td colspan="6">P (pos(n, parent(n))|s, t)</td><td colspan="5">than the 1-best performance (92.6) and thus there is</td></tr><tr><td></td><td></td><td></td><td></td><td>n∈t</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="4">a lot of room for improvement.</td><td></td></tr><tr><td></td><td colspan="11">Here, position is modelled in terms of closeness</td><td style="text-align: right">4.2</td><td>Model</td><td></td><td></td><td></td></tr><tr><td>to</td><td colspan="3">the head in the</td><td colspan="5">dependency tree.</td><td colspan="3">The closest</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td colspan="12">pre-modifier of a given head has position −1; the</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="5">The log-linear reranking model is defined as fol-</td></tr><tr><td>closest</td><td colspan="4">post-modifier</td><td>has</td><td colspan="2">a</td><td>position</td><td style="text-align: right">1.</td><td>Figure</td><td style="text-align: right">2</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="4">lows. For each sentence pair spl</td><td>(l = 1, 2, ..., L) in</td></tr><tr><td colspan="12">shows an example dependency tree pair annotated</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="5">the training data, we have N candidate target word</td></tr><tr><td colspan="12">with head-relative positions. A small set of features</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="5">orders o l,1 , ol,2 , ..., ol,N , which are the orders gener-</td></tr><tr><td colspan="12">is used to reflect local information in the dependency</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="5">ated from the simpler models. Without loss of gen-</td></tr><tr><td colspan="12">tree to model P (pos(n, parent(n))|s, t): (i) lexical</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="5">erality, we define ol,1 to be the order with the highest</td></tr><tr><td colspan="12">items of n and parent(n), (ii) lexical items of the</td><td></td><td></td><td></td><td></td><td style="text-align: right">8</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="5">BLEU score with respect to the correct order.</td></tr><tr><td colspan="12">source nodes aligned to n and parent(n), (iii) part-</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="5">We define a set of feature functions fm (o l,n , spl )</td></tr><tr><td colspan="12">of-speech of the source nodes aligned to the node</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="4">to describe a target word order o l,n</td><td>of a given sen-</td></tr><tr><td colspan="12">and its parent, and (iv) head-relative position of the</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="5">tence pair spl . In the log-linear model, a correspond-</td></tr><tr><td colspan="10">source node aligned to the target node.</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="5">ing weights vector λ is used to define the distribution</td></tr><tr><td></td><td colspan="11">We train a log-linear model which uses these fea-</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="4">over all possible candidate orders:</td><td></td></tr><tr><td>tures</td><td>on</td><td>a</td><td>training</td><td>set</td><td></td><td>of</td><td colspan="2">aligned</td><td>sentences</td><td colspan="2">with</td><td></td><td></td><td></td><td colspan="2">λF  )<br/>(ol,n ,spl</td></tr><tr><td colspan="12">source and target dependency trees in the form of</td><td></td><td colspan="3">ep(o l,n |spl , λ) = P</td><td>λF (ol,n′ ,spl )<br/>e</td></tr><tr><td colspan="12">Figure 2. The model is a local (non-sequence) clas-</td><td></td><td></td><td></td><td>n′</td><td></td></tr><tr><td colspan="12">sifier, because the decision on where to place each</td><td colspan="2">7We used the value λ</td><td>=</td><td colspan="2">.5, which we selected on a devel-</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="4">opment set to maximize BLEU.</td><td></td></tr><tr><td colspan="12">node does not depend on the placement of any other</td><td colspan="5">8To avoid the problem that all orders could have a BLEU</td></tr><tr><td>nodes.</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="5">score of 0 if none of them contains a correct word four-gram,</td></tr><tr><td></td><td colspan="11">Since the local tree order model learns to order</td><td colspan="5">we define sentence-level k-gram BLEU, where k is the highest</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="5">order, k ≤ 4, for which there exists a correct k-gram in at least</td></tr><tr><td colspan="12">whole subtrees of the target dependency tree, and</td><td colspan="2">one of the N-Best orders.</td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td style="text-align: right" colspan="2">13</td><td></td><td></td><td></td><td></td><td></td></tr></table></page>
<page number="6">
<table data-filename="file.pdf" data-page="6" data-table="1"><tr><td colspan="10">We train the parameters λ by minimizing the neg-</td><td>(N</td><td>(N</td><td></td><td colspan="2">(N</td><td>(NQ</td><td colspan="2">(N</td><td>(NQ</td></tr><tr><td>ative</td><td colspan="2">log-likelihood</td><td>of</td><td>the</td><td colspan="2">training</td><td>data</td><td>plus</td><td>a</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td colspan="2"></td><td></td><td></td><td colspan="2"></td><td></td><td></td><td></td><td></td><td>-L</td><td>-L</td><td>-L</td><td>-L</td><td></td><td>-L</td><td>-L</td><td></td></tr><tr><td colspan="5">quadratic regularization term:</td><td colspan="2"></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td colspan="2">P</td><td></td><td></td><td colspan="2"></td><td>P</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td colspan="8">L(λ) = − l log p(ol,1 |spi , λ) + 2σ1 2</td><td colspan="2">m λ m 2</td><td colspan="8">(a) parallel (b) crossing (c) widening</td><td></td></tr><tr><td colspan="10">We also explored maximizing expected BLEU as</td><td colspan="8"></td><td></td></tr><tr><td colspan="10">our objective function, but since it is not convex, the</td><td colspan="9">Figure 3: Displacement feature: different alignment</td></tr><tr><td colspan="10">performance was less stable and ultimately slightly</td><td colspan="9">patterns of two contiguous words in the target sen-</td></tr><tr><td colspan="10">worse, as compared to the log-likelihood objective.</td><td colspan="8">tence.</td><td></td></tr><tr><td style="text-align: right">4.3</td><td>Features</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="9">set MT-train in Table 3. The sentences were anno-</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="9">tated with alignment (using GIZA++ (Och and Ney,</td></tr><tr><td colspan="10">We design features to capture both the head-relative</td><td colspan="8"></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="9">2004)) and syntactic dependency structures of the</td></tr><tr><td colspan="10">movement and the surface sequence movement of</td><td colspan="8"></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="9">source and target, obtained as described in Section</td></tr><tr><td colspan="3">words in a sentence.</td><td></td><td colspan="6">We experiment with different</td><td colspan="8"></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="9">2. Japanese POS tags were assigned by an automatic</td></tr><tr><td colspan="10">combinations of features and show their contribu-</td><td colspan="8"></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="9">POS tagger, which is a local classifier not using tag</td></tr><tr><td colspan="10">tion in Table 2 for reference sentences and Table 4</td><td colspan="8"></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="8">sequence information.</td><td></td></tr><tr><td colspan="10">in machine translation. The notations used in the ta-</td><td colspan="8"></td><td></td></tr><tr><td colspan="5">bles are defined as follows:</td><td></td><td></td><td></td><td></td><td></td><td colspan="9">We used 400K sentence pairs from the complete</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="9">set to train the first pass models: the language model</td></tr><tr><td colspan="2">Baseline:</td><td colspan="8">LTOM+LM as described in Section 4.1</td><td colspan="8"></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="9">was trained on 400K sentences, and the local tree</td></tr><tr><td colspan="2">Word Bigram:</td><td colspan="8">Word bigrams of the target sen-</td><td colspan="8"></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="9">order model was trained on 100K of them. We gen-</td></tr><tr><td colspan="2">tence.</td><td colspan="8">Examples from Figure 2: “kore”+“niyori”,</td><td colspan="8"></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="9">erated N-best target tree orders for the rest of the</td></tr><tr><td colspan="3">“niyori”+“roku”.</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="8"></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="9">data (45K sentence pairs), and used it for training</td></tr><tr><td colspan="2">DISP:</td><td colspan="4">Displacement feature.</td><td colspan="4">For each word posi-</td><td colspan="8"></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="9">and evaluating the re-ranking model. The re-ranking</td></tr><tr><td colspan="10">tion in the target sentence, we examine the align-</td><td colspan="8"></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="9">model was trained on 44K sentence pairs. All mod-</td></tr><tr><td colspan="10">ment of the current word and the previous word, and</td><td colspan="8"></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="9">els were evaluated on the remaining 1,000 sentence</td></tr><tr><td colspan="10">categorize the possible patterns into 3 kinds: (a) par-</td><td colspan="8"></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="8">pairs set, which is the set Ref-test in Table 3.</td><td></td></tr><tr><td colspan="10">allel, (b) crossing, and (c) widening. Figure 3 shows</td><td colspan="8"></td><td></td></tr><tr><td colspan="8">how these three categories are defined.</td><td></td><td></td><td colspan="8">The top part of Table 2 presents the</td><td>1-best</td></tr><tr><td colspan="2">Pharaoh DISP:</td><td colspan="8">Displacement as used in Pharaoh</td><td colspan="9">BLEU scores (actual performance) and 30-best or-</td></tr><tr><td colspan="10">(Koehn, 2004). For each position in the sentence,</td><td colspan="9">acle BLEU scores of the first-pass models and their</td></tr><tr><td colspan="10">the value of the feature is one less than the difference</td><td colspan="9">log-linear combination, described in Section 4. We</td></tr><tr><td colspan="10">(absolute value) of the positions of the source words</td><td colspan="9">can see that the combination of the language model</td></tr><tr><td colspan="10">aligned to the current and the previous target word.</td><td colspan="9">and the local tree order model outperformed either</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="9">model by a large margin. This indicates that combin-</td></tr><tr><td colspan="2">POSs and POSt:</td><td colspan="8">POS tags on the source and target</td><td colspan="8"></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="9">ing syntactic (from the LTOM model) and surface-</td></tr><tr><td colspan="10">sides. For Japanese, we have a set of 19 POS tags.</td><td colspan="8"></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="9">based (from the language model) information is very</td></tr><tr><td>’+’</td><td>means</td><td>making</td><td></td><td colspan="2">conjunction</td><td>of</td><td colspan="2">features</td><td>and</td><td colspan="8"></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="9">effective even at this stage of selecting N-best orders</td></tr><tr><td colspan="10">prev() means using the information associated with</td><td colspan="8"></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="9">for re-ranking. According to the 30-best oracle per-</td></tr><tr><td colspan="5">the word from position −1.</td><td></td><td></td><td></td><td></td><td></td><td colspan="8"></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="9">formance of the combined model LTOM+LM, 98.0</td></tr><tr><td>In</td><td>all</td><td>explored</td><td colspan="2">models,</td><td>we</td><td colspan="2">include</td><td>the</td><td>log-</td><td colspan="8"></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="9">BLEU is the upper bound on performance of our re-</td></tr><tr><td colspan="10">probability of an order according to the language</td><td colspan="8"></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="8">ranking approach.</td><td></td></tr><tr><td colspan="10">model and the log-probability according to the lo-</td><td colspan="8"></td><td></td></tr><tr><td colspan="10">cal tree order model, the two features used by the</td><td colspan="9">The bottom part of the table shows the perfor-</td></tr><tr><td colspan="2">baseline model.</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="9">mance of the global log-linear model, when features</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="9">in addition to the scores from the two first-pass mod-</td></tr><tr><td style="text-align: right">5</td><td colspan="8">Evaluation on Reference Sentences</td><td></td><td colspan="9">els are added to the model. Adding word-bigram</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="9">features increased performance by about 0.6 BLEU</td></tr><tr><td>Our</td><td>experiments</td><td colspan="2">on</td><td colspan="2">ordering</td><td colspan="4">reference sentences</td><td colspan="9">points, indicating that training language-model like</td></tr><tr><td colspan="10">use a set of 445K English sentences with their ref-</td><td colspan="9">features discriminatively to optimize ordering per-</td></tr><tr><td colspan="10">erence Japanese translations. This is a subset of the</td><td colspan="9">formance, is indeed worthwhile. Next we compare</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td style="text-align: right">14</td><td colspan="8"></td><td></td></tr></table></page>
<page number="7">
<table data-filename="file.pdf" data-page="7" data-table="1"><tr><td></td><td>First-pass models</td><td></td><td></td><td></td><td>data set</td><td>num sent.</td><td colspan="2">English</td><td colspan="2">Japanese</td></tr><tr><td>Model</td><td></td><td colspan="2">BLEU<br/>1 best 30 best</td><td></td><td>MT-train<br/>MT-test</td><td>500K<br/>1K</td><td>avg. len<br/>15.8<br/>17.5</td><td>vocab<br/>77K<br/>–</td><td>avg. len<br/>18.7<br/>20.9</td><td>vocab<br/>79K<br/>–</td></tr><tr><td colspan="2">Lang Model (Permutations )</td><td style="text-align: right">58.8</td><td style="text-align: right">71.2</td><td></td><td>Ref-test</td><td>1K</td><td style="text-align: right">17.5</td><td>–</td><td style="text-align: right">21.2</td><td>–</td></tr><tr><td colspan="2">Lang Model (TargetProjective )<br/>Local Tree Order Model<br/>Local Tree Order Model + Lang Model</td><td>83.9<br/>75.8<br/>92.6</td><td>95.0<br/>87.3<br/>98.0</td><td colspan="7">Table 3: Main data sets used in experiments.</td></tr><tr><td></td><td>Re-ranking Models</td><td></td><td></td><td colspan="7">target words and/or will not be projective with re-</td></tr><tr><td>Features</td><td></td><td colspan="2">BLEU</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Baseline</td><td></td><td style="text-align: right" colspan="2">92.60</td><td colspan="5">spect to the best possible order.</td><td></td><td></td></tr><tr><td>Word Bigram</td><td></td><td style="text-align: right" colspan="2">93.19</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Pharaoh DISP</td><td></td><td style="text-align: right" colspan="2">92.94</td><td style="text-align: right">6.1</td><td colspan="3">Baseline MT System</td><td></td><td></td><td></td></tr><tr><td>DISP</td><td></td><td style="text-align: right" colspan="2">93.57</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>DISP+POSs</td><td></td><td style="text-align: right" colspan="2">94.04</td><td colspan="7">Our baseline SMT system is the system of Quirk et</td></tr><tr><td>DISP+POSs+POSt</td><td></td><td style="text-align: right" colspan="2">94.14</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td colspan="2">DISP+POSs+POSt, prev(DISP)+POSs+POSt</td><td style="text-align: right" colspan="2">94.34</td><td colspan="7">al. (2005). It translates by first deriving a depen-</td></tr><tr><td colspan="2">DISP+POSs+POSt, prev(DISP)+POSs+POSt, WB</td><td style="text-align: right" colspan="2">94.50</td><td colspan="7">dency tree for the source sentence and then trans-</td></tr><tr><td></td><td></td><td></td><td></td><td colspan="7">lating the source dependency tree to a target depen-</td></tr><tr><td colspan="4">Table 2: Performance of the first-pass order models</td><td></td><td></td><td colspan="2"></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td colspan="7">dency tree, using a set of probabilistic models. The</td></tr><tr><td colspan="4">and 30-best oracle performance, followed by perfor-</td><td></td><td></td><td colspan="2"></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td colspan="5">translation is based on treelet pairs.</td><td colspan="2">A treelet is a</td></tr><tr><td colspan="4">mance of re-ranking model for different feature sets.</td><td></td><td></td><td colspan="2"></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td colspan="7">connected subgraph of the source or target depen-</td></tr><tr><td colspan="2">Results are on reference sentences.</td><td></td><td></td><td></td><td></td><td colspan="2"></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td>dency tree.</td><td colspan="6">A treelet translation pair is a pair of</td></tr><tr><td colspan="4">the Pharaoh displacement feature to the displace-</td><td></td><td></td><td colspan="2"></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td colspan="6">word-aligned source and target treelets.</td><td></td></tr><tr><td colspan="2">ment feature we illustrated in Figure 3.</td><td></td><td>We can</td><td></td><td></td><td colspan="2"></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td colspan="7">The baseline SMT model combines this treelet</td></tr><tr><td colspan="4">see that the Pharaoh displacement feature improves</td><td></td><td></td><td colspan="2"></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td colspan="7">translation model with other feature functions — a</td></tr><tr><td colspan="4">performance of the baseline by .34 points, whereas</td><td></td><td></td><td colspan="2"></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td colspan="7">target language model, a tree order model, lexical</td></tr><tr><td colspan="4">our displacement feature improves performance by</td><td></td><td></td><td colspan="2"></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td colspan="7">weighting features to smooth the translation prob-</td></tr><tr><td colspan="4">nearly 1 BLEU point. Concatenating the DISP fea-</td><td></td><td></td><td colspan="2"></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td colspan="7">abilities, word count feature, and treelet-pairs count</td></tr><tr><td colspan="4">ture with the POS tag of the source word aligned to</td><td></td><td></td><td colspan="2"></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td colspan="7">feature. These models are combined as feature func-</td></tr><tr><td colspan="4">the current word improved performance slightly.</td><td></td><td></td><td colspan="2"></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td colspan="7">tions in a (log)linear model for predicting a target</td></tr><tr><td colspan="4">The results show that surface movement features</td><td></td><td></td><td colspan="2"></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td colspan="7">sentence given a source sentence, in the framework</td></tr><tr><td>(i.e.</td><td colspan="3">the DISP feature) improve the performance</td><td></td><td></td><td colspan="2"></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td colspan="4">proposed by (Och and Ney,</td><td style="text-align: right">2002).</td><td colspan="2">The weights</td></tr><tr><td colspan="4">of a model using syntactic-movement features (i.e.</td><td></td><td></td><td colspan="2"></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td colspan="7">of this model are trained to maximize BLEU (Och</td></tr><tr><td>the LTOM model).</td><td>Additionally,</td><td colspan="2">adding part-of-</td><td></td><td></td><td colspan="2"></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td colspan="2">and Ney, 2004).</td><td colspan="5">The SMT system is trained using</td></tr><tr><td colspan="4">speech information from both languages in combi-</td><td></td><td></td><td colspan="2"></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td colspan="6">the same form of data as our order model:</td><td>parallel</td></tr><tr><td colspan="4">nation with displacement, and using a higher order</td><td></td><td></td><td colspan="2"></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td colspan="7">source and target dependency trees as in Figure 2.</td></tr><tr><td colspan="2">on the displacement features was useful.</td><td></td><td>The per-</td><td></td><td></td><td colspan="2"></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td colspan="7">Of particular interest are the components in the</td></tr><tr><td colspan="4">formance of our best model, which included all in-</td><td></td><td></td><td colspan="2"></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td colspan="7">baseline SMT system contributing most to word or-</td></tr><tr><td colspan="4">formation sources, is 94.5 BLEU points, which is a</td><td></td><td></td><td colspan="2"></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td colspan="7">der decisions. The SMT system uses the same target</td></tr><tr><td colspan="4">35% improvement over the fist-pass models, relative</td><td></td><td></td><td colspan="2"></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td colspan="7">language trigram model and local tree order model,</td></tr><tr><td>to the upper bound.</td><td></td><td></td><td></td><td></td><td></td><td colspan="2"></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td colspan="7">as we are using for generating N-best orders for re-</td></tr><tr><td></td><td></td><td></td><td></td><td colspan="7">ranking. Thus the baseline system already uses our</td></tr><tr><td style="text-align: right">6</td><td colspan="3">Evaluation in Machine Translation</td><td></td><td></td><td colspan="2"></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td colspan="7">first-pass order models and only lacks the additional</td></tr><tr><td colspan="4">We apply our model to machine translation by re-</td><td colspan="7">information provided by our re-ranking order model.</td></tr><tr><td colspan="4">ordering the translation produced by a baseline MT</td><td></td><td></td><td colspan="2"></td><td></td><td></td><td></td></tr><tr><td colspan="4">system. Our baseline MT system constructs, for</td><td colspan="6">6.2 Data and Experimental Results</td><td></td></tr><tr><td colspan="2">each target translation hypothesis,</td><td colspan="2">a target depen-</td><td colspan="7">The baseline MT system was trained on the MT-train</td></tr><tr><td colspan="4">dency tree. Thus we can apply our model to MT</td><td colspan="7">dataset described in Table 3. The test set for the MT</td></tr><tr><td colspan="4">output in exactly the same way as for reference sen-</td><td colspan="7">experiment is a 1K sentences set from the same do-</td></tr><tr><td colspan="4">tences, but using much noisier input: a source sen-</td><td colspan="7">main (shown as MT-test in the table). The weights</td></tr><tr><td colspan="4">tence with a dependency tree, word alignment and</td><td colspan="7">in the linear model used by the baseline SMT system</td></tr><tr><td colspan="4">an unordered target dependency tree as the example</td><td colspan="6">were tuned on a separate development set.</td><td></td></tr><tr><td colspan="4">shown in Figure 2. The difference is that the target</td><td colspan="7">Table 4 shows the performance of the first-pass</td></tr><tr><td colspan="4">dependency tree will likely not contain the correct</td><td colspan="7">models in the top part, and the performance of our</td></tr><tr><td></td><td></td><td></td><td style="text-align: right">15</td><td></td><td></td><td colspan="2"></td><td></td><td></td><td></td></tr></table></page>
<page number="8">
<table data-filename="file.pdf" data-page="8" data-table="1"><tr><td colspan="2"></td><td>First-pass models</td><td></td><td></td><td colspan="5">lect from the space of orders projective with respect</td></tr><tr><td colspan="2">Model</td><td></td><td></td><td>BLEU</td><td colspan="5"></td></tr><tr><td colspan="2"></td><td></td><td>1 best</td><td>30 best</td><td colspan="5">to a target dependency tree. We investigated a com-</td></tr><tr><td colspan="2">Baseline MT System</td><td></td><td style="text-align: right">33.0</td><td>–</td><td colspan="5">bination of features modeling surface movement and</td></tr><tr><td colspan="3">Lang Model (Permutations )</td><td style="text-align: right">26.3</td><td style="text-align: right">28.7</td><td colspan="5"></td></tr><tr><td colspan="3">Lang Model (TargetCohesive )</td><td style="text-align: right">31.7</td><td style="text-align: right">35.0</td><td colspan="5">syntactic movement phenomena and showed that</td></tr><tr><td colspan="3">Local Tree Order Model</td><td style="text-align: right">27.2</td><td style="text-align: right">31.5</td><td colspan="5">these two information sources are complementary</td></tr><tr><td colspan="3">Local Tree Order Model + Lang Model</td><td style="text-align: right">33.6</td><td style="text-align: right">36.0</td><td colspan="5"></td></tr><tr><td colspan="2"></td><td>Re-ranking Models</td><td></td><td></td><td colspan="5">and their combination is powerful. Our results on or-</td></tr><tr><td colspan="2">Features</td><td></td><td></td><td>BLEU</td><td colspan="5">dering MT output and reference sentences were very</td></tr><tr><td colspan="2">Baseline</td><td></td><td style="text-align: right" colspan="2">33.56</td><td colspan="5"></td></tr><tr><td colspan="2">Word Bigram</td><td></td><td style="text-align: right" colspan="2">34.11</td><td colspan="5">encouraging. We obtained substantial improvement</td></tr><tr><td colspan="2">Pharaoh DISP</td><td></td><td style="text-align: right" colspan="2">34.67</td><td colspan="5">by the simple method of post-processing the 1-best</td></tr><tr><td colspan="2">DISP</td><td></td><td style="text-align: right" colspan="2">34.90</td><td colspan="5"></td></tr><tr><td colspan="2">DISP+POSs</td><td></td><td style="text-align: right" colspan="2">35.28</td><td colspan="5">MT output to re-order the proposed translation. In</td></tr><tr><td colspan="2">DISP+POSs+POSt</td><td></td><td style="text-align: right" colspan="2">35.22</td><td colspan="5">the future, we would like to explore tighter integra-</td></tr><tr><td colspan="3">DISP+POSs+POSt, prev(DISP)+POSs+POSt</td><td style="text-align: right" colspan="2">35.33</td><td colspan="5"></td></tr><tr><td colspan="3">DISP+POSs+POSt, prev(DISP)+POSs+POSt, WB</td><td style="text-align: right" colspan="2">35.37</td><td colspan="5">tion of our order model with the SMT system and to</td></tr><tr><td></td><td></td><td></td><td colspan="2"></td><td colspan="5">develop more accurate algorithms for constructing</td></tr><tr><td colspan="5">Table 4: Performance of the first pass order models</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td colspan="2"></td><td colspan="4">projective target dependency trees in translation.</td><td></td></tr><tr><td colspan="5">and 30-best oracle performance, followed by perfor-</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td colspan="5">mance of re-ranking model for different feature sets.</td><td colspan="2">References</td><td></td><td></td><td></td></tr><tr><td colspan="3">Results are in MT.</td><td colspan="2"></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td colspan="2"></td><td colspan="2">Y. Al-Onaizan and K. Papineni.</td><td style="text-align: right" colspan="2">2006.</td><td>Distortion models for</td></tr><tr><td colspan="3">re-ranking model in the bottom part.</td><td colspan="2">The first row</td><td></td><td colspan="3">statistical machine translation. In ACL.</td><td></td></tr><tr><td colspan="5">of the table shows the performance of the baseline</td><td colspan="5">D. Chiang. 2005. A hierarchical phrase-based model for statis-</td></tr><tr><td></td><td></td><td></td><td colspan="2"></td><td></td><td colspan="2">tical machine translation. In ACL.</td><td></td><td></td></tr><tr><td colspan="5">MT system, which is a BLEU score of 33. Our first-</td><td colspan="5">M. Collins. 2000. Discriminative reranking for natural language</td></tr><tr><td colspan="5">pass and re-ranking models re-order the words of</td><td></td><td colspan="3">parsing. In ICML, pages 175–182.</td><td></td></tr><tr><td colspan="5">this 1-best output from the MT system. As for ref-</td><td colspan="2">J Eisner and R. W. Tromble.</td><td colspan="3">2006. Local search with very</td></tr><tr><td></td><td></td><td></td><td colspan="2"></td><td></td><td colspan="4">large-scale neighborhoods for optimal permutations in ma-</td></tr><tr><td colspan="5">erence sentences, the combination of the two first-</td><td></td><td colspan="3">chine translation. In HLT-NAACL Workshop.</td><td></td></tr><tr><td colspan="5">pass models outperforms the individual models. The</td><td colspan="5">H. Fox. 2002. Phrasal cohesion and statistical machine transla-</td></tr><tr><td colspan="5">1-best performance of the combination is 33.6 and</td><td></td><td>tion. In EMNLP.</td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td colspan="2"></td><td>M.</td><td colspan="2">Galley, J. Graehl, K. Knight,</td><td>D.</td><td>Marcu, S. DeNeefe,</td></tr><tr><td colspan="5">the 30-best oracle is 36.0. Thus the best we could</td><td></td><td colspan="4">W. Wang, and I. Thayer. 2006. Scalable inference and train-</td></tr><tr><td colspan="5">do with our re-ranking model in this setting is 36</td><td></td><td colspan="3">ing of context-rich syntactic translation models. In ACL.</td><td></td></tr><tr><td></td><td style="text-align: right">9</td><td></td><td colspan="2"></td><td colspan="5">P. Koehn. 2004. Pharaoh: A beam search decoder for phrase-</td></tr><tr><td colspan="2">BLEU points.</td><td colspan="3">Our best re-ranking model achieves</td><td></td><td colspan="3">based statistical machine translation models. In AMTA.</td><td></td></tr><tr><td colspan="5">2.4 BLEU points improvement over the baseline MT</td><td colspan="5">R. Kuhn, D. Yuen, M. Simard, P. Paul, G. Foster, E. Joanis, and</td></tr><tr><td colspan="5">system and 1.8 points improvement over the first-</td><td></td><td colspan="4">H. Johnson. 2006. Segment choice models: Feature-rich</td></tr><tr><td></td><td></td><td></td><td colspan="2"></td><td></td><td colspan="4">models for global distortion in statistical machine transla-</td></tr><tr><td colspan="5">pass models, as shown in the table. The trends here</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td colspan="2"></td><td></td><td>tion. In HLT-NAACL.</td><td></td><td></td><td></td></tr><tr><td colspan="5">are similar to the ones observed in our reference ex-</td><td colspan="2">F. J. Och and H. Ney. 2002.</td><td colspan="3">Discriminative training and max-</td></tr><tr><td colspan="5">periments, with the difference that target POS tags</td><td></td><td colspan="3">imum entropy models for statistical machine translation.</td><td>In</td></tr><tr><td></td><td></td><td></td><td colspan="2"></td><td></td><td>ACL.</td><td></td><td></td><td></td></tr><tr><td colspan="5">were less useful (perhaps due to ungrammatical can-</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td colspan="2"></td><td colspan="5">F. J. Och and H. Ney. 2004. The alignment template approach</td></tr><tr><td colspan="5">didates) and the displacement features were more</td><td></td><td colspan="4">to statistical machine translation. Computational Linguistics,</td></tr><tr><td colspan="5">useful. We can see that our re-ranking model al-</td><td></td><td style="text-align: right">30(4).</td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td colspan="2"></td><td colspan="5">K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2001. BLEU: a</td></tr><tr><td colspan="5">most reached the upper bound oracle performance,</td><td></td><td colspan="3">method for automatic evaluation of machine translation.</td><td>In</td></tr><tr><td colspan="5">reducing the gap between the first-pass models per-</td><td></td><td>ACL.</td><td></td><td></td><td></td></tr><tr><td colspan="5">formance (33.6) and the oracle (36.0) by 75%.</td><td colspan="5">C. Quirk, A. Menezes, and C. Cherry. 2005. Dependency treelet</td></tr><tr><td></td><td></td><td></td><td colspan="2"></td><td></td><td colspan="4">translation: Syntactically informed phrasal SMT. In ACL.</td></tr><tr><td></td><td></td><td></td><td colspan="2"></td><td colspan="5">B. Wellington, S. Waxmonsky, and I. Dan Melamed. 2006.</td></tr><tr><td style="text-align: right">7</td><td colspan="4">Conclusions and Future Work</td><td></td><td colspan="4">Empirical lower bounds on the complexity of translational</td></tr><tr><td></td><td></td><td></td><td colspan="2"></td><td></td><td colspan="2">equivalence. In ACL-COLING.</td><td></td><td></td></tr><tr><td colspan="5">We have presented a discriminative syntax-based or-</td><td colspan="5">D. Wu. 1997. Stochastic inversion transduction grammars and</td></tr><tr><td colspan="5">der model for machine translation, trained to to se-</td><td></td><td colspan="4">bilingual parsing of parallel corpora. Computational Lin-</td></tr><tr><td></td><td></td><td></td><td colspan="2"></td><td></td><td>guistics, 23(3):377–403.</td><td></td><td></td><td></td></tr><tr><td></td><td style="text-align: right">9</td><td></td><td colspan="2"></td><td colspan="2">D. Xiong, Q. Liu, and S. Lin.</td><td style="text-align: right">2006.</td><td colspan="2">Maximum entropy based</td></tr><tr><td></td><td colspan="4">Notice that the combination of our two first-pass models</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td colspan="5">outperforms the baseline MT system by half a point (33.6 ver-</td><td></td><td colspan="4">phrase reordering model for statistical machine translation.</td></tr><tr><td colspan="5">sus 33.0). This is perhaps due to the fact that the MT system</td><td></td><td>In ACL.</td><td></td><td></td><td></td></tr><tr><td colspan="5">searches through a much larger space (possible word transla-</td><td colspan="5">K. Yamada and Kevin Knight. 2001. A syntax-based statistical</td></tr><tr><td colspan="5">tions in addition to word orders), and thus could have a higher</td><td></td><td>translation model. In ACL.</td><td></td><td></td><td></td></tr><tr><td colspan="2">search error.</td><td></td><td colspan="2"></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td style="text-align: right" colspan="2">16</td><td></td><td></td><td></td><td></td><td></td></tr></table></page>
</document>
