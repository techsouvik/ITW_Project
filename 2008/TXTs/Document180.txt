













































Active Learning with Confidence


Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 233–236,
Columbus, Ohio, USA, June 2008. c©2008 Association for Computational Linguistics

Active Learning with Confidence

Mark Dredze and Koby Crammer
Department of Computer and Information Science

University of Pennsylvania
Philadelphia, PA 19104

{mdredze,crammer}@cis.upenn.edu

Abstract

Active learning is a machine learning ap-
proach to achieving high-accuracy with a
small amount of labels by letting the learn-
ing algorithm choose instances to be labeled.
Most of previous approaches based on dis-
criminative learning use the margin for choos-
ing instances. We present a method for in-
corporating confidence into the margin by us-
ing a newly introduced online learning algo-
rithm and show empirically that confidence
improves active learning.

1 Introduction

Successful applications of supervised machine
learning to natural language rely on quality labeled
training data, but annotation can be costly, slow and
difficult. One popular solution is Active Learning,
which maximizes learning accuracy while minimiz-
ing labeling efforts. In active learning, the learning
algorithm itself selects unlabeled examples for anno-
tation. A variety of techniques have been proposed
for selecting examples that maximize system perfor-
mance as compared to selecting instances randomly.

Two learning methodologies dominate NLP ap-
plications: probabilistic methods — naive Bayes,
logistic regression — and margin methods — sup-
port vector machines and passive-aggressive. Active
learning for probabilistic methods often uses uncer-
tainty sampling: label the example with the lowest
probability prediction (the most “uncertain”) (Lewis
and Gale, 1994). The equivalent technique for mar-
gin learning associates the margin with prediction
certainty: label the example with the lowest margin

(Tong and Koller, 2001). Common intuition equates
large margins with high prediction confidence.

However, confidence and margin are two distinct
properties. For example, an instance may receive
a large margin based on a single feature which has
been updated only a small number of times. Another
example may receive a small margin, but its features
have been learned from a large number of examples.
While the first example has a larger margin it has
low confidence compared to the second. Both the
margin value and confidence should be considered
in choosing which example to label.

We present active learning with confidence us-
ing a recently introduced online learning algo-
rithm called Confidence-Weighted linear classifica-
tion. The classifier assigns labels according to a
Gaussian distribution over margin values instead of
a single value, which arises from parameter confi-
dence (variance). The variance of this distribution
represents the confidence in the mean (margin). We
then employ this distribution for a new active learn-
ing criteria, which in turn could improve other mar-
gin based active learning techniques. Additionally,
we favor the use of an online method since online
methods have achieved good NLP performance and
are fast to train — an important property for inter-
active learning. Experimental validation on a num-
ber of datasets shows that active learning with con-
fidence can improve standard methods.

2 Confidence-Weighted Linear Classifiers

Common online learning algorithms, popular in
many NLP tasks, are not designed to deal with
the particularities of natural language data. Fea-

233



ture representations have very high dimension and
most features are observed on a small fraction of in-
stances. Confidence-weighted (CW) linear classifi-
cation (Dredze et al., 2008), a new online algorithm,
maintains a probabilistic measure of parameter con-
fidence leading to a measure of prediction confi-
dence, potentially useful for active learning. We
summarize CW learning to familiarize the reader.

Parameter confidence is formalized with a distri-
bution over weight vectors, specifically a Gaussian
distribution with mean µ ∈ RN and diagonal co-
variance Σ ∈ RN×N . The values µj and Σj,j repre-
sent knowledge of and confidence in the parameter
for feature j. The smaller Σj,j , the more confidence
we have in the mean parameter value µj .

A model predicts the label with the highest prob-
ability, maxy∈{±1} Prw∼N (µ,Σ) [y(w · x) ≥ 0] .
The Gaussian distribution over parameter vectors w
induces a univariate Gaussian distribution over the
unsigned-margin M = w · x parameterized by µ,
Σ and the instance x: M ∼ N (M,V ), where the
mean is M = µ · x and the variance V = x>Σx.

CW is an online algorithm inspired by the Passive
Aggressive (PA) update (Crammer et al., 2006) —
which ensures a positive margin while minimizing
parameter change. CW replaces the Euclidean dis-
tance used in the PA update with the KL divergence
over Gaussian distributions. It also replaces the min-
imal margin constraint with a minimal probability
constraint: with some given probability η ∈ (0.5, 1]
a drawn classifier will assign the correct label. This
strategy yields the following objective solved on
each round of learning:

(µi+1,Σi+1) = min DKL (N (µ,Σ) ‖N (µi,Σi))
s.t. Pr [yi (µ · xi) ≥ 0] ≥ η ,

where (µi,Σi) are the parameters on round i and(
µi+1,Σi+1

)
are the new parameters after update.

The constraint ensures that the resulting parameters
will correctly classify xi with probability at least η.
For convenience we write φ = Φ−1 (η), where Φ is
the cumulative function of the normal distribution.
The optimization problem above is not convex, but
a closed form approximation of its solution has the
following additive form: µi+1 = µi +αiyiΣixi and

Σ−1i+1 = Σ
−1
i + 2αiφxix

>
i for,

αi =
−(1+2φMi)+

√
(1+2φMi)

2−8φ (Mi−φVi)
4φVi

.

Each update changes the feature weights µ, and in-
creases confidence (variance Σ always decreases).

3 Active Learning with Confidence

We consider pool based active learning. An active
learning algorithm is given a pool of unlabeled in-
stances U = {xi}ni=1, a learning algorithm A and a
set of labeled examples initially set to be L = ∅ . On
each round the active learner uses its selection crite-
ria to return a single instance xi to be labeled by an
annotator with yi ∈ {−1,+1} (for binary classifica-
tion). The instance and label are added to the labeled
set L ← L ∪ {(xi, yi)} and passed to the learning
algorithm A, which in turn generates a new model.
At the end of labeling the algorithm returns a classi-
fier trained on the final labeled set. Effective active
learning minimizes prediction error and the number
of labeled examples.

Most active learners for margin based algorithms
rely on the magnitude of the margin. Tong and
Koller (2001) motivate this approach by consider-
ing the half-space representation of the hypothesis
space for learning. They suggest three margin based
active learning methods: Simple margin, MaxMin
margin, and Ratio margin. In Simple margin, the al-
gorithm predicts an unsigned margin M for each in-
stance in U and returns for labeling the instance with
the smallest margin. The intuition is that instances
for which the classifier is uncertain (small margin)
provide the most information for learning. Active
learning based on PA algorithms runs in a similar
fashion but full SVM retraining on every round is
replaced with a single PA update using the new la-
beled example, greatly increasing learning speed.

Maintaining a distribution over prediction func-
tions makes the CW algorithm attractive for ac-
tive learning. Instead of using a geometrical
quantity (“margin”), it use a probabilistic quan-
tity and picks the example whose label is pre-
dicted with the lowest probability. Formally,
the margin criteria, x = arg minz∈U (w · z),
is replaced with a probabilistic criteria x =
arg minz∈U |

(
Prw∼N (µi,Σi) [sign(w · z) = 1]

)
− 1

2
| .

234



The selection criteria naturally captures the notion
that we should label the example with the highest
uncertainty. Interestingly, we can show (omitted due
to lack of space) that the probabilistic criteria can be
translated into a corrected geometrical criteria. In
practice, we can compute this normalized margin as
M̄ = M/

√
V . We call this selection criteria Active

Confident Learning (ACL).

4 Evaluation

To evaluate our active learning methods we used
a similar experimental setup to Tong and Koller
(2001). Each active learning algorithm was given
two labeled examples, one from each class, for ini-
tial training of a classifier, and remaining data as un-
labeled examples. On each round the algorithm se-
lected a single instance for which it was then given
the correct label. The algorithm updated the online
classifier and evaluated it on held out test data to
measure learning progress.

We selected four binary NLP datasets for evalu-
ation: 20 Newsgroups1 and Reuters (Lewis et al.,
2004) (used by Tong and Koller) and sentiment clas-
sification (Blitzer et al., 2007) and spam (Bickel,
2006). For each dataset we extracted binary uni-
gram features and sentiment was prepared accord-
ing to Blitzer et al. (2007). From 20 Newsgroups
we created 3 binary decision tasks to differentiate
between two similar labels from computers, sci-
ence and talk. We created 3 similar problems from
Reuters from insurance, business services and re-
tail distribution. Sentiment used 4 Amazon domains
(book, dvd, electronics, kitchen). Spam used the
three users from task A data. Each problem had
2000 instances except for 20 Newsgroups, which
used between 1850 and 1971 instances. This created
13 classification problems across four tasks.

Each active learning algorithm was evaluated us-
ing a PA (with slack variable c = 1) or CW classifier
(φ = 1) using 10-fold cross validation. We eval-
uated several methods in the Simple margin frame-
work: PA Margin and CW Margin, which select ex-
amples with the smallest margin, and ACL. As a
baseline we included selecting a random instance.
We also evaluated CW and a PA classifier trained on
all training instances. Each method was evaluated by

1
http://people.csail.mit.edu/jrennie/20Newsgroups/

labeling up to 500 labels, about 25% of the training
data. The 10 runs on each dataset for each problem
appear in the left and middle panel of Fig. 1, which
show the test accuracy after each round of active
learning. Horizontal lines indicate CW (solid) and
PA (dashed) training on all instances. Legend num-
bers are accuracy after 500 labels. The left panel av-
erages results over 20 Newsgroups, and the middle
panel averages results over all 13 datasets.

To achieve 80% of the accuracy of training on all
data, a realistic goal for less than 100 labels, PA
Margin required 93% the number of labels of PA
Random, while CW Margin needed only 73% of
the labels of CW Random. By using fewer labels
compared to random selection baselines, CW Mar-
gin learns faster in the active learning setting as com-
pared with PA. Furthermore, adding confidence re-
duced labeling cost compared to margin alone. ACL
improved over CW Margin on every task and after
almost every round; it required 63% of the labels of
CW Random to reach the 80% mark.

We computed the fraction of labels CW Margin
and ACL required (compared to CW Random) to
achieve the 80% accuracy mark of training with all
data. The results are summarized in the right panel
of Fig. 1, where we plot one point per dataset. Points
above the diagonal-line demonstrate the superiority
of ACL over CW Margin. ACL required fewer la-
bels than CW margin twice as often as the opposite
occurred (8 vs 4). Note that CW Margin used more
labels than CW Random in three cases, while ACL
only once, and this one time only about a dozen la-
bels were needed. To conclude, not only does CW
Margin outperforms PA Margin for active-learning,
CW maintains additional valuable information (con-
fidence), which further improves performance.

5 Related Work

Active learning has been widely used for NLP tasks
such as part of speech tagging (Ringger et al., 2007),
parsing (Tang et al., 2002) and word sense disam-
biguation (Chan and Ng, 2007). Many methods rely
on entropy-based scores such as uncertainty sam-
pling (Lewis and Gale, 1994). Others use margin
based methods, such as Kim et al. (2006), who com-
bined margin scores with corpus diversity, and Sas-
sano (2002), who considered SVM active learning

235



100 150 200 250 300 350 400 450 500
Labels

0.65

0.70

0.75

0.80

0.85

0.90

0.95

T
e
st

 A
c
c
u
ra

c
y

20 Newsgroups

PA Random (82.53)
CW Random (92.92)
PA Margin (88.06)
CW Margin (95.39)
ACL (95.51)

100 150 200 250 300 350 400 450 500
Labels

0.75

0.80

0.85

0.90

T
e
st

 A
c
c
u
ra

c
y

All

PA Random (81.30)
CW Random (86.67)
PA Margin (83.99)
CW Margin (88.61)
ACL (88.79)

0.2 0.4 0.6 0.8 1.0 1.2 1.4
ACL Labels

0.2

0.4

0.6

0.8

1.0

1.2

1.4

C
W

 M
a
rg

in
 L

a
b
e
ls

Reuters
20 Newsgroups
Sentiment
Spam

Figure 1: Results averaged over 20 Newsgroups (left) and all datasets (center) showing test accuracy over active
learning rounds. The right panel shows the amount of labels needed by CW Margin and ACL to achieve 80% of the
accuracy of training on all data - each points refers to a different dataset.

for Japanese word segmentation. Our confidence
based approach can be used to improve these tasks.
Furthermore, margin methods can outperform prob-
abilistic methods; CW beats maximum entropy on
many NLP tasks (Dredze et al., 2008).

A theoretical analysis of margin based methods
selected labels that maximize the reduction of the
version space, the hypothesis set consistent with the
training data (Tong and Koller, 2001). Another ap-
proach selects instances that minimize the future er-
ror in probabilistic algorithms (Roy and McCallum,
2001). Since we consider an online learning algo-
rithm our techniques can be easily extended to on-
line active learning (Cesa-Bianchi et al., 2005; Das-
gupta et al., 2005; Sculley, 2007).

6 Conclusion

We have presented techniques for incorporating con-
fidence into the margin for active learning and have
shown that CW selects better examples than PA, a
popular online algorithm. This approach creates op-
portunities for new active learning frameworks that
depend on margin confidence.

References

S. Bickel. 2006. Ecml-pkdd discovery challenge
overview. In The Discovery Challenge Workshop.

J. Blitzer, M. Dredze, and F. Pereira. 2007. Biographies,
bollywood, boom-boxes and blenders: Domain adap-
tation for sentiment classification. In ACL.

Nicolò Cesa-Bianchi, Gábor Lugosi, and Gilles Stolt.
2005. Minimizing regret with label efficient predic-
tion. IEEE Tran. on Inf. Theory, 51(6), June.

Y. S. Chan and H. T. Ng. 2007. Domain adaptation with
active learning for word sense disambiguation. In As-
sociation for Computational Linguistics (ACL).

K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive al-
gorithms. JMLR, 7:551–585.

S. Dasgupta, A.T. Kalai, and C. Monteleoni. 2005. Anal-
ysis of perceptron-based active learning. In COLT.

Mark Dredze, Koby Crammer, and Fernando Pereira.
2008. Confidence-weighted linear classification. In
ICML.

S. Kim, Yu S., K. Kim, J-W Cha, and G.G. Lee. 2006.
Mmr-based active machine learning for bio named en-
tity recognition. In NAACL/HLT.

D. D. Lewis and W. A. Gale. 1994. A sequential algo-
rithm for training text classifiers. In SIGIR.

D. D. Lewis, Y. Yand, T. Rose, and F. Li. 2004. Rcv1:
A new benchmark collection for text categorization re-
search. JMLR, 5:361–397.

E. Ringger, P. McClanahan, R. Haertel, G. Busby,
M. Carmen, J. Carroll, K. Seppi, and D. Lonsdale.
2007. Active learning for part-of-speech tagging: Ac-
celerating corpus annotation. In ACL Linguistic Anno-
tation Workshop.

N. Roy and A. McCallum. 2001. Toward optimal active
learning through sampling estimation of error reduc-
tion. In ICML.

Manabu Sassano. 2002. An empirical study of active
learning with support vector machines for japanese
word segmentation. In ACL.

D. Sculley. 2007. Online active learning methods for fast
label-efficient spam filtering. In CEAS.

M. Tang, X. Luo, and S. Roukos. 2002. Active learning
for statistical natural language parsing. In ACL.

S. Tong and D. Koller. 2001. Supprt vector machine
active learning with applications to text classification.
JMLR.

236


