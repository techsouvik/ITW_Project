













































Multi-domain Sentiment Classification


Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 257–260,
Columbus, Ohio, USA, June 2008. c©2008 Association for Computational Linguistics

Multi-domain Sentiment Classification 

 
Shoushan Li and Chengqing Zong 

National Laboratory of Pattern Recognition 
Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China 

{sshanli,cqzong}@nlpr.ia.ac.cn 
 

 

 

 

 

Abstract 

This paper addresses a new task in sentiment 
classification, called multi-domain sentiment 
classification, that aims to improve perform-
ance through fusing training data from multi-
ple domains. To achieve this, we propose two 
approaches of fusion, feature-level and classi-
fier-level, to use training data from multiple 
domains simultaneously. Experimental stud-
ies show that multi-domain sentiment classi-
fication using the classifier-level approach 
performs much better than single domain 
classification (using the training data indi-
vidually). 

1 Introduction 
Sentiment classification is a special task of text 
categorization that aims to classify documents 
according to their opinion of, or sentiment toward 
a given subject (e.g., if an opinion is supported or 
not) (Pang et al., 2002). This task has created a 
considerable interest due to its wide applications. 

Sentiment classification is a very domain-
specific problem; training a classifier using the 
data from one domain may fail when testing 
against data from another. As a result, real 
application systems usually require some labeled 
data from multiple domains, guaranteeing an 
acceptable performance for different domains. 
However, each domain has a very limited amount 
of training data due to the fact that creating large-
scale high-quality labeled corpora is difficult and 
time-consuming.  Given the limited multi-domain 
training data, an interesting task arises, how to 
best make full use of all training data to improve 
sentiment classification performance. We name 

this new task, ‘multi-domain sentiment 
classification’. 

In this paper, we propose two approaches to 
multi-domain sentiment classification. In the first, 
called feature-level fusion, we combine the feature 
sets from all the domains into one feature set. 
Using the unified feature set, we train a classifier 
using all the training data regardless of domain. In 
the second approach, classifier-level fusion, we 
train a base classifier using the training data from 
each domain and then apply combination methods 
to combine the base classifiers. 

2 Related Work 
Sentiment classification has become a hot topic 
since the publication work that discusses classifi-
cation of movie reviews by Pang et al. (2002). 
This was followed by a great many studies into 
sentiment classification focusing on many do-
mains besides that of movie. 

Research into sentiment classification over 
multiple domains remains sparse. It is worth not-
ing that Blitzer et al. (2007) deal with the domain 
adaptation problem for sentiment classification 
where labeled data from one domain is used to 
train a classifier for classifying data from a differ-
ent domain. Our work focuses on the problem of 
how to make multiple domains ‘help each other’ 
when all contain some labeled samples. These two 
problems are both important for real applications 
of sentiment classification. 

3 Our Approaches 

3.1 Problem Statement 
In a standard supervised classification problem, 
we seek a predictor f (also called a classifier) that 

257



maps an input vector x to the corresponding class 
label y. The predictor is trained on a finite set of 
labeled examples { ( , )i iX Y } (i=1,…,n) and its 
objective is to minimize expected error, i.e., 

arg min ( ( ), )
n

i i
f i

f L f X Y
∈

= ∑
Η

 

Where L is a prescribed loss function and H is a 
set of functions called the hypothesis space, which 
consists of functions from x to y. In sentiment 
classification, the input vector of one document is 
constructed from weights of terms. The terms 

1( ,..., )Nt t  are possibly words, word n-grams, or 
even phrases extracted from the training data, with 
N being the number of terms. The output label y 
has a value of 1 or -1 representing a positive or 
negative sentiment classification. 

  In multi-domain classification, m different 
domains are indexed by k={1,…,m}, each with 

kn training samples ( , )k ki iX Y {1,..., }k ki n= . A 

straightforward approach is to train a predictor kf  
for the k-th domain only using the training 
data {( , )}

k ki i
X Y . We call this approach single 

domain classification and show its architecture in 
Figure 1. 

 
Figure 1: The architecture of single domain classifica-
tion. 

3.2 Feature-level Fusion Approach 
Although terms are extracted from multiple do-
mains, some occur in all domains and convey the 
same sentiment (this can be called global senti-
ment information). For example, some terms like 
‘excellent’ and ‘perfect’ express positive senti-
ment information independent of domain. To learn 
the global sentiment information more correctly, 
we can pool the training data from all domains for 
training. Our first approach is using a common set 
of terms 1( ' ,..., ' )allNt t  to construct a uniform fea-
ture vector 'x  and then train a predictor using all 
training data: 

1 1
arg min ( ( ' ), )

k

k k
all all k

nm

all i i
f k i

f L f X Y
∈ = =

= ∑∑
Η

 

We call this approach feature-level fusion and 
show its architecture in Figure 2. The common set 
of terms is the union of the term sets from 
multiple domains.  

 
Figure 2: The architecture of the feature-level fusion 
approach  
 

Feature-level fusion approach is simple to 
implement and needs no extra labeled data. Note 
that training data from different domains 
contribute differently to the learning process for a 
specific domain. For example, given data from 
three domains, books, DVDs and kitchen, we 
decide to train a classifier for classifying reviews 
from books. As the training data from DVDs is 
much more similar to books than that from 
kitchen (Blitzer et al., 2007), we should give the 
data from DVDs a higher weight. Unfortunately, 
the feature-level fusion approach lacks the 
capacity to do this. A more qualified approach is 
required to deal with the differences among the 
classification abilities of training data from 
different domains. 

3.3 Classifier-level Fusion Approach 
As mentioned in sub-Section 2.1, single domain 
classification is used to train a single classifier for 
each domain using the training data in the corre-
sponding domain. As all these single classifiers 
aim to determine the sentiment orientation of a 
document, a single classifier can certainly be used 
to classify documents from other domains. Given 
multiple single classifiers, our second approach is 
to combine them to be a multiple classifier system 
for sentiment classification. We call this approach 
classifier-level fusion and show its architecture in 
Figure 3. This approach consists of two main steps: 

Training Data 
from Domain 1 

Training Data 
from Domain 2 

Training Data 
from Domain m

Classifier  
1 

Classifier 
2 

Classifier 
m 

Testing Data 
from Domain 1 

Testing Data 
from Domain 2 

Testing Data 
from Domain m

   . . . 

   . . . 

   . . . 

Training Data 
from Domain 1

Training Data 
from Domain 2 

Training Data 
from Domain m

Classifier 

Testing Data 
from Domain 1

Testing Data 
from Domain 2

Testing Data 
from Domain m

   . . . 

   . . . 

Training Data from all Domains 
using a Uniform Feature Vector 

258



(1) train multiple base classifiers (2) combine the 
base classifiers. In the first step, the base classifi-
ers are multiple single classifiers kf  (k=1,…,m) 
from all domains. In the second step, many com-
bination methods can be applied to combine the 
base classifiers. A well-known method called 
meta-learning (ML) has been shown to be very 
effective (Vilalta and Drissi, 2002). The key idea 
behind this method is to train a meta-classifier 
with input attributes that are the output of the base 
classifiers. 

 
Figure 3: The architecture of the classifier-level fusion 
approach 
 

Formally, let 'kX denote a feature vector of a 
sample from the development data of the 

'-thk domain ( ' 1,..., )k m= . The output of the 
-thk base classifier kf on this sample is the 

probability distribution over the set of classes 
1 2{ , ,..., }nc c c , i.e., 

' 1 ' '( )  ( | ),..., ( | )k k k k k n kp X p c X p c X= < >  
For the '-thk domain, we train a meta-classifier 

'  ( ' 1,..., )kf k m= using the development data from 
the '-thk domain with the meta-level feature 
vector '

meta m n
kX R

⋅∈  

' 1 ' ' ' ( ),..., ( ),..., ( )
meta
k k k k m kX p X p X p X= < >  

Each meta-classifier is then used to test the testing 
data from the same domain.  

Different from the feature-level approach, the 
classifier-level approach treats the training data 
from different domains individually and thus has 
the ability to take the differences in classification 
abilities into account. 

4 Experiments 
Data Set:  We carry out our experiments on the 
labeled product reviews from four domains: books, 
DVDs, electronics, and kitchen appliances1. Each 
domain contains 1,000 positive and 1,000 
negative reviews.  

Experiment Implementation: We apply SVM 
algorithm to construct our classifiers which has 
been shown to perform better than many other 
classification algorithms (Pang et al., 2002). Here, 
we use LIBSVM2 with a linear kernel function for 
training and testing. In our experiments, the data 
in each domain are partitioned randomly into 
training data, development data and testing data 
with the proportion of 70%, 20% and 10% 
respectively. The development data are used to 
train the meta-classifier. 

Baseline: The baseline uses the single domain 
classification approach mentioned in sub-Section 
2.1. We test four different feature sets to construct 
our feature vector. First, we use unigrams (e.g., 
‘happy’) as features and perform the standard fea-
ture selection process to find the optimal feature 
set of unigrams (1Gram). The selection method is 
Bi-Normal Separation (BNS) that is reported to be 
excellent in many text categorization tasks (For-
man, 2003). The criterion of the optimization is to 
find the set of unigrams with the best performance 
on the development data through selecting the 
features with high BNS scores.  Then, we get the 
optimal word bi-gram (e.g., ‘very happy’) (2Gram) 
and mixed feature set (1+2Gram) in the same way. 
The fourth feature set (1Gram+2Gram) also con-
sists of unigrams and bi-grams just like the third 
one. The difference between them lies in their se-
lection strategy. The third feature set is obtained 
through selecting the unigrams and bi-grams with 
high BNS scores while the fourth one is obtained 
through simply uniting the two optimal sets of 
1Gram and 2Gram.  

From Table 1, we see that 1Gram+2Gram fea-
tures perform much better than other types of fea-
tures, which implies that we need to select good 
unigram and bi-gram features separately before 
combine them. Although the size of our training 
data are smaller than that reported in Blitzer et al. 
                                                           
1 This data set is collected by Blitzer et al. (2007): 
http://www.seas.upenn.edu/~mdredze/datasets/sentiment/ 
2 LIBSVM is an integrated software for SVM: 
http://www.csie.ntu.edu.tw/~cjlin/libsvm/ 

Training Data 
from Domain 1 

Training Data 
from Domain 2 

Training Data 
from Domain m

Multiple Classifier 
System 1 

Testing Data 
from Domain 1 

Testing Data 
from Domain 2 

Testing Data 
from Domain m

   . . . 

   . . . 

Base Classifier 
1 

Base Classifier 
2 

Base Classifier
m 

   . . . 

Multiple Classifier 
System 2 

Multiple Classifier 
System m 

Development Data 
from Domain 1 

Development Data 
from Domain 2 

Development Data 
from Domain m 

   . . . 

   . . . 

259



(2007) (70% vs. 80%), the classification perform-
ance is comparative to theirs. 

 
We implement the fusion using 1+2Gram and 

1Gram+2Gram respectively. From Figure 4, we 
see that both the two fusion approaches generally 
outperform single domain classification when us-
ing 1+2Gram features. They increase the average 
accuracy from 0.8 to 0.82375 and 0.83875, a sig-
nificant relative error reduction of 11.87% and 
19.38% over baseline.  

1+2Gram Features

76.5

81
80

8382.5 82.5 82.5
81

83 84

86

83

72

74

76

78

80

82

84

86

88

Books DVDs Electronics Kitchen

A
c
c
u
r
a
c
y
(
%
)

  
1Gram+2Gram Features

79

84.584

82

84.5
85

83
82

83.5

86

88
89

74

76

78

80

82

84

86

88

90

Books DVDs Electronics Kitchen

A
c
c
u
r
a
c
y
(
%
)

Single domain classification

Feature-level fusion

Classifier-level fusion with ML

 Figure 4: Accuracy results on the testing data using 
multi-domain classification with different approaches. 

 
However, when the performance of baseline in-

creases, the feature level approach fails to help the 
performance improvement in three domains. This 
is mainly because the base classifiers perform ex-
tremely unbalanced on the testing data of these 
domains. For example, the four base classifiers 
from Books, DVDs, Electronics, and Kitchen 
achieve the accuracies of 0.675, 0.62, 0.85, and 

0.79 on the testing data from Electronics respec-
tively. Dealing with such an unbalanced perform-
ance, we definitely need to put enough high 
weight on the training data from Electronics. 
However, the feature-level fusion approach sim-
ply pools all training data from different domains 
and treats them equally. Thus it can not capture 
the unbalanced information. In contrast, meta-
learning is able to learn the unbalance automati-
cally through training the meta-classifier using the 
development data. Therefore, it can still increase 
the average accuracy from 0.8325 to 0.8625, an 
impressive relative error reduction of 17.91% over 
baseline. 

5 Conclusion 
In this paper, we propose two approaches to multi-
domain classification task on sentiment classifica-
tion. Empirical studies show that the classifier-
level approach generally outperforms the feature 
approach.  Compared to single domain classifica-
tion, multi-domain classification with the classi-
fier-level approach can consistently achieve much 
better results. 

Acknowledgments 
The research work described in this paper has 
been partially supported by the Natural Science 
Foundation of China under Grant No. 60575043, 
and 60121302, National High-Tech Research and 
Development Program of China under Grant No. 
2006AA01Z194, National Key Technologies 
R&D Program of China under Grant No. 
2006BAH03B02, and Nokia (China) Co. Ltd as 
well. 

References 
J. Blitzer, M. Dredze, and F. Pereira. 2007. Biographies, 

Bollywood, Boom-boxes and Blenders: Domain ad-
aptation for sentiment classification. In Proceedings 
of ACL.  

G. Forman. 2003. An extensive empirical study of fea-
ture selection metrics for text classification. Journal 
of Machine Learning Research, 3: 1533-7928. 

B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs 
up? Sentiment classification using machine learning 
techniques. In Proceedings of EMNLP. 

R. Vilalta and Y. Drissi. 2002. A perspective view and 
survey of meta-learning. Artificial Intelligence Re-
view, 18(2): 77–95. 

Features Books DVDs Elec-
tronic 

Kitchen

1Gram 0.75 0.84 0.8 0.825 
2Gram 0.75 0.73 0.815 0.785 
1+2Gram 0.765 0.81 0.825 0.80 

1Gram+2Gram 0.79 0.845 0.85 0.845 
 

Table 1: Accuracy results on the testing data of single 
domain classification using different feature sets. 

260


