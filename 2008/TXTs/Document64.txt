













































Robustness and Generalization of Role Sets: PropBank vs. VerbNet


Proceedings of ACL-08: HLT, pages 550–558,
Columbus, Ohio, USA, June 2008. c©2008 Association for Computational Linguistics

Robustness and Generalization of Role Sets: PropBank vs. VerbNet

Beñat Zapirain and Eneko Agirre
IXA NLP Group

University of the Basque Country
{benat.zapirain,e.agirre}@ehu.es

Lluı́s Màrquez
TALP Research Center

Technical University of Catalonia
lluism@lsi.upc.edu

Abstract

This paper presents an empirical study on the
robustness and generalization of two alterna-
tive role sets for semantic role labeling: Prop-
Bank numbered roles and VerbNet thematic
roles. By testing a state–of–the–art SRL sys-
tem with the two alternative role annotations,
we show that the PropBank role set is more
robust to the lack of verb–specific semantic
information and generalizes better to infre-
quent and unseen predicates. Keeping in mind
that thematic roles are better for application
needs, we also tested the best way to generate
VerbNet annotation. We conclude that tagging
first PropBank roles and mapping into Verb-
Net roles is as effective as training and tagging
directly on VerbNet, and more robust for do-
main shifts.

1 Introduction

Semantic Role Labeling is the problem of analyzing
clause predicates in open text by identifying argu-
ments and tagging them with semantic labels indi-
cating the role they play with respect to the verb.
Such sentence–level semantic analysis allows to de-
termine “who” did “what” to “whom”, “when” and
“where”, and, thus, characterize the participants and
properties of the events established by the predi-
cates. This kind of semantic analysis is very inter-
esting for a broad spectrum of NLP applications (in-
formation extraction, summarization, question an-
swering, machine translation, etc.), since it opens
the door to exploit the semantic relations among lin-
guistic constituents.

The properties of the semantically annotated cor-
pora available have conditioned the type of research

and systems that have been developed so far. Prop-
Bank (Palmer et al., 2005) is the most widely used
corpus for training SRL systems, probably because
it contains running text from the Penn Treebank cor-
pus with annotations on all verbal predicates. Also,
a few evaluation exercises on SRL have been con-
ducted on this corpus in the CoNLL-2004 and 2005
conferences. However, a serious criticisms to the
PropBank corpus refers to the role set it uses, which
consists of a set of numbered core arguments, whose
semantic translation is verb-dependent. While Arg0
and Arg1 are intended to indicate the general roles
of Agent and Theme, other argument numbers do
not generalize across verbs and do not correspond
to general semantic roles. This fact might compro-
mise generalization and portability of SRL systems,
especially when the training corpus is small.

More recently, a mapping from PropBank num-
bered arguments into VerbNet thematic roles has
been developed and a version of the PropBank cor-
pus with thematic roles has been released (Loper et
al., 2007). Thematic roles represent a compact set of
verb-independent general roles widely used in lin-
guistic theory (e.g., Agent, Theme, Patient, Recipi-
ent, Cause, etc.). We foresee two advantages of us-
ing such thematic roles. On the one hand, statisti-
cal SRL systems trained from them could generalize
better and, therefore, be more robust and portable,
as suggested in (Yi et al., 2007). On the other hand,
roles in a paradigm like VerbNet would allow for in-
ferences over the assigned roles, which is only pos-
sible in a more limited way with PropBank.

In a previous paper (Zapirain et al., 2008), we pre-
sented a first comparison between the two previous
role sets on the SemEval-2007 Task 17 corpus (Prad-
han et al., 2007). The SemEval-2007 corpus only

550



comprised examples about 50 different verbs. The
results of that paper were, thus, considered prelim-
inary, as they could depend on the small amount of
data (both in training data and number of verbs) or
the specific set of verbs being used. Now, we ex-
tend those experiments to the entire PropBank cor-
pus, and we include two extra experiments on do-
main shifts (using the Brown corpus as test set) and
on grouping VerbNet labels. More concretely, this
paper explores two aspects of the problem. First,
having in mind the claim that general thematic roles
should be more robust to changing domains and
unseen predicates, we study the performance of a
state-of-the-art SRL system trained on either codi-
fication of roles and some specific settings, i.e. in-
cluding/excluding verb-specific information, label-
ing unseen verb predicates, or domain shifts. Sec-
ond, assuming that application scenarios would pre-
fer dealing with general thematic role labels, we ex-
plore the best way to label a text with thematic roles,
namely, by training directly on VerbNet roles or by
using the PropBank SRL system and perform a pos-
terior mapping into thematic roles.

The results confirm our preliminary findings (Za-
pirain et al., 2008). We observe that the PropBank
roles are more robust in all tested experimental con-
ditions, i.e., the performance decrease is more se-
vere for VerbNet. Besides, tagging first PropBank
roles and then mapping into VerbNet roles is as ef-
fective as training and tagging directly on VerbNet,
and more robust for domain shifts.

The rest of the paper is organized as follows: Sec-
tion 2 contains some background on PropBank and
VerbNet role sets. Section 3 presents the experimen-
tal setting and the base SRL system used for the role
set comparisons. In Section 4 the main compara-
tive experiments on robustness are described. Sec-
tion 5 is devoted to analyze the posterior mapping of
PropBank outputs into VerbNet thematic roles, and
includes results on domain–shift experiments using
Brown as test set. Finally, Sections 6 and 7 contain
a discussion of the results.

2 Corpora and Semantic Role Sets

The PropBank corpus is the result of adding a se-
mantic layer to the syntactic structures of Penn Tree-
bank II (Palmer et al., 2005). Specifically, it pro-

vides information about predicate-argument struc-
tures to all verbal predicates of the Wall Street Jour-
nal section of the treebank. The role set is theory–
neutral and consists of a set of numbered core ar-
guments (Arg0, Arg1, ..., Arg5). Each verb has a
frameset listing its allowed role labels and mapping
each numbered role to an English-language descrip-
tion of its semantics.

Different senses for a polysemous verb have dif-
ferent framesets, but the argument labels are seman-
tically consistent in all syntactic alternations of the
same verb–sense. For instance in “Kevin broke [the
window]Arg1 ” and in “[The door]Arg1 broke into a
million pieces”, for the verb broke.01, both Arg1 ar-
guments have the same semantic meaning, that is
“broken entity”. Nevertheless, argument labels are
not necessarily consistent across different verbs (or
verb senses). For instance, the same Arg2 label is
used to identify the Destination argument of a propo-
sition governed by the verb send and the Beneficiary
argument of the verb compose. This fact might com-
promise generalization of systems trained on Prop-
Bank, which might be focusing too much on verb–
specific knowledge. It is worth noting that the two
most frequent arguments, Arg0 and Arg1, are in-
tended to indicate the general roles of Agent and
Theme and are usually consistent across different
verbs. However, this correspondence is not total.
According to the study by (Yi et al., 2007), Arg0
corresponds to Agent 85.4% of the time, but also
to Experiencer (7.2%), Theme (2.1%), and Cause
(1.9%). Similarly, Arg1 corresponds to Theme in
47.0% of the occurrences but also to Topic (23.0%),
Patient (10.8%), and Product (2.9%), among others.
Contrary to core arguments, adjuncts (Temporal and
Location markers, etc.) are annotated with a closed
set of general and verb-independent labels.

VerbNet (Kipper et al., 2000) is a computational
verb lexicon in which verbs are organized hier-
archically into classes depending on their syntac-
tic/semantic linking behavior. The classes are based
on Levin’s verb classes (Levin, 1993) and each con-
tains a list of member verbs and a correspondence
between the shared syntactic frames and the se-
mantic information, such as thematic roles and se-
lectional constraints. There are 23 thematic roles
(Agent, Patient, Theme, Experiencer, Source, Ben-
eficiary, Instrument, etc.) which, unlike the Prop-

551



Bank numbered arguments, are considered as gen-
eral verb-independent roles.

This level of abstraction makes them, in princi-
ple, better suited (compared to PropBank numbered
arguments) for being directly exploited by general
NLP applications. But, VerbNet by itself is not an
appropriate resource to train SRL systems. As op-
posed to PropBank, the number of tagged examples
is far more limited in VerbNet. Fortunately, in the
last years a twofold effort has been made in order
to generate a large corpus fully annotated with the-
matic roles. Firstly, the SemLink1 resource (Loper
et al., 2007) established a mapping between Prop-
Bank framesets and VerbNet thematic roles. Sec-
ondly, the SemLink mapping was applied to a repre-
sentative portion of the PropBank corpus and man-
ually disambiguated (Loper et al., 2007). The re-
sulting corpus is currently available for the research
community and makes possible comparative studies
between role sets.

3 Experimental Setting

3.1 Datasets

The data used in this work is the benchmark corpus
provided by the SRL shared task of CoNLL-2005
(Carreras and Màrquez, 2005). The dataset, of over
1 million tokens, comprises PropBank sections 02–
21 for training, and sections 24 and 23 for develop-
ment and test, respectively. From the input informa-
tion, we used part of speech tags and full parse trees
(generated using Charniak’s parser) and discarded
named entities. Also, we used the publicly avail-
able SemLink mapping from PropBank into Verb-
Net roles (Loper et al., 2007) to generate a replicate
of the CoNLL-2005 corpus containing also the Verb-
Net annotation of roles.

Unfortunately, SemLink version 1.0 does not
cover all propositions and arguments in the Prop-
Bank corpus. In order to have an homogeneous cor-
pus and not to bias experimental evaluation, we de-
cided to discard all incomplete examples and keep
only those propositions that were 100% mapped into
VerbNet roles. The resulting corpus contains 56% of
the original propositions, that is, over 50,000 propo-
sitions in the training set. This subcorpus is much
larger than the SemEval-2007 Task 17 dataset used

1http://verbs.colorado.edu/semlink/

in our previous experimental work (Zapirain et al.,
2008). The difference is especially noticeable in
the diversity of predicates represented. In this case,
there are 1,709 different verbs (1,505 lemmas) com-
pared to the 50 verbs of the SemEval corpus. We
believe that the size and richness of this corpus is
enough to test and extract reliable conclusions on
the robustness and generalization across verbs of the
role sets under study.

In order to study the behavior of both role sets
in out–of–domain data, we made use of the Prop-
Banked Brown corpus (Marcus et al., 1994) for test-
ing, as it is also mapped into VerbNet thematic roles
in the SemLink resource. Again, we discarded those
propositions that were not entirely mapped into the-
matic roles (45%).

3.2 SRL System

Our basic Semantic Role Labeling system represents
the tagging problem as a Maximum Entropy Markov
Model (MEMM). The system uses full syntactic
information to select a sequence of constituents
from the input text and tags these tokens with Be-
gin/Inside/Outside (BIO) labels, using state-of-the-
art classifiers and features. The system achieves very
good performance in the CoNLL-2005 shared task
dataset and in the SRL subtask of the SemEval-2007
English lexical sample task (Zapirain et al., 2007).
Check this paper for a complete description of the
system.

When searching for the most likely state se-
quence, the following constraints are observed2:

1. No duplicate argument classes for Arg0–Arg5
PropBank (or VerbNet) roles are allowed.

2. If there is a R-X argument (reference), then
there has to be a X argument before (referent).

3. If there is a C-X argument (continuation), then
there has to be a X argument before.

4. Before a I-X token, there has to be a B-X or I-X
token.

5. Given a predicate, only the arguments de-
scribed in its PropBank (or VerbNet) lexical en-
try (i.e., the verbal frameset) are allowed.

2Note that some of the constraints are dependent of the role
set used, i.e., PropBank or VerbNet

552



Regarding the last constraint, the lexical entries
of the verbs were constructed from the training data
itself. For instance, the verb build appears with
four different PropBank core roles (Arg0–3) and five
VerbNet roles (Product, Material, Asset, Attribute,
Theme), which are the only ones allowed for that
verb at test time. Note that in the cases where the
verb sense was known we could constraint the pos-
sible arguments to those that appear in the lexical en-
try of that sense, as opposed of using the arguments
that appear in all senses.

4 On the Generalization of Role Sets

We first seek a basic reference of the comparative
performance of the classifier on each role set. We
devised two settings based on our dataset. In the
first setting (‘SemEval’) we use all the available in-
formation provided in the corpus, including the verb
senses in PropBank and VerbNet. This information
was available both in the training and test, and was
thus used as an additional feature by the classifier
and to constrain further the possible arguments when
searching for the most probable Viterbi path. We call
this setting ‘SemEval’ because the SemEval-2007
competition (Pradhan et al., 2007) was performed
using this configuration.

Being aware that, in a real scenario, the sense in-
formation will not be available, we devised the sec-
ond setting (‘CoNLL’), where the hand-annotated
verb sense information was discarded. This is the
setting used in the CoNLL 2005 shared task (Car-
reras and Màrquez, 2005).

The results for the first setting are shown in the
‘SemEval setting’ rows of Table 1. The correct,
excess, missed, precision, recall and F1 measures
are reported, as customary. The significance inter-
vals for F1 are also reported. They have been ob-
tained with bootstrap resampling (Noreen, 1989).
F1 scores outside of these intervals are assumed to
be significantly different from the related F1 score
(p < 0.05). The results for PropBank are slightly
better, which is reasonable, as the number of labels
that the classifier has to learn in the case of VerbNet
should make the task harder. In fact, given the small
difference, one could think that VerbNet labels, be-
ing more numerous, are easier to learn, perhaps be-
cause they are more consistent across verbs.

In the second setting (‘CoNLL setting’ row in
the same table) the PropBank classifier degrades
slightly, but the difference is not statistically signif-
icant. On the contrary, the drop of 1.6 points for
VerbNet is significant, and shows greater sensitivity
to the absence of the sense information for verbs.
One possible reason could be that the VerbNet clas-
sifier is more dependant on the argument filter (i.e.,
the 5th constraint in Section 3.2, which only allows
roles that occur in the verbal frameset) used in the
Viterbi search, and lacking the sense information
makes the filter less useful. In fact, we have attested
that the 5th constrain discard more than 60% of the
possible candidates for VerbNet, making the task of
the classifier easier.

In order to test this hypothesis, we run the CoNLL
setting with the 5th constraint disabled (that is, al-
lowing any argument). The results in the ‘CoNLL
setting (no 5th)’ rows of Table 1 show that the drop
for PropBank is negligible and not significant, while
the drop for VerbNet is more important, and statisti-
cally significant.

Another view of the data is obtained if we com-
pute the F1 scores for core arguments and adjuncts
separately (last two columns in Table 1). The per-
formance drop for PropBank in the first three rows
is equally distributed on both core arguments and ad-
juncts. On the contrary, the drop for VerbNet roles
is more acute in core arguments (3.7 points), while
adjuncts with the 5th constraint disabled get results
close to the SemEval setting. These results confirm
that the information in the verbal frameset is more
important in VerbNet than in PropBank, as only core
arguments are constrained in the verbal framesets.
The explanation could stem from the fact that cur-
rent SRL systems rely more on syntactic information
than pure semantic knowledge. While PropBank ar-
guments Arg0–5 are easier to distinguish on syntac-
tic grounds alone, it seems quite difficult to distin-
guish among roles like Theme and Topic unless we
have access to the specific verbal frameset. This cor-
responds nicely with the performance drop for Verb-
Net when there is less information about the verb in
the algorithm (i.e., sense or frameset).

We further analyzed the results by looking at each
of the individual core arguments and adjuncts. Ta-
ble 2 shows these results on the CoNLL setting. The
performance for the most frequent roles is similar

553



PropBank
Experiment correct excess missed precision recall F1 F1 core F1 adj.
SemEval setting 6,022 1,378 1,722 81.38 77.76 79.53 ±0.9 82.25 72.48
CoNLL setting 5,977 1,424 1,767 80.76 77.18 78.93 ±0.9 81.64 71.90
CoNLL setting (no 5th) 5,972 1,434 1,772 80.64 77.12 78.84 ±0.9 81.49 71.50
No verbal features 5,557 1,828 2,187 75.25 71.76 73.46 ±1.0 74.87 70.11
Unseen verbs 267 89 106 75.00 71.58 73.25 ±4.0 76.21 64.92

VerbNet
Experiment correct excess missed precision recall F1 F1 core F1 adj.
SemEval setting 5,927 1,409 1,817 80.79 76.54 78.61 ±0.9 81.28 71.83
CoNLL setting 5,816 1,548 1,928 78.98 75.10 76.99 ±0.9 79.44 70.20
CoNLL setting (no 5th) 5,746 1,669 1,998 77.49 74.20 75.81 ±0.9 77.60 71.67
No verbal features 4,679 2,724 3,065 63.20 60.42 61.78 ±0.9 59.19 69.95
Unseen verbs 207 136 166 60.35 55.50 57.82 ±4.3 55.04 63.41

Table 1: Basic results using PropBank (top) and VerbNet (bottom) role sets on different settings.

for both. Arg0 gets 88.49, while Agent and Expe-
riencer get 87.31 and 87.76 respectively. Arg2 gets
79.91, but there is more variation on Theme, Topic
and Patient (which get 75.46, 85.70 and 78.64 re-
spectively).

Finally, we grouped the results according to the
frequency of the verbs in the training data. Table 3
shows that both PropBank and VerbNet get decreas-
ing results for less frequent verbs. PropBank gets
better results in all frequency ranges, except for the
most frequent, which contains a single verb (say).

Overall, the results on this section point out at the
weaknesses of the VerbNet role set regarding robust-
ness and generalization. The next sections examine
further its behavior.

4.1 Generalization to Unseen Predicates

In principle, the PropBank core roles (Arg0–4) get
a different interpretation depending of the verb, that
is, the meaning of each of the roles is described sepa-
rately for each verb in the PropBank framesets. Still,
the annotation criteria used with PropBank tried to
make the two main roles (Arg0 and Arg1, which ac-
count for most of the occurrences) consistent across
verbs. On the contrary, in VerbNet all roles are com-
pletely independent of the verb, in the sense that the
interpretation of the role does not vary across verbs.
But, at the same time, each verbal entry lists the pos-
sible roles it accepts, and the combinations allowed.

This experiment tests the sensitivity of the two ap-
proaches when the SRL system encounters a verb
which does not occur in the training data. In prin-
ciple, we would expect the VerbNet semantic la-
bels, which are more independent across verbs, to be

more robust at tagging new predicates. It is worth
noting that this is a realistic scenario, even for the
verb-specific PropBank labels. Predicates which do
not occur in the training data, but do have a Prop-
Bank lexicon entry, could appear quite often in the
text to be analyzed.

For this experiment, we artificially created a test
set for unseen verbs. We chose 50 verbs at random,
and split them into 40 verbs for training and 10 for
testing (yielding 13,146 occurrences for training and
2,723 occurrences for testing; see Table 4).

The results obtained after training and testing the
classifier are shown in the last rows in Table 1. Note
that they are not directly comparable to the other re-
sults mentioned so far, as the train and test sets are
smaller. Figures indicate that the performance of the
PropBank argument classifier is considerably higher
than the VerbNet classifier, with a ∼15 point gap.

This experiment shows that lacking any informa-
tion about verbal head, the classifier has a hard time
to distinguish among VerbNet roles. In order to con-
firm this, we performed the following experiment.

4.2 Sensitivity to Verb-dependent Features

In this experiment we want to test the sensitivity of
the role sets when the classifier does not have any in-
formation of the verb predicate. We removed from
the training and testing data all the features which
make any reference to the verb, including, among
others: the surface form, lemma and POS of the
verb, and all the combined features that include the
verb form (please, refer to (Zapirain et al., 2007) for
a complete description of the feature set).

The results are shown in the ‘No verbal features’

554



CoNLL setting No verb features
PBank VNet PBank VNet

corr. F1 corr. F1 F1 F1
Overall 5977 78.93 5816 76.99 73.46 61.78
Arg0 1919 88.49 84.02
Arg1 2240 79.81 73.29
Arg2 303 65.44 48.58
Arg3 10 52.63 14.29
Actor1 44 85.44 0.00
Actor2 10 71.43 25.00
Agent 1603 87.31 77.21
Attribut. 25 71.43 50.79
Cause 51 62.20 5.61
Experien. 215 87.76 86.69
Location 31 64.58 25.00
Patient1 38 67.86 5.71
Patient 208 78.64 25.06
Patient2 21 67.74 43.33
Predicate 83 62.88 28.69
Product 44 61.97 2.44
Recipient 85 79.81 62.73
Source 29 60.42 30.95
Stimulus 39 63.93 13.70
Theme 1021 75.46 52.14
Theme1 20 57.14 4.44
Theme2 21 70.00 23.53
Topic 683 85.70 73.58
ADV 132 53.44 129 52.12 52.67 53.31
CAU 13 53.06 13 52.00 53.06 45.83
DIR 22 53.01 27 56.84 40.00 46.34
DIS 133 77.78 137 79.42 77.25 78.34
LOC 126 61.76 126 61.02 59.56 57.34
MNR 109 58.29 111 54.81 52.99 51.49
MOD 249 96.14 248 95.75 96.12 95.57
NEG 124 98.41 124 98.80 98.41 98.01
PNC 26 44.07 29 44.62 38.33 41.79
TMP 453 75.00 450 73.71 73.06 73.89

Table 2: Detailed results on the CoNLL setting. Refer-
ence arguments and verbs have been omitted for brevity,
as well as those with less than 10 occ. The last two
columns refer to the results on the CoNLL setting with
no verb features.

Freq. PBank VNet Freq. PBank VNet
0-50 74,21 71,11 500-900 77,97 75,77
50-100 74,79 71,83 > 900 91,83 92,23
100-500 77,16 75,41

Table 3: F1 results split according to the frequency of the
verb in the training data.

Train affect, announce, ask, attempt, avoid, believe, build, care,
cause, claim, complain, complete, contribute, describe,
disclose, enjoy, estimate, examine, exist, explain, express,
feel, fix, grant, hope, join, maintain, negotiate, occur,
prepare, promise, propose, purchase, recall, receive,
regard, remember, remove, replace, say

Test allow, approve, buy, find, improve, kill, produce, prove,
report, rush

Table 4: Verbs used in the unseen verb experiment

rows of Table 1. The performance drops more than
5 points in PropBank, but the drop for VerbNet is
dramatic, with more than 15 points.

A closer look at the detailed role-by-role perfor-
mances can be done if we compare the F1 rows in the
CoNLL setting and in the ‘no verb features’ setting
in Table 2. Those results show that both Arg0 and
Arg1 are quite robust to the lack of target verb in-
formation, while Arg2 and Arg3 get more affected.
Given the relatively low number of Arg2 and Arg3
arguments, their performance drop does not affect
so much the overall PropBank performance. In the
case of VerbNet, the picture is very different. Focus-
ing on the most frequent roles first, while the perfor-
mance drop for Experiencer, Agent and Topic is of
1, 10 and 12 points respectively, the other roles get
very heavy losses (e.g. Theme and Patient drop 23
and 50 points), and the rest of roles are barely found.
It is worth noting that the adjunct labels get very
similar performances in both PropBank and Verb-
Net cases. In fact, Table 1 in the last two rows shows
very clearly that the performance drop is caused by
the core arguments.

The better robustness of the PropBank roles can
be explained by the fact that, when creating Prop-
Bank, the human PropBank annotators tried to be
consistent when tagging Arg0 and Arg1 across
verbs. We also think that both Arg0 and Arg1 can
be detected quite well relying on unlexicalized syn-
tactic features only, that is, not knowing which are
the verbal and nominal heads. On the other hand,
distinguishing between Arg2–4 is more dependant
on the subcategorization frame of the verb, and thus
more sensitive to the lack of verbal information.

In the case of VerbNet, the more fine-grained dis-
tinction among roles seems to depend more on the
meaning of the predicate. For instance, distinguish-
ing between Agent–Experiencer, or Theme–Topic–
Patient. The lack of the verbal head makes it much
more difficult to distinguish among those roles. The
same phenomena can be observed among the roles
not typically realized as Subject or Object such as
Recipient, Source, Product, or Stimulus.

5 Mapping into VerbNet Thematic Roles

As mentioned in the introduction, the interpretation
of PropBank roles depends on the verb, and that

555



Test on WSJ all core adj.
PropBank to VerbNet (hand) 79.17 ±0.9 81.77 72.50
VerbNet (SemEval setting) 78.61 ±0.9 81.28 71.84
PropBank to VerbNet (MF) 77.15 ±0.9 79.09 71.90
VerbNet (CoNLL setting) 76.99 ±0.9 79.44 70.88
Test on Brown
PropBank to VerbNet (MF) 64.79 ±1.0 68.93 55.94
VerbNet (CoNLL setting) 62.87 ±1.0 67.07 54.69

Table 5: Results on VerbNet roles using two different
strategies. Topmost 4 rows for the usual test set (WSJ),
and the 2 rows below for the Brown test set.

makes them less suitable for NLP applications. On
the other hand, VerbNet roles have a direct inter-
pretation. In this section, we test the performance
of two different approaches to tag input sentences
with VerbNet roles: (1) train on corpora tagged with
VerbNet, and tag the input directly; (2) train on cor-
pora tagged with PropBank, tag the input with Prop-
Bank roles, and use a PropBank to VerbNet mapping
to output VerbNet roles.

The results for the first approach are already avail-
able (cf. Table 1). For the second approach, we
just need to map PropBank roles into VerbNet roles
using SemLink (Loper et al., 2007). We devised
two experiments. In the first one we use the hand-
annotated verb class in the test set. For each predi-
cate we translate PropBank roles into VerbNet roles
making use of the SemLink mapping information
corresponding to that verb lemma and its verbal
class.

For instance, consider an occurrence of allow in a
test sentence. If the occurrence has been manually
annotated with the VerbNet class 29.5, we can use
the following entry in SemLink to add the VerbNet
role Predicate to the argument labeled with Arg1,
and Agent to the Arg0 argument.

<predicate lemma="allow">
<argmap pb-roleset="allow.01" vn-class="29.5">
<role pb-arg="1" vn-theta="Predicate" />
<role pb-arg="0" vn-theta="Agent" />

</argmap>
</predicate>

The results obtained using the hand-annotated
VerbNet classes (and the SemEval setting for Prop-
Bank), are shown in the first row of Table 5. If we
compare these results to those obtained by VerbNet
in the SemEval setting (second row of Table 5), they
are 0.5 points better, but the difference is not statis-
tically significant.

experiment corr. F1
Grouped (CoNLL Setting) 5,951 78.11±0.9
PropBank to VerbNet to Grouped 5,970 78.21±0.9

Table 6: Results for VerbNet grouping experiments.

In a second experiment, we discarded the sense
annotations from the dataset, and tried to predict the
VerbNet class of the target verb using the most fre-
quent class for the verb in the training data. Sur-
prisingly, the accuracy of choosing the most fre-
quent class is 97%. In the case of allow the most
frequent class is 29.5, so we would use the same
SemLink entry as above. The third row in Table 5
shows the results using the most frequent VerbNet
class (and the CoNLL setting for PropBank). The
performance drop compared to the use of the hand-
annotated VerbNet class is of 2 points and statisti-
cally significant, and 0.2 points above the results ob-
tained using VerbNet directly on the same conditions
(fourth row of the same Table).

The last two rows in table 5 show the results when
testing on the the Brown Corpus. In this case, the
difference is larger, 1.9 points, and statistically sig-
nificant in favor of the mapping approach. These
results show that VerbNet roles are less robust to
domain shifts. The performance drop when mov-
ing to an out–of–domain corpus is consistent with
previously published results (Carreras and Màrquez,
2005).

5.1 Grouping experiments

VerbNet roles are more numerous than PropBank
roles, and that, in itself, could cause a drop in per-
formance. Motivated by the results in (Yi et al.,
2007), we grouped the 23 VerbNet roles in 7 coarser
role groups. Note that their groupings are focused
on the roles which map to PropBank Arg2. In our
case we are interested in a more general grouping
which covers all VerbNet roles, so we added two
additional groups (Agent-Experiencer and Theme-
Topic-Patient). We re-tagged the roles in the datasets
with those groups, and then trained and tested our
SRL system on those grouped labels. The results
are shown in the first row of Table 6. In order to
judge if our groupings are easier to learn, we can
see that he performance gain with respect to the un-
grouped roles (fourth row of Table 5) is small (76.99

556



vs. 78.11) but significant. But if we compare them
to the results of the PropBank to VerbNet mapping,
where we simply substitute the fine-grained roles by
their corresponding groups, we see that they still lag
behind (second row in Table 6).

Although one could argue that better motivated
groupings could be proposed, these results indicate
that the larger number of VerbNet roles does not ex-
plain in itself the performance difference when com-
pared to PropBank.

6 Related Work

As far as we know, there are only two other works
performing comparisons of alternative role sets on
a common test data. Gildea and Jurafsky (2002)
mapped FrameNet frame elements into a set of ab-
stract thematic roles (i.e., more general roles such as
Agent, Theme, Location), and concluded that their
system could use these thematic roles without degra-
dation in performance.

(Yi et al., 2007) is a closely related work. They
also compare PropBank and VerbNet role sets, but
they focus on the performance of Arg2. They show
that splitting Arg2 instances into subgroups based on
VerbNet thematic roles improves the performance of
the PropBank-based classifier. Their claim is that
since VerbNet uses argument labels that are more
consistent across verbs, they would provide more
consistent training instances which would general-
ize better, especially to new verbs and genres. In fact
they get small improvements in PropBank (WSJ)
and a large improvement when testing on Brown.

An important remark is that Yi et al. use a com-
bination of grouped VerbNet roles (for Arg2) and
PropBank roles (for the rest of arguments). In con-
trast, our study compares both role sets as they stand,
without modifications or mixing. Another difference
is that they compare the systems based on the Prop-
Bank roles —by mapping the output VerbNet labels
back to PropBank Arg2— while in our case we de-
cided to do just the contrary (i.e., mapping PropBank
output into VerbNet labels and compare there). As
we already said, we think that VerbNet–based labels
can be more useful for NLP applications, so our tar-
get is to have a SRL system that provides VerbNet
annotations. While not in direct contradiction, both
studies show different angles of the complex relation

between the two role sets.

7 Conclusion and Future work

In this paper we have presented a study of the per-
formance of a state-of-the-art SRL system trained
on two alternative codifications of roles (PropBank
and VerbNet) and some particular settings, e.g., in-
cluding/excluding verb–specific information in fea-
tures, labeling of infrequent and unseen verb pred-
icates, and domain shifts. We observed that Prop-
Bank labeling is more robust in all previous experi-
mental conditions, showing less performance drops
than VerbNet labels.

Assuming that application-based scenarios would
prefer dealing with general thematic role labels, we
explore the best way to label a text with VerbNet
thematic roles, namely, by training directly on Verb-
Net roles or by using the PropBank SRL system
and performing a posterior mapping into thematic
roles. While results are similar and not statistically
significant in the WSJ test set, when testing on the
Brown out–of–domain test set the difference in favor
of PropBank plus mapping step is statistically signif-
icant. We also tried to map the fine-grained VerbNet
roles into coarser roles, but it did not yield better re-
sults than the mapping from PropBank roles. As a
side-product, we show that a simple most frequent
sense disambiguation strategy for verbs is sufficient
to provide excellent results in the PropBank to Verb-
Net mapping.

Regarding future work, we would like to explore
ways to improve the performance on VerbNet roles,
perhaps using selectional preferences. We also want
to work on the adaptation to new domains of both
roles sets.

Acknowledgements

We are grateful to Martha Palmer and Edward Loper
for kindly providing us with the SemLink map-
pings. This work has been partially funded by
the Basque Government (IT-397-07) and by the
Ministry of Education (KNOW TIN2006-15049,
OpenMT TIN2006-15307-C03-02). Beñat is sup-
ported by a PhD grant from the University of the
Basque Country.

557



References

Xavier Carreras and Lluı́s Màrquez. 2005. Introduction
to the CoNLL-2005 shared task: Semantic role label-
ing. In Ido Dagan and Daniel Gildea, editors, Proceed-
ings of the Ninth Conference on Computational Nat-
ural Language Learning (CoNLL-2005), pages 152–
164, Ann Arbor, Michigan, USA, June. Association
for Computational Linguistics.

Karin Kipper, Hoa Trang Dang, and Martha Palmer.
2000. Class based construction of a verb lexicon. In
Proceedings of the 17th National Conference on Arti-
ficial Intelligence (AAAI-2000), Austin, TX, July.

Beth Levin. 1993. English Verb Classes and Alterna-
tions: A Preliminary Investigation. The University of
Chicago Press, Chicago.

Edward Loper, Szu-Ting Yi, and Martha Palmer. 2007.
Combining lexical resources: Mapping between prop-
bank and verbnet. In Proceedings of the 7th In-
ternational Workshop on Computational Linguistics,
Tilburg, the Netherlands.

Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen
Katz, and Britta Schasberger. 1994. The penn tree-
bank: annotating predicate argument structure. In
HLT ’94: Proceedings of the workshop on Human
Language Technology, pages 114–119, Morristown,
NJ, USA. Association for Computational Linguistics.

Eric W. Noreen. 1989. Computer-Intensive Methods for
Testing Hypotheses. John Wiley & Sons.

Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1):71–
105.

Sameer Pradhan, Edward Loper, Dmitriy Dligach, and
Martha Palmer. 2007. Semeval-2007 task-17: En-
glish lexical sample, SRL and all words. In Proceed-
ings of the Fourth International Workshop on Seman-
tic Evaluations (SemEval-2007), pages 87–92, Prague,
Czech Republic, June. Association for Computational
Linguistics.

Szu-Ting Yi, Edward Loper, and Martha Palmer. 2007.
Can semantic roles generalize across genres? In Pro-
ceedings of the Human Language Technology Con-
ferences/North American Chapter of the Associa-
tion for Computational Linguistics Annual Meeting
(HLT/NAACL-2007).

Beñat Zapirain, Eneko Agirre, and Lluı́s Màrquez. 2007.
Sequential SRL Using Selectional Preferences. An
Approach with Maximum Entropy Markov Models. In
Proceedings of the 4th International Workshop on Se-
mantic Evaluations (SemEval-2007), pages 354–357.

Beñat Zapirain, Eneko Agirre, and Lluı́s Màrquez. 2008.
A Preliminary Study on the Robustness and General-
ization of Role Sets for Semantic Role Labeling. In

Proceedings of the 9th International Conference on
Computational Linguistics and Intelligent Text Pro-
cessing (CICLing-2008), pages 219–230, Haifa, Israel,
February.

558


