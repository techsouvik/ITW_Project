






















































Hierarchical Multi-Label Text Categorization with Global Margin Maximization


Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 165–168,
Suntec, Singapore, 4 August 2009. c©2009 ACL and AFNLP

Hierarchical Multi-Class Text Categorization
with Global Margin Maximization

Xipeng Qiu
School of Computer Science

Fudan University
xpqiu@fudan.edu.cn

Wenjun Gao
School of Computer Science

Fudan University
wjgao616@gmail.com

Xuanjing Huang
School of Computer Science

Fudan University
xjhuang@fudan.edu.cn

Abstract
Text categorization is a crucial and well-
proven method for organizing the collec-
tion of large scale documents. In this pa-
per, we propose a hierarchical multi-class
text categorization method with global
margin maximization. We not only max-
imize the margins among leaf categories,
but also maximize the margins among
their ancestors. Experiments show that the
performance of our algorithm is competi-
tive with the recently proposed hierarchi-
cal multi-class classification algorithms.

1 Introduction

In the past serval years, hierarchical text catego-
rization has become an active research topic in
database area (Koller and Sahami, 1997; Weigend
et al., 1999) and machine learning area (Rousu et
al., 2006; Cai and Hofmann, 2007).

Hierarchical categorization methods can be di-
vided in two types: local and global approaches
(Wang et al., 1999; Sun and Lim, 2001). A lo-
cal approach usually proceeds in a top-down fash-
ion, which firstly picks the most relevant cate-
gories of the top level and then recursively making
the choice among the low-level categories. The
global approach builds only one classifier to dis-
criminate all categories in a hierarchy. Due that the
global hierarchical categorization can avoid the
drawbacks about those high-level irrecoverable er-
ror, it is more popular in the machine learning do-
main.

The essential idea behind global approach is
that the close classes(nodes) have some common
underlying factors. Especially, the descendant
classes can share the characteristics of the ances-
tor classes, which is similar with multi-task learn-
ing(Caruana, 1997). A key problem for global hi-
erarchical categorization is how to combine these
underlying factors.

In this paper, we propose an method for hierar-
chical multi-class text categorization with global
margin maximization. We emphasize that it is im-
portant to separate all the nodes of the correct path
in the class hierarchy from their sibling node, then
we incorporate such information into the formula-
tion of hierarchical support vector machine.

The rest of the paper is organized as follows.
Section 2 describes the basic model of multi-class
hierarchical categorization with maximizing mar-
gin. Then we propose our improved versions in
section 3. Section 4 gives the experimental analy-
sis. Section 5 concludes the paper.

2 Hierarchical Multi-Class Text
Categorization

Multiclass SVM can be generalized to the problem
of hierarchical categorization (Cai and Hofmann,
2007), which has more than two categories in most
of the case. Denote Yi as the multilabels of xi and
Ȳi the multilabels set not in Yi. The separation
margin of w, with respect to xi, can be approxi-
mated as:

γi(w) = min
y∈Yi,ȳ∈Ȳi

〈Φ(xi,y)− Φ(xi, ȳ),w〉 (1)

The loss function can be accommodated to
multi-class SVM to scale the penalties for margin
violations proportional to the loss. This is moti-
vated by the fact that margin violations involving
an incorrect class with high loss should be penal-
ized more severely. So the cost-sensitive hierar-
chical multiclass formulation takes takes the fol-
lowing form:

min
w,ξ

1
2
||w||2 + C

n∑
i=1

ξi (2)

s.t.〈w,δΦi(y,ȳ)〉≥1− ξil(y,ȳ) , (∀i,y∈Yi,ȳ∈Ȳi)
ξi ≥ 0(∀i)

165



where δΦi(y, ȳ) = Φ(xi,y) − Φ(xi, ȳ),
l(y, ȳ) > 0 and Φ(x,y) is the joint feature of in-
put x and output y, which can be represented as:

Φ(x,y) = Λ(y)⊗ φ(x) (3)

where ⊗ is the tensor product. Λ(y) is the feature
representation of y.

Thus, we can classify a document x to label y?:

y? = arg max
y
F (w,Φ(x,y)) (4)

where F (·) is a map function.
There are different kinds of loss functions

l(y, ȳ).
One is thezero-one loss, l0/1(y,u) = [y 6= u].
Another is specially designed for the hierarchy

is tree loss(Dekel et al., 2004). Tree loss is defined
as the length of the path between two multilabels
with positive microlabels,

ltr = |path(i : yi = 1, j : uj = 1)| (5)

(Rousu et al., 2006) proposed a simplified ver-
sion of lH , namely lH̃ :

l
Ĥ

=
∑
j

cj [yj 6= uj&ypa(j) = upa(j)], (6)

that penalizes a mistake in a child only if the label
of the parent was correct. There are some different
choices for setting cj . One naive idea is to use
a uniform weighting (cj = 1). Another possible
choice is to divide the loss among the sibling:

croot = 1, cj = cParent(j)/(|Sib(j)|+ 1) (7)

Another possible choice is to scale the loss by the
proportion of the hierarchy that is in the subtree
T (j) rooted by j:

cj = |T (j)|/|T (root)| (8)

Using these scaling weights, the derived losses are
referred as l ˆuni,lŝib and l ˆsub respectively.

3 Hierarchical Multi-Class Text
Categorization with Global Margin
Maximization

In previous literature (Cai and Hofmann, 2004;
Tsochantaridis et al., 2005), they focused on sep-
arating the correct path from those incorrect path.
Inspired by the example in Figure 1, we emphasize

it is also important to separate the ancestor node in
the correct path from their sibling node.

The vector w can be decomposed in to the set
of wi for each node (category) in the hierarchy. In
Figure 1, the example hierarchy has 7 nodes and 4
of them are leaf nodes. The category is encode
as an integer, 1, . . . , 7. Suppose that the train-
ing pattern x belongs to category 4. Both w in
the Figure 1a and Figure 1b can successfully clas-
sify x into category 4, since F (w,Φ(x,y4)) =∑

1,2,4 〈wi,x〉 is the maximal among all the possi-
ble discriminate functions. So both learned param-
eter w is acceptable in current hierarchical support
vector machine.
Here we claim the w in Figure 1b is better than the
w in Figure 1a. Since we notice in Figure 1a, the
discriminate function 〈w2,x〉 is smaller than the
discriminate function 〈w3,x〉. The discriminate
function 〈wi,x〉 measures the similarity of x to
category i. The larger the discriminate function is,
the more similar x is to category i. Since category
2 is in the path from the root to the correct cate-
gory and category 3 is not, intuitively, x should be
closer to category 2 than category 3. But the dis-
criminate function in Figure 1a is contradictive to
this assumption. But such information is reflected
correctly in Figure 1b. So we conclude w in Fig.
1b is superior to w in 1a.

Here we propose a novel formulation to incor-
porate such information. Denote Ai as the mul-
tilabel in Yi that corresponds to the nonleaf cate-
gories and Sib(z) denotes the sibling nodes of z,
that is the set of nodes that have the same parent
with z, except z itself. Implementing the above
idea, we can get the following formulation:

min
w,ξ,ζ

1
2
‖w‖2 + C1

∑
i

ξi + C2
∑
i

ζi (9)

s.t.〈w, δΦi(y, ȳ)〉 ≥ 1−
ξi

l(y, ȳ)
, (∀i, y ∈ Yi

ȳ ∈ Ȳi
)

〈w, δΦi(z, z̄)〉 ≥ 1−
ζi

l(z, z̄)
, (∀i, z ∈ A(i)

z̄ ∈ Sib(z))
ξi ≥ 0(∀i)
ζi ≥ 0(∀i)

It arrives at the following Lagrangian:
L(w, ξ1, ..., ξn, ζ1, ..., ζn)

=
1

2
‖w‖2 + C1

X
i

ξi + C2
X
i

ζi

−
X
i

X
y∈Yi
ȳ∈Ȳi

αiyȳ(〈w, δΦi(y, ȳ)〉 − 1 + ξi
l(y, ȳ)

)

166



1

2 3

4 5 6 7

10,
1

xw

3,
2

xw 5,
3

xw

5,
4

xw 1,
7

xw1,
5

xw 2,
6

xw

1

2 3

4 5 6 7

10,
1

xw

7,
2

xw 3,
3

xw

5,
4

xw 1,
7

xw1,
5

xw 2,
6

xw

a) b)

Figure 1: Two different discriminant function in a hierarchy

−
X
i

X
z∈Ai

z̄∈Sib(z)

βizz̄(〈w, δΦi(z, z̄)〉 − 1 + ζi
l(z, z̄)

)

−
X
i

ciξi −
X
i

diζi (10)

The dual QP becomes

max
α

Θ(α) =
∑
i

∑
y∈Yi
ȳ∈Ȳi

αiyȳ +
∑
i

∑
z∈Ai

z̄∈Sib(z)

βizz̄

−1
2

∑
i,j

∑
y∈Yi
ȳ∈Ȳi

∑
r∈Yj
r̄∈Ȳj

θ1i,j,y,ȳ,r,r̄ (11)

−1
2

∑
i,j

∑
z∈Ai

z̄∈Sib(z)

∑
k∈Aj

k̄∈Sib(k)

θ2
i,j,z,z̄,k,k̄

,

s.t.αiyȳ ≥ 0, (12)
βjzz̄ ≥ 0, (13)∑

y∈Yi
ȳ∈Ȳi

αiyȳ
l(y, ȳ)

≤ C1, (14)

∑
z∈Ai

z̄∈Sib(z)

βizz̄
l(z, z̄)

≤ C2, (15)

where θ1i,j,y,ȳ,r,r̄ =
αiyȳαjrr̄〈δΦi(y, ȳ), δΦj(r, r̄)〉 and θ2i,j,z,z̄,k,k̄ =
βizz̄βjkk̄〈δΦi(z, z̄), δΦj(k, k̄)〉.
3.1 Optimization Algorithm
The derived QP can be very large, since the num-
ber of α and β variables is up to O(n∗2N ), where
n is number of training pattern and N is the num-
ber of nodes in the hierarchy. But two properties
of the dual problem can be exploited to design a
much more efficient optimization.

First, the constraints in the dual problem Eq. 11
- Eq. 15 factorize over the instance index for both
α-variables and β-variables. The constraints in

Eq. 14 do not couple α-variables and β-variables
together. Further, dual variables αiyȳ and αjy′ȳ′
belonging to different training instances i and j do
not join in a same constraints. This inspired an
optimization procedure which iteratively performs
subspace optimization over all dual variables αiyȳ
belonging to the same training instance. This will
in general reduced to a much smaller QP, since
it freezes all αjyȳ with j 6= i and β-variables at
their current values. This strategy can be applied
in solving β-variables.

Secondly, the number of active constraints at the
solution is expected to be relatively small, since
only a small fraction of categories ȳ ∈ Ȳi ( or
ȳ ∈ Sib(y) when y ∈ Ai) will typically fail to
achieve the required margin. The expected sparse-
ness of the variable for the dual problem can be
exploited by employing a variable selection strat-
egy. Equivalently, this corresponds to a cutting
plane algorithm for the primal QP. Intuitively, we
will identify the most violated margin constraint
with index (i,y, ȳ) and then add the correspond-
ing variable to the optimization problem. This
means that we start with extremely sparse prob-
lems and only successively increase the number of
variables in the active set. This general approach
to deal with large linear or quadratic optimization
problems is also known as column selection. In
practice, it is often not necessary to optimize until
final convergence, which adds to the attractiveness
of this approach.

We have used the LOQO optimization package
(Vanderbei, 1999) in our experiments.

4 Experiment

We evaluate our proposed model on the section D
in the WIPO-alpha collection1, which consists of
the 1372 training and 358 testing document. The

1World Intellectual Property Organization (WIPO)

167



Table 1: Prediction losses (%) obtained on WIPO.
The values per column is calculated with the dif-
ferent loss function.

XXXXXXXTrain
Test

l0/1 l∆ ltr luni lsib lsub

HSVM 48.6 188.8 94.4 97.2 5.4 7.5
l0/1 HSVM-S 48.3 186.6 93.3 96.6 5.2 7.4

HSVM 49.7 187.7 93.9 99.4 5.0 7.1
l∆ HSVM-S 47.8 165.3 89.7 90.5 4.8 6.9

HM3 70.9 167.0 - 89.1 5.0 7.0
HSVM 49.4 186.0 93.0 98.9 5.0 7.5

ltr HSVM-S 48.9 181.4 90.2 97.8 4.9 7.1
HSVM 47.2 181.0 90.5 94.4 5.0 7.0

l ˆuni HSVM-S 46.9 179.3 88.7 91.9 4.9 6.9
HM3 70.1 172.1 - 88.8 5.2 7.4
HSVM 49.4 184.9 92.5 98.9 4.8 7.4

l ˆsib HSVM-S 48.9 170.2 91.6 90.8 4.7 7.4
HM3 64.8 172.9 - 92.7 4.8 7.1
HSVM 50.6 189.9 95.0 101.1 5.2 7.5

l ˆsub HSVM-S 47.2 169.4 85.2 89.4 4.3 6.6
HM3 65.0 170.9 - 91.9 4.8 7.2

number of nodes in the hierarchy is 188, with max-
imum depth 3.

We compared the performance of our proposed
method HSVM-S with two algorithms: HSVM(Cai
and Hofmann, 2007) and HM3(Rousu et al., 2006).

4.1 Effect of Different Loss Function

We compare the methods based on different loss
functions, l0/1, l∆, ltr, lûni, lŝib and lŝub. The per-
formances for three algorithms can be seen in Ta-
ble 1. Those empty cells, denoted by “-”, are not
available in (Rousu et al., 2006).

As expected, l0/1 is inferior to other hierarchi-
cal losses by getting poorest performance in all the
testing losses, since it can not take into account the
hierarchical information between categories. The
results suggests that training with a hierarchical
losses function, like lŝib or lûni, would lead to a
better reduced l0/1 on the test set as well as in
terms of the hierarchical loss. In Table 1, we can
also point out that when training with the same
hierarchical loss, the performance of HSVM-S is
better than HSVM under the measure of most hier-
archical losses, since HSVM-S includes more hier-
archical information,the relationship between the
sibling categories, than HSVM which only separate
the leave categories.

5 Conclusion

In this paper we present a hierarchical multi-class
document categorization, which focus on maxi-
mize the margin of the classes at the different
levels in the class hierarchy. In future work, we
plan to extend the proposed hierarchical learning

method to the case where the hierarchy is a DAG
instead of tree and scale up the method further.

Acknowledgments

This work was (partially) funded by Chinese
NSF 60673038, Doctoral Fund of Ministry of
Education of China 200802460066, and Shang-
hai Science and Technology Development Funds
08511500302.

References
L. Cai and T Hofmann. 2004. Hierarchical docu-

ment categorization with support vector machines.
In Proceedings of the ACM Conference on Informa-
tion and Knowledge Management.

L. Cai and T. Hofmann. 2007. Exploiting known tax-
onomies in learning overlapping concepts. In Pro-
ceedings of International Joint Conferences on Arti-
ficial Intelligence.

R. Caruana. 1997. Multi-task learning. Machine
Learning, 28(1):41–75.

Ofer Dekel, Joseph Keshet, and Yoram Singer. 2004.
Large margin hierarchical classification. In Pro-
ceedings of the 21 st International Conference on
Machine Learning.

D. Koller and M Sahami. 1997. Hierarchically classi-
fying documents using very few words. In Proceed-
ings of the International Conference on Machine
Learning (ICML).

Juho Rousu, Craig Saunders, Sandor Szedmak, and
John Shawe-Taylor. 2006. Kernel-based learning
of hierarchical multilabel classification models. In
Journal of Machine Learning Research.

A. Sun and E.-P Lim. 2001. Hierarchical text classi-
fication and evaluation. In Proceedings of the IEEE
International Conference on Data Mining (ICDM).

Ioannis Tsochantaridis, Thorsten Joachims, Thomas
Hofmann, and Yasemin Altun. 2005. Large mar-
gin methods for structured and interdependent out-
put variables. In Journal of Machine Learning.

R. J. Vanderbei. 1999. Loqo: An interior point code
for quadratic programming. In Optimization Meth-
ods and Software.

K. Wang, S. Zhou, and S Liew. 1999. Building hier-
archical classifiers using class proximities. In Pro-
ceedings of the International Conference on Very
Large Data Bases (VLDB).

A. Weigend, E. Wiener, and J Pedersen. 1999. Exploit-
ing hierarchy in text categorization. In Information
Retrieval.

168


