






















































iChi: a bilingual dictionary generating tool


Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 217–220,
Suntec, Singapore, 4 August 2009. c©2009 ACL and AFNLP

iChi: a bilingual dictionary generating tool 

 
 

Varga István 
Yamagata University,  

Graduate School of Science and Engineering 

dyn36150@dip.yz.yamagata-u.ac.jp 

Yokoyama Shoichi 
Yamagata University,  

Graduate School of Science and Engineering 

yokoyama@yz.yamagata-u.ac.jp 

 
  

 

Abstract 

In this paper we introduce a bilingual diction-
ary generating tool that does not use any large 
bilingual corpora. With this tool we implement 
our novel pivot based bilingual dictionary 
generation method that uses mainly the 
WordNet of the pivot language to build a new 
bilingual dictionary. We propose the usage of 
WordNet for good accuracy, introducing also a 
double directional selection method with local 
thresholds to maximize recall.  

1 Introduction 

Bilingual dictionaries are an essential, perhaps even 
indispensable tool not only as resources for ma-
chine translation, but also in every day activities or 
language education. While such dictionaries are 
available to and from numerous widely used lan-
guages, less represented language pairs have rarely 
a reliable dictionary with good coverage. The need 
for bilingual dictionaries for these less common 
language pairs is increasing, but qualified human 
resources are scarce. Considering that in these con-
ditions manual compilation is highly costly, alter-
native methods are imperative.  

Pivot language based bilingual dictionary gen-
eration is one plausible such alternative (Tanaka 
and Umemura, 1994; Sjöbergh, 2005; Shirai and 
Yamamoto, 2001; Bond and Ogura, 2007). These 
methods do not use large bilingual corpora, thus 
being suitable for low-resourced languages. 

Our paper presents iChi, the implementation 
of our own method, an easy-to-use, customizable 
tool that generates a bilingual dictionary. 

The paper is structured as follows: first we 
briefly describe the methodological background 
of our tool, after which we describe its basic 
functions, concluding with discussions. Thor-
ough description and evaluation, including com-
parative analysis, are available in Varga and Yo-
koyama (2009).  

2 Methodological background 

2.1 Pivot based dictionary generation 

Pivot language based bilingual dictionary gen-
eration methods rely on the idea that the lookup 
of a word in an uncommon language through a 
third, intermediated language can be automated. 
Bilingual dictionaries to a third, intermediate 
language are used to link the source and target 
words. The pivot language translations of the 
source and target head words are compared, the 
suitability of the source-target word pair being 
estimated based on the extent of the common 
elements. 

There are two known problems of conven-
tional pivot methods. First, a global threshold is 
used to determine correct translation pairs. How-
ever, the scores highly depend on the entry itself 
or the number of translations in the intermediate 
language, therefore there is a variance in what 
that score represents. Second, current methods 
perform a strictly lexical overlap of the source-
intermediate and target-intermediate entries. 
Even if the translations from the source and tar-
get languages are semantically transferred to the 
intermediate language, lexically it is rarely the 
case. However, due to the different word-usage 
or paraphrases, even semantically identical or 
very similar words can have different definitions 
in different dictionaries. As a result, because of 
the lexical characteristic of their overlap, current 
methods cannot identify the differences between 
totally different definitions resulted by unrelated 
concepts, and differences in only nuances re-
sulted by lexicographers describing the same 
concept, but with different words. 

2.2 Specifics of our method 

To overcome the limitations, namely low preci-
sion of previous pivot methods, we expand the 
translations in the intermediate language using 

217



information extracted from WordNet (Miller et. 
al., 1990). We use the following information: 
sense description, synonymy, antonymy and se-
mantic categories, provided by the tree structure 
of nouns and verbs. 

To improve recall, we introduce bidirectional 
selection. As we stated above, the global thresh-
old eliminates a large number of good translation 
pairs, resulting in a low recall. As a solution, we 
can group the translations that share the same 
source or target entry, and set local thresholds 
for each head word. For example, for a source 
language head word entry_source there could be 
multiple target language candidates:  en-
try_target1, … ,entry_targetn. If the top scoring 
entry_targetk candidates are selected, we ensure 
that at least one translation will be available for 
entry_source, maintaining a high recall. Since we 
can group the entries in the source language and 
target language as well, we perform this selection 
twice, once in each direction. Local thresholds 
depend on the top scoring entry_target, being set 
to maxscore·c. Constant c varies between 0 and 1, 
allowing a small window for not maximum, but 
high scoring candidates. It is language and selec-
tion method dependent (See 3.2 for details). 

2.3 Brief method description 

First, using the source-pivot and pivot-target dic-
tionaries, we connect the source (s) and target (t) 
entries that share at least one common translation 
in the intermediate (i) language. We consider 
each such source-target pair a translation candi-
date. Next we eliminate erroneous candidates. 
We examine the translation candidates one by 
one, looking up the source-pivot and target-pivot 
dictionaries, comparing pivot language transla-
tions. There are six types of translations that we 
label A-F and explain below as follows. 

First, we select translation candidates whose 
translations into the intermediate language match 
perfectly (type A translations). 

For most words WordNet offers sense descrip-
tion in form of synonyms for most of its senses. 
For a given translation candidate (s,t) we look up 
the source-pivot and target-pivot translations 
(s→I={s→i1,…,s→in}, t→I={t→i1,…,t→im}). 
We select the elements that are common in the 

two definitions (I’=(s→I)∩(t→I)) and we at-
tempt to identify their respective senses from 

WordNet (sns(I’)), comparing each synonym in 
the WordNet’s synonym description with each 
word from the pivot translations. As a result, we 
arrive at a certain set of senses from the source-

pivot definitions (sns((s→I’)) and target-pivot 
definitions (sns((t→I’)). We mark scoreB(s,t) the 
Jaccard coefficient of these two sets. Scores that 
pass a global threshold (0.1) are selected as 
translation pairs. Since synonymy information is 
available for nouns (N), verbs (V), adjectives (A) 
and adverbs (R), four separate scores are calcu-
lated for each POS (type B). 

( )
( ) ( )
( ) ( )''

''
max,

' itsnsissns

itsnsissns
tsscore

ItIsi
B

→∪→

→∩→
=

→→∈ I

 (1) 

We expand the source-to-pivot and target-to-
pivot definitions with information from WordNet 
(synonymy, antonymy and semantic category). 
The similarity of the two expanded pivot lan-
guage descriptions gives a better indication on 
the suitability of the translation candidate. Since 
the same word or concept’s translations into the 
pivot language also share the same semantic 
value, the extension with synonyms 

(ext(l→i)=(l→i)∪syn(l→i), where l={s,t}) the 
extended translation should share more common 
elements (type C). 

In case of antonymy, we expand the initial 
definitions with the antonyms of the antonyms 

(ext(l→i)=(l→i)∪ant(ant(l→i)), where l={s,t}). 
This extension is different from the synonymy 
extension, in most cases the resulting set of 
words being considerably larger (type D). 

Synonymy and antonymy information are 
available for nouns, verbs, adjectives and ad-
verbs, thus four separate scores are calculated for 
each POS. 

Semantic categories are provided by the tree 
structure (hypernymy/hyponymy) of nouns and 
verbs of WordNet. We transpose each entry from 
the pivot translations to its semantic category 

(ext(l→i)=(l→i)∪semcat(l→i), where l={s,t}). 
We assume that the correct translation pairs 
share a high percentage of semantic categories. 

Local thresholds are set based on the best 
scoring candidate for a given entry. The thresh-

olds were maxscore·0.9 for synonymy and an-
tonymy; and maxscore·0.8 for the semantic cate-
gories (see §3.2 for details). 

( )
( ) ( )
( ) ( )itextisext

itextisext
tsscore EDC

→∪→

→∩→
=,,,  (2) 

For a given entry, the three separate candidate 
lists of type C, D and E selection methods re-
sulted in slightly different results. The good 
translations were among the top scoring ones, but 
not always scoring best. To correct this fault, a 
combined selection method is performed com-
bining these lists. For every translation candidate 

we select the maximum score (scorerel(s,t)) from 

218



the several POS (noun, verb, adjective and ad-
verb for synonymy and antonymy relations; noun 
and verb for semantic category) based scores, 
multiplied by a multiplication factor (mfactor). 
This factor varies between 0 and 1, awarding the 
candidates that were selected both times during 
the double directional selection; and punishing 
when selection was made only in a single direc-

tion. c1, c2 and c3 are adjustable language de-
pendent constants, the defaults being 1, 0.5 and 
0.8, respectively (type F). 

( )
( )( )( )
( )( )∏ 











⋅+

⋅+
=

rel rel

rel
F

tsmfactorcc

tsscorec
tsscore

,

,max
,

32

1
 (3) 

2.4 Evaluation 

We generated a Japanese-Hungarian dictionary 
using selection methods A, B and F; with C, D 
and E contributing indirectly through F. 

(a) Recall evaluation 

We used a Japanese frequency dictionary that we 
generated from the Japanese EDR corpus (Isa-
hara, 2007) to weight each Japanese entry. Set-
ting the standard to the frequency dictionary (its 
recall value being 100), we automatically search 
each entry from the frequency dictionary, verify-
ing whether or not it is included in the bilingual 
dictionary. If it is recalled, we weight it with its 
frequency from the frequency dictionary. 

Our method maintains the recall value of the 
initial translation candidates, owing to the bidi-
rectional selection method with local thresholds. 
However, the recall value of a manually created 
Japanese-English dictionary is higher than any 
automatically generated dictionary’s value (Ta-
ble 1). 

 

method recall 

our method 51.68 

initial candidates 51.68 

Japanese-English(*) 73.23 
Table 1: Recall evaluation results (* marks a manu-

ally created dictionary) 

 (b) 1-to-1 precision evaluation 

We evaluated 2000 randomly selected translation 
pairs, manually scoring them as correct (the 
translation conveys the same meaning, or the 
meanings are slightly different, but in a certain 
context the translation is possible: 79.15%), un-
decided (the translation pair’s semantic value is 
similar, but a translation based on them would be 
faulty: 6.15%) or wrong (the translation pair’s 
two entries convey a different meaning: 14.70%). 

 (c) 1-to-multiple evaluation 

With 1-to-multiple evaluation we quantify the 
true reliability of the dictionary: when looking up 
the meanings or translations of a certain key-
word, the user, whether he’s a human or a ma-
chine, expects all translations to be accurate. We 
evaluated 2000 randomly selected Japanese en-
tries from the initial translation candidates, scor-
ing all Hungarian translations as correct (all 
translations are correct: 71.45%), acceptable (the 
good translations are predominant, but there are 
up to 2 erroneous translations: 13.85%), wrong 
(the number or wrong translations exceeds 2: 
14.70%).  

3 iChi 

iChi is an implementation of our method. Pro-
grammed in Java, it is a platform-independent 
tool with a user friendly graphical interface (Im-
age 1). Besides the MySql database it consists of: 

iChi.jar (java executable), iChi.cfg (configura-

tion file), iChi.log (log file) and iChip.jar (pa-
rameter estimation tool). The major functions of 
iChi are briefly explained below. 
 

 
Image 1: User interface of iChi 

3.1 Resources 

The two bilingual dictionaries used as resources 
are text files, with a translation pair in each line: 

source entry 1@pivot entry 1 

source entry 2@pivot entry 2 

The location of the pivot language’s WordNet 
also needs to be specified. All paths are stored in 
the configuration file. 

3.2 Parameter settings 

iChip.jar estimates language dependent parame-
ters needed for the selection methods. Its single 
argument is a text file that contains marked (cor-
rect: $+ or incorrect: $-) translation pairs: 

219



$+source entry 1@correct target entry 1 

$-source entry 2@incorrect target entry 2 

The parameter estimation tool experiments 
with various threshold settings on the same (cor-
rect or incorrect) source entries. For example, 
with Hungarian-Japanese we considered all 
translation candidates whose Hungarian entry 

starts with “zs” (IPA: ʒ). 133 head words total-
ling 515 translation candidates comprise this set, 

273 entries being marked as correct. iChip ex-
perimented with a number of thresholds to de-
termine which ones provide with the best F-
scores, e.g. retain most marked correct transla-
tions (Table 2). The F-scores were determined as 
follows: for example using synonymy informa-
tion (type C) in case of threshold=0.85%, 343 of 
the 515 translation pairs were above the thresh-
old. Among these, 221 were marked as correct, 
thus the precision being 221/343·100=64.43 and 
the recall being 221/273·100=80.95. F-score is 
the harmonic mean of precision and recall (71.75 
in this case). 

 

threshold value (%) selection 
type 0.75 0.80 0.85 0.90 0.95 

C 70.27 70.86 71.75 72.81 66.95 

D 69.92 70.30 70.32 70.69 66.66 

E 73.71 74.90 72.52 71.62 65.09 

F 78.78 79.07 79.34 78.50 76.94 
Table 2: Selection type F-scores with varying thresh-

olds (best scores in bold) 

The output is saved into the configuration file. 
If no parameter estimation data is available, the 
parameters estimated using Hungarian-Japanese 
are used as default. 

3.3 Save settings 

The generated source-target dictionary is saved 
into a text file that uses the same format de-
scribed in §3.1. The output can be customized by 
choosing the desired selection methods. The de-
fault value is a dictionary with selection types A, 
B and F; selection types C, D and E are used 
only indirectly with type F. 

3.4 Tasks 

The tasks are run sequentially, every step being 
saved in the internal database, along with being 
logged into the log file. 

4 Discussion 

If heavily unbalanced resources dictionaries are 
used, due to the bidirectional selection method 

many erroneous entries will be generated. If one 
polysemous pivot entry has multiple translations 
into the source, but only some of them are trans-
lated into the target languages, unique, but incor-
rect source-target pairs will be generated. For 
example, with an English pivoted dictionary that 
has multiple translation of ‘bank’ onto the source 
(‘financial institution’, ‘river bank’), but only 
one into the target language (‘river bank’), the 
incorrect source(‘financial institution’)-
target(‘river bank’) pair will be generated, since 
target(‘river bank’) has no other alternative. 

Thorough discussion on recall and precision 
problems concerning the methodology of iChi, 
are available in Varga and Yokoyama (2009). 

5 Conclusions 

In this paper we presented iChi, a user friendly 
tool that uses two dictionaries into a third, inter-
mediate language together with the WordNet of 
that third language to generate a new dictionary. 
We briefly described the methodology, together 
with the basic functions. The tool is freely avail-
able online (http://mj-nlp.homeip.net/ichi). 

References  

Bond, F., Ogura, K. 2007. Combining linguistic re-
sources to create a machine-tractable Japanese-
Malay dictionary, Language Resources and 
Evaluation, 42(2), pp. 127-136. 

Breen, J.W. 1995. Building an Electric Japanese-
English Dictionary, Japanese Studies Association 
of Australia Conference, Brisbane, Queensland, 
Australia. 

Isahara, H. (2007). EDR Electronic Dictionary – pre-

sent status (EDR 電子化辞書の現状), NICT-EDR 
symposium, pp. 1-14. (in Japanese) 

Miller G.A., Beckwith R., Fellbaum C., Gross D., 
Miller K.J. (1990). Introduction to WordNet: An 
Online Lexical Database, Int J Lexicography 3(4), 
pp. 235-244. 

Sjöbergh, J. 2005. Creating a free Japanese-English 
lexicon, Proceedings of PACLING, pp. 296-300. 

Shirai, S., Yamamoto, K. 2001. Linking English 
words in two bilingual dictionaries to generate an-
other pair dictionary, ICCPOL-2001, pp. 174-179. 

Tanaka, K., Umemura, K. 1994. Construction of a 
bilingual dictionary intermediated by a third lan-
guage, Proceedings of COLING-94, pp. 297-303. 

Varga, I., Yokoyama, S. 2009. Bilingual dictionary 
generation for low-resourced language pairs, Pro-
ceedings of EMNLP 2009. 

220


