













































Co-Training for Cross-Lingual Sentiment Classification


Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 235–243,
Suntec, Singapore, 2-7 August 2009. c©2009 ACL and AFNLP

Co-Training for Cross-Lingual Sentiment Classification 

 
Xiaojun Wan 

Institute of Compute Science and Technology & Key Laboratory of Computational Lin-
guistics, MOE 

Peking University, Beijing 100871, China 
wanxiaojun@icst.pku.edu.cn 

 
 

Abstract 

The lack of Chinese sentiment corpora limits 
the research progress on Chinese sentiment 
classification. However, there are many freely 
available English sentiment corpora on the 
Web.  This paper focuses on the problem of 
cross-lingual sentiment classification, which 
leverages an available English corpus for Chi-
nese sentiment classification by using the Eng-
lish corpus as training data. Machine transla-
tion services are used for eliminating the lan-
guage gap between the training set and test set, 
and English features and Chinese features are 
considered as two independent views of the 
classification problem. We propose a co-
training approach to making use of unlabeled 
Chinese data.  Experimental results show the 
effectiveness of the proposed approach, which 
can outperform the standard inductive classifi-
ers and the transductive classifiers.  

1 Introduction 
Sentiment classification is the task of identifying 
the sentiment polarity of a given text. The senti-
ment polarity is usually positive or negative and 
the text genre is usually product review. In recent 
years, sentiment classification has drawn much 
attention in the NLP field and it has many useful 
applications, such as opinion mining and summa-
rization (Liu et al., 2005; Ku et al., 2006; Titov 
and McDonald, 2008). 

To date, a variety of corpus-based methods 
have been developed for sentiment classification. 
The methods usually rely heavily on an anno-
tated corpus for training the sentiment classifier. 
The sentiment corpora are considered as the most 
valuable resources for the sentiment classifica-
tion task. However, such resources in different 
languages are very imbalanced. Because most 
previous work focuses on English sentiment 
classification, many annotated corpora for Eng-
lish sentiment classification are freely available 
on the Web. However, the annotated corpora for 

Chinese sentiment classification are scarce and it 
is not a trivial task to manually label reliable 
Chinese sentiment corpora. The challenge before 
us is how to leverage rich English corpora for 
Chinese sentiment classification. In this study, 
we focus on the problem of cross-lingual senti-
ment classification, which leverages only English 
training data for supervised sentiment classifica-
tion of Chinese product reviews, without using 
any Chinese resources. Note that the above prob-
lem is not only defined for Chinese sentiment 
classification, but also for various sentiment 
analysis tasks in other different languages.  

Though pilot studies have been performed to 
make use of English corpora for subjectivity 
classification in other languages (Mihalcea et al., 
2007; Banea et al., 2008), the methods are very 
straightforward by directly employing an induc-
tive classifier (e.g. SVM, NB), and the classifica-
tion performance is far from satisfactory because 
of the language gap between the original lan-
guage and the translated language.  

In this study, we propose a co-training ap-
proach to improving the classification accuracy 
of polarity identification of Chinese product re-
views. Unlabeled Chinese reviews can be fully 
leveraged in the proposed approach. First, ma-
chine translation services are used to translate 
English training reviews into Chinese reviews 
and also translate Chinese test reviews and addi-
tional unlabeled reviews into English reviews. 
Then, we can view the classification problem in 
two independent views: Chinese view with only 
Chinese features and English view with only 
English features. We then use the co-training 
approach to making full use of the two redundant 
views of features. The SVM classifier is adopted 
as the basic classifier in the proposed approach. 
Experimental results show that the proposed ap-
proach can outperform the baseline inductive 
classifiers and the more advanced transductive 
classifiers.  

The rest of this paper is organized as follows: 
Section 2 introduces related work. The proposed 

235



co-training approach is described in detail in 
Section 3. Section 4 shows the experimental re-
sults. Lastly we conclude this paper in Section 5. 

2 Related Work 
2.1 Sentiment Classification 
Sentiment classification can be performed on 
words, sentences or documents. In this paper we 
focus on document sentiment classification. The 
methods for document sentiment classification 
can be generally categorized into lexicon-based 
and corpus-based.  

Lexicon-based methods usually involve deriv-
ing a sentiment measure for text based on senti-
ment lexicons.  Turney (2002) predicates the sen-
timent orientation of a review by the average se-
mantic orientation of the phrases in the review 
that contain adjectives or adverbs, which is de-
noted as the semantic oriented method. Kim and 
Hovy (2004) build three models to assign a sen-
timent category to a given sentence by combin-
ing the individual sentiments of sentiment-
bearing words. Hiroshi et al. (2004) use the tech-
nique of deep language analysis for machine 
translation to extract sentiment units in text 
documents. Kennedy and Inkpen (2006) deter-
mine the sentiment of a customer review by 
counting positive and negative terms and taking 
into account contextual valence shifters, such as 
negations and intensifiers. Devitt and Ahmad 
(2007) explore a computable metric of positive 
or negative polarity in financial news text.  

Corpus-based methods usually consider the 
sentiment analysis task as a classification task 
and they use a labeled corpus to train a sentiment 
classifier. Since the work of Pang et al. (2002), 
various classification models and linguistic fea-
tures have been proposed to improve the classifi-
cation performance (Pang and Lee, 2004; Mullen 
and Collier, 2004; Wilson et al., 2005; Read, 
2005). Most recently, McDonald et al. (2007) 
investigate a structured model for jointly classi-
fying the sentiment of text at varying levels of 
granularity. Blitzer et al. (2007) investigate do-
main adaptation for sentiment classifiers, focus-
ing on online reviews for different types of prod-
ucts. Andreevskaia and Bergler (2008) present a 
new system consisting of the ensemble of a cor-
pus-based classifier and a lexicon-based classi-
fier with precision-based vote weighting. 

Chinese sentiment analysis has also been stud-
ied (Tsou et al., 2005; Ye et al., 2006; Li and Sun, 
2007) and most such work uses similar lexicon-

based or corpus-based methods for Chinese sen-
timent classification.  

To date, several pilot studies have been per-
formed to leverage rich English resources for 
sentiment analysis in other languages. Standard 
Naïve Bayes and SVM classifiers have been ap-
plied for subjectivity classification in Romanian 
(Mihalcea et al., 2007; Banea et al., 2008), and 
the results show that automatic translation is a 
viable alternative for the construction of re-
sources and tools for subjectivity analysis in a 
new target language. Wan (2008) focuses on lev-
eraging both Chinese and English lexicons to 
improve Chinese sentiment analysis by using 
lexicon-based methods. In this study, we focus 
on improving the corpus-based method for cross-
lingual sentiment classification of Chinese prod-
uct reviews by developing novel approaches.  

2.2 Cross-Domain Text Classification 
Cross-domain text classification can be consid-
ered as a more general task than cross-lingual 
sentiment classification. In the problem of cross-
domain text classification, the labeled and unla-
beled data come from different domains, and 
their underlying distributions are often different 
from each other, which violates the basic as-
sumption of traditional classification learning.  

To date, many semi-supervised learning algo-
rithms have been developed for addressing the 
cross-domain text classification problem by 
transferring knowledge across domains, includ-
ing Transductive SVM (Joachims, 1999), 
EM(Nigam et al., 2000), EM-based Naïve Bayes 
classifier (Dai et al., 2007a), Topic-bridged 
PLSA (Xue et al., 2008), Co-Clustering based 
classification (Dai et al., 2007b), two-stage ap-
proach (Jiang and Zhai, 2007). DauméIII and 
Marcu (2006) introduce a statistical formulation 
of this problem in terms of a simple mixture 
model.  

In particular, several previous studies focus on 
the problem of cross-lingual text classification, 
which can be considered as a special case of 
general cross-domain text classification. Bel et al. 
(2003) present practical and cost-effective solu-
tions. A few novel models have been proposed to 
address the problem, e.g. the EM-based algo-
rithm (Rigutini et al., 2005), the information bot-
tleneck approach (Ling et al., 2008), the multi-
lingual domain models (Gliozzo and Strapparava, 
2005), etc. To the best of our knowledge, co-
training has not yet been investigated for cross-
domain or cross-lingual text classification. 

236



3 The Co-Training Approach  

3.1 Overview 
The purpose of our approach is to make use of 
the annotated English corpus for sentiment polar-
ity identification of Chinese reviews in a super-
vised framework, without using any Chinese re-
sources. Given the labeled English reviews and 
unlabeled Chinese reviews, two straightforward 
methods for addressing the problem are as fol-
lows:  

1) We first learn a classifier based on the la-
beled English reviews, and then translate Chi-
nese reviews into English reviews. Lastly, we 
use the classifier to classify the translated Eng-
lish reviews.  

2) We first translate the labeled English re-
views into Chinese reviews, and then learn a 
classifier based on the translated Chinese reviews 
with labels. Lastly, we use the classifier to clas-
sify the unlabeled Chinese reviews.  

The above two methods have been used in 
(Banea et al., 2008) for Romanian subjectivity 
analysis, but the experimental results are not very 
promising. As shown in our experiments, the 
above two methods do not perform well for Chi-
nese sentiment classification, either, because the 
underlying distribution between the original lan-
guage and the translated language are different.  

In order to address the above problem, we 
propose to use the co-training approach to make 
use of some amounts of unlabeled Chinese re-
views to improve the classification accuracy. The 
co-training approach can make full use of both 
the English features and the Chinese features in a 
unified framework. The framework of the pro-
posed approach is illustrated in Figure 1. 

 The framework consists of a training phase 
and a classification phase. In the training phase, 
the input is the labeled English reviews and some 
amounts of unlabeled Chinese reviews1. The la-
beled English reviews are translated into labeled 
Chinese reviews, and the unlabeled Chinese re-
views are translated into unlabeled English re-
views, by using machine translation services. 
Therefore, each review is associated with an 
English version and a Chinese version. The Eng-
lish features and the Chinese features for each 
review are considered two independent and re-
dundant views of the review. The co-training 
algorithm is then applied to learn two classifiers 

                                                 
1 The unlabeled Chinese reviews used for co-training do not 
include the unlabeled Chinese reviews for testing, i.e., the 
Chinese reviews for testing are blind to the training phase.  

and finally the two classifiers are combined into 
a single sentiment classifier. In the classification 
phase, each unlabeled Chinese review for testing 
is first translated into English review, and then 
the learned classifier is applied to classify the 
review into either positive or negative.  

The steps of review translation and the co-
training algorithm are described in details in the 
next sections, respectively.  
 

 
Figure 1. Framework of the proposed approach 

3.2 Review Translation 
In order to overcome the language gap, we must 
translate one language into another language. 
Fortunately, machine translation techniques have 
been well developed in the NLP field, though the 
translation performance is far from satisfactory. 
A few commercial machine translation services 
can be publicly accessed, e.g. Google Translate2, 
Yahoo Babel Fish3 and Windows Live Translate4. 

                                                 
2 http://translate.google.com/translate_t 
3 http://babelfish.yahoo.com/translate_txt 
4 http://www.windowslivetranslator.com/ 

Unlabeled 
Chinese 
Reviews

Labeled 
English 
Reviews

Machine 
Translation 
(CN-EN) 

Co-Training 

Machine 
Translation 
(EN-CN) 

Labeled 
Chinese 
Reviews

Unlabeled 
English 
Reviews

Pos\Neg 

Chinese View English View

Test     
Chinese 
Review

Sentiment 
Classifier 

Machine 
Translation 
(CN-EN) 

Test     
English 
Review

Training Phase

Classification Phase

237



In this study, we adopt Google Translate for both 
English-to-Chinese Translation and Chinese-to-
English Translation, because it is one of the 
state-of-the-art commercial machine translation 
systems used today. Google Translate applies 
statistical learning techniques to build a transla-
tion model based on both monolingual text in the 
target language and aligned text consisting of 
examples of human translations between the lan-
guages.  

3.3 The Co-Training Algorithm 
The co-training algorithm (Blum and Mitchell, 
1998) is a typical bootstrapping method, which 
starts with a set of labeled data, and increase the 
amount of annotated data using some amounts of 
unlabeled data in an incremental way. One im-
portant aspect of co-training is that two condi-
tional independent views are required for co-
training to work, but the independence assump-
tion can be relaxed. Till now, co-training has 
been successfully applied to statistical parsing 
(Sarkar, 2001), reference resolution (Ng and 
Cardie, 2003), part of speech tagging (Clark et 
al., 2003), word sense disambiguation (Mihalcea, 
2004) and email classification (Kiritchenko and 
Matwin, 2001). 

In the context of cross-lingual sentiment clas-
sification, each labeled English review or unla-
beled Chinese review has two views of features: 
English features and Chinese features. Here, a 
review is used to indicate both its Chinese ver-
sion and its English version, until stated other-
wise. The co-training algorithm is illustrated in 
Figure 2. In the algorithm, the class distribution 
in the labeled data is maintained by balancing the 
parameter values of p and n at each iteration. 

The intuition of the co-training algorithm is 
that if one classifier can confidently predict the 
class of an example, which is very similar to 
some of labeled ones, it can provide one more 
training example for the other classifier. But, of 
course, if this example happens to be easy to be 
classified by the first classifier, it does not mean 
that this example will be easy to be classified by 
the second classifier, so the second classifier will 
get useful information to improve itself and vice 
versa (Kiritchenko and Matwin, 2001). 

In the co-training algorithm, a basic classifica-
tion algorithm is required to construct Cen and 
Ccn. Typical text classifiers include Support Vec-
tor Machine (SVM), Naïve Bayes (NB), Maxi-
mum Entropy (ME), K-Nearest Neighbor (KNN), 
etc. In this study, we adopt the widely-used SVM 
classifier (Joachims, 2002). Viewing input data 

as two sets of vectors in a feature space, SVM 
constructs a separating hyperplane in the space 
by maximizing the margin between the two data 
sets. The English or Chinese features used in this 
study include both unigrams and bigrams5 and 
the feature weight is simply set to term fre-
quency6. Feature selection methods (e.g. Docu-
ment Frequency (DF), Information Gain (IG), 
and Mutual Information (MI)) can be used for 
dimension reduction. But we use all the features 
in the experiments for comparative analysis, be-
cause there is no significant performance im-
provement after applying the feature selection 
techniques in our empirical study. The output 
value of the SVM classifier for a review indi-
cates the confidence level of the review’s classi-
fication. Usually, the sentiment polarity of a re-
view is indicated by the sign of the prediction 
value.  

Given: 
- Fen and Fcn are redundantly sufficient 

sets of features, where Fen represents 
the English features, Fcn represents the 
Chinese features; 

- L is a set of labeled training reviews; 
- U is a set of unlabeled reviews; 

Loop for I iterations: 
1. Learn the first classifier Cen from L 

based on Fen; 
2. Use Cen to label reviews from U 

based on Fen; 
3. Choose p positive and n negative the 

most confidently predicted reviews 
Een from U; 

4. Learn the second classifier Ccn from L 
based on Fcn; 

5. Use Ccn to label reviews from U 
based on Fcn; 

6. Choose p positive and n negative the 
most confidently predicted reviews 
Ecn from U; 

7. Removes reviews Een∪Ecn from U7; 
8. Add reviews Een∪Ecn with the corre-

sponding labels to L; 
Figure 2. The co-training algorithm 

In the training phase, the co-training algorithm 
learns two separate classifiers: Cen and Ccn. 
                                                 
5 For Chinese text, a unigram refers to a Chinese word and a 
bigram refers to two adjacent Chinese words.  
6 Term frequency performs better than TFIDF by our em-
pirical analysis.  
7 Note that the examples with conflicting labels are not in-
cluded in Een∪Ecn In other words, if an example is in both 
Een and Ecn, but the labels for the example is conflicting, the 
example will be excluded from Een∪Ecn. 

238



Therefore, in the classification phase, we can 
obtain two prediction values for a test review.  
We normalize the prediction values into [-1, 1] 
by dividing the maximum absolute value. Finally, 
the average of the normalized values is used as 
the overall prediction value of the review.  

4 Empirical Evaluation 
4.1 Evaluation Setup 

4.1.1 Data set 

The following three datasets were collected and 
used in the experiments: 

Test Set (Labeled Chinese Reviews): In or-
der to assess the performance of the proposed 
approach, we collected and labeled 886 product 
reviews (451 positive reviews + 435 negative 
reviews) from a popular Chinese IT product web 
site-IT1688. The reviews focused on such prod-
ucts as mp3 players, mobile phones, digital cam-
era and laptop computers.  

Training Set (Labeled English Reviews): 
There are many labeled English corpora avail-
able on the Web and we used the corpus con-
structed for multi-domain sentiment classifica-
tion (Blitzer et al., 2007)9, because the corpus 
was large-scale and it was within similar do-
mains as the test set. The dataset consisted of 
8000 Amazon product reviews (4000 positive 
reviews + 4000 negative reviews) for four differ-
ent product types: books, DVDs, electronics and 
kitchen appliances.  

Unlabeled Set (Unlabeled Chinese Reviews): 
We downloaded additional 1000 Chinese product 
reviews from IT168 and used the reviews as the 
unlabeled set. Therefore, the unlabeled set and 
the test set were in the same domain and had 
similar underlying feature distributions.  

Each Chinese review was translated into Eng-
lish review, and each English review was trans-
lated into Chinese review. Therefore, each re-
view has two independent views: English view 
and Chinese view. A review is represented by 
both its English view and its Chinese view.  

Note that the training set and the unlabeled set 
are used in the training phase, while the test set is 
blind to the training phase. 

4.1.2 Evaluation Metric 

We used the standard precision, recall and F-
measure to measure the performance of positive 
and negative class, respectively, and used the 
                                                 
8 http://www.it168.com 
9 http://www.cis.upenn.edu/~mdredze/datasets/sentiment/ 

accuracy metric to measure the overall perform-
ance of the system. The metrics are defined the 
same as in general text categorization. 

4.1.3 Baseline Methods 

In the experiments, the proposed co-training ap-
proach (CoTrain) is compared with the following 
baseline methods: 

SVM(CN): This method applies the inductive 
SVM with only Chinese features for sentiment 
classification in the Chinese view. Only English-
to-Chinese translation is needed. And the unla-
beled set is not used.  

SVM(EN): This method applies the inductive 
SVM with only English features for sentiment 
classification in the English view. Only Chinese-
to-English translation is needed. And the unla-
beled set is not used. 

SVM(ENCN1): This method applies the in-
ductive SVM with both English and Chinese fea-
tures for sentiment classification in the two 
views. Both English-to-Chinese and Chinese-to-
English translations are required. And the unla-
beled set is not used. 

SVM(ENCN2): This method combines the re-
sults of SVM(EN) and SVM(CN) by averaging 
the prediction values in the same way with the 
co-training approach.  

TSVM(CN): This method applies the trans-
ductive SVM with only Chinese features for sen-
timent classification in the Chinese view. Only 
English-to-Chinese translation is needed. And 
the unlabeled set is used.  

TSVM(EN): This method applies the trans-
ductive SVM with only English features for sen-
timent classification in the English view. Only 
Chinese-to-English translation is needed. And 
the unlabeled set is used. 

TSVM(ENCN1): This method applies the 
transductive SVM with both English and Chinese 
features for sentiment classification in the two 
views. Both English-to-Chinese and Chinese-to-
English translations are required. And the unla-
beled set is used.  

TSVM(ENCN2): This method combines the 
results of TSVM(EN) and TSVM(CN) by aver-
aging the prediction values. 

Note that the first four methods are straight-
forward methods used in previous work, while 
the latter four methods are strong baselines be-
cause the transductive SVM has been widely 
used for improving the classification accuracy by 
leveraging additional unlabeled examples.  

239



4.2 Evaluation Results 

4.2.1 Method Comparison 

In the experiments, we first compare the pro-
posed co-training approach (I=40 and p=n=5) 
with the eight baseline methods. The three pa-
rameters in the co-training approach are empiri-
cally set by considering the total number (i.e. 
1000) of the unlabeled Chinese reviews. In our 
empirical study, the proposed approach can per-
form well with a wide range of parameter values, 
which will be shown later. Table 1 shows the 
comparison results.  

Seen from the table, the proposed co-training 
approach outperforms all eight baseline methods 
over all metrics. Among the eight baselines, the 
best one is TSVM(ENCN2), which combines the 
results of two transductive SVM classifiers. Ac-
tually, TSVM(ENCN2) is similar to CoTrain 
because CoTrain also combines the results of 
two classifiers in the same way. However, the 
co-training approach can train two more effective 
classifiers, and the accuracy values of the com-
ponent English and Chinese classifiers are 0.775 
and 0.790, respectively, which are higher than 
the corresponding TSVM classifiers. Overall, the 
use of transductive learning and the combination 
of English and Chinese views are beneficial to 
the final classification accuracy, and the co-
training approach is more suitable for making 
use of the unlabeled Chinese reviews than the 
transductive SVM.  

4.2.2 Influences of Iteration Number (I) 

Figure 3 shows the accuracy curve of the co-
training approach (Combined Classifier) with 
different numbers of iterations. The iteration 
number I is varied from 1 to 80. When I is set to 
1, the co-training approach is degenerated into 
SVM(ENCN2). The accuracy curves of the com-
ponent English and Chinese classifiers learned in 
the co-training approach are also shown in the 

figure. We can see that the proposed co-training 
approach can outperform the best baseline-
TSVM(ENCN2) after 20 iterations. After a large 
number of iterations, the performance of the co-
training approach decreases because noisy train-
ing examples may be selected from the remain-
ing unlabeled set. Finally, the performance of the 
approach does not change any more, because the 
algorithm runs out of all possible examples in the 
unlabeled set. Fortunately, the proposed ap-
proach performs well with a wide range of itera-
tion numbers. We can also see that the two com-
ponent classifier has similar trends with the co-
training approach. It is encouraging that the com-
ponent Chinese classifier alone can perform bet-
ter than the best baseline when the iteration 
number is set between 40 and 70. 

4.2.3 Influences of Growth Size (p, n) 

Figure 4 shows how the growth size at each it-
eration (p positive and n negative confident ex-
amples) influences the accuracy of the proposed 
co-training approach. In the above experiments, 
we set p=n, which is considered as a balanced 
growth. When p differs very much from n, the 
growth is considered as an imbalanced growth. 
Balanced growth of (2, 2), (5, 5), (10, 10) and 
(15, 15) examples and imbalanced growth of (1, 
5), (5, 1) examples are compared in the figure. 
We can see that the performance of the co-
training approach with the balanced growth can 
be improved after a few iterations. And the per-
formance of the co-training approach with large 
p and n will more quickly become unchanged, 
because the approach runs out of the limited ex-
amples in the unlabeled set more quickly. How-
ever, the performance of the co-training ap-
proaches with the two imbalanced growths is 
always going down quite rapidly, because the 
labeled unbalanced examples hurt the perform-
ance badly at each iteration.  

 
Positive Negative Total Method 

Precision Recall F-measure Precision Recall F-measure Accuracy
SVM(CN) 0.733 0.865 0.793 0.828 0.674 0.743 0.771 
SVM(EN) 0.717 0.803 0.757 0.766 0.671 0.716 0.738 

SVM(ENCN1) 0.744 0.820 0.781 0.792 0.708 0.748 0.765 
SVM(ENCN2) 0.746 0.847 0.793 0.816 0.701 0.754 0.775 

TSVM(CN) 0.724 0.878 0.794 0.838 0.653 0.734 0.767 
TSVM(EN) 0.732 0.860 0.791 0.823 0.674 0.741 0.769 

TSVM(ENCN1) 0.743 0.878 0.805 0.844 0.685 0.756 0.783 
TSVM(ENCN2) 0.744 0.896 0.813 0.863 0.680 0.761 0.790 

CoTrain          
(I=40; p=n=5) 0.768 0.905 0.831 0.879 0.717 0.790 0.813 

Table 1. Comparison results  

240



0.72

0.73

0.74
0.75

0.76

0.77

0.78

0.79
0.8

0.81

0.82

1 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80

Iteration Number (I )

A
cc

ur
ac

y

English Classifier(CoTrain) Chinese Classifier(CoTrain)
Combined Classifier(CoTrain) TSVM(ENCN2)

 
Figure 3. Accuracy vs. number of iterations for co-training (p=n=5) 

0.5

0.55

0.6

0.65

0.7

0.75

0.8

1 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80
Iteration Number (I )

A
cc

ur
ac

y

(p=2,n=2) (p=5,n=5) (p=10,n=10)
(p=15,n=15) (p=1,n=5) (p=5,n=1)

 
Figure 4. Accuracy vs. different (p, n) for co-training 

0.76

0.77

0.78

0.79

0.8

0.81

0.82

25% 50% 75% 100%

Feature size

A
cc

ur
ac

y

TSVM(ENCN1) TSVM(ENCN2) CoTrain (I=40; p=n=5)

 
Figure 5. Influences of feature size 

 

241



4.2.4 Influences of Feature Selection 

In the above experiments, all features (unigram + 
bigram) are used. As mentioned earlier, feature 
selection techniques are widely used for dimen-
sion reduction. In this section, we further con-
duct experiments to investigate the influences of 
feature selection techniques on the classification 
results. We use the simple but effective docu-
ment frequency (DF) for feature selection.  Fig-
ures 6 show the comparison results of different 
feature sizes for the co-training approach and 
two strong baselines. The feature size is meas-
ured as the proportion of the selected features 
against the total features (i.e. 100%).   

We can see from the figure that the feature se-
lection technique has very slight influences on 
the classification accuracy of the methods. It can 
be seen that the co-training approach can always 
outperform the two baselines with different fea-
ture sizes. The results further demonstrate the 
effectiveness and robustness of the proposed co-
training approach.  

5 Conclusion and Future Work  
In this paper, we propose to use the co-training 
approach to address the problem of cross-lingual 
sentiment classification. The experimental results 
show the effectiveness of the proposed approach. 

In future work, we will improve the sentiment 
classification accuracy in the following two ways: 

1) The smoothed co-training approach used in 
(Mihalcea, 2004) will be adopted for sentiment 
classification. The approach has the effect of 
“smoothing” the learning curves. During the 
bootstrapping process of smoothed co-training, 
the classifier at each iteration is replaced with a 
majority voting scheme applied to all classifiers 
constructed at previous iterations.  

2) The feature distributions of the translated 
text and the natural text in the same language are 
still different due to the inaccuracy of the ma-
chine translation service. We will employ the 
structural correspondence learning (SCL) domain 
adaption algorithm used in (Blitzer et al., 2007) 
for linking the translated text and the natural text.  

Acknowledgments 
This work was supported by NSFC (60873155), 
RFDP (20070001059), Beijing Nova Program 
(2008B03), National High-tech R&D Program 
(2008AA01Z421) and NCET (NCET-08-0006). 
We also thank the anonymous reviewers for their 
useful comments. 

References  
A. Andreevskaia and S. Bergler. 2008. When special-

ists and generalists work together: overcoming 
domain dependence in sentiment tagging. In Pro-
ceedings of ACL-08: HLT. 

C. Banea, R. Mihalcea, J. Wiebe and S. Hassan. 2008. 
Multilingual subjectivity analysis using machine 
translation. In Proceedings of EMNLP-2008.  

N. Bel, C. H. A. Koster, and M. Villegas. 2003. 
Cross-lingual text categorization. In Proceedings of 
ECDL-03. 

J. Blitzer, M. Dredze and F. Pereira. 2007. Biogra-
phies, bollywood, boom-boxes and blenders: do-
main adaptation for sentiment classification. In 
Proceedings of ACL-07. 

A. Blum and T. Mitchell. 1998. Combining labeled 
and unlabeled data with cotraining. In Proceedings 
of COLT-98. 

S. Brody, R. Navigli and M. Lapata. 2006. Ensemble 
methods for unsupervised WSD. In Proceedings of 
COLING-ACL-2006. 

S. Clark, J. R. Curran, and M. Osborne. 2003. Boot-
strapping POS taggers using unlabelled data. In 
Proceedings of CoNLL-2003. 

W. Dai, G.-R. Xue, Q. Yang, Y. Yu. 2007a. Transfer-
ring Naïve Bayes Classifiers for text classification. 
In Proceedings of AAAI-07. 

W. Dai, G.-R. Xue, Q. Yang, Y. Yu. 2007b. Co-
clustering based classification for out-of-domain 
documents. In Proceedings of KDD-07.  

H. DauméIII and D. Marcu. 2006. Domain adaptation 
for statistical classifiers. Journal of Artificial Intel-
ligence Research, 26:101–126. 

A. Devitt and K. Ahmad. 2007. Sentiment polarity 
identification in financial news: a cohesion-based 
approach. In Proceedings of ACL2007. 

T. G. Dietterich. 1997. Machine learning research: 
four current directions. AI Magazine, 18(4), 1997. 

A. Gliozzo and C. Strapparava. 2005. Cross language 
text categorization by acquiring multilingual do-
main models from comparable corpora. In Pro-
ceedings of the ACL Workshop on Building and 
Using Parallel Texts.  

K. Hiroshi, N. Tetsuya and W. Hideo. 2004. Deeper 
sentiment analysis using machine translation tech-
nology. In Proceedings of COLING-04. 

J. Jiang and C. Zhai. 2007. A two-stage approach to 
domain adaptation for statistical classifiers. In Pro-
ceedings of CIKM-07.  

T. Joachims. 1999. Transductive inference for text 
classification using support vector machines. In 
Proceedings of ICML-99. 

242



T. Joachims. 2002. Learning to classify text using 
support vector machines. Dissertation, Kluwer, 
2002.   

A. Kennedy and D. Inkpen. 2006. Sentiment classifi-
cation of movie reviews using contextual valence 
shifters. Computational Intelligence, 22(2):110-
125. 

S.-M. Kim and E. Hovy. 2004. Determining the sen-
timent of opinions. In Proceedings of COLING-04. 

S. Kiritchenko and S. Matwin. 2001. Email classifica-
tion with co-training. In Proceedings of the 2001 
Conference of the Centre for Advanced Studies on 
Collaborative Research.  

L.-W. Ku, Y.-T. Liang and H.-H. Chen. 2006. Opin-
ion extraction, summarization and tracking in news 
and blog corpora. In Proceedings of AAAI-2006. 

J. Li and M. Sun. 2007. Experimental study on senti-
ment classification of Chinese review using ma-
chine learning techniques. In Proceeding of IEEE-
NLPKE-07. 

X. Ling, W. Dai, Y. Jiang, G.-R. Xue, Q. Yang, and Y. 
Yu. 2008. Can Chinese Web pages be classified 
with English data source? In Proceedings of 
WWW-08. 

B. Liu, M. Hu and J. Cheng. 2005. Opinion observer: 
Analyzing and comparing opinions on the web. In 
Proceedings of WWW-2005. 

R. McDonald, K. Hannan, T. Neylon, M. Wells and J. 
Reynar. 2007. Structured models for fine-to-coarse 
sentiment analysis. In Proceedings of ACL-07. 

R. Mihalcea. 2004. Co-training and self-training for 
word sense disambiguation. In Proceedings of 
CONLL-04.  

R. Mihalcea, C. Banea and J. Wiebe. 2007. Learning 
multilingual subjective language via cross-lingual 
projections. In Proceedings of ACL-2007. 

T. Mullen and N. Collier. 2004. Sentiment analysis 
using support vector machines with diverse infor-
mation sources. In Proceedings of EMNLP-04. 

V. Ng and C. Cardie. 2003. Weakly supervised natu-
ral language learning without redundant views. In 
Proceedings of HLT-NAACL-03. 

K. Nigam, A. K. McCallum, S. Thrun, and T. 
Mitchell. 2000. Text Classification from Labeled 
and Unlabeled Documents using EM. Machine 
Learning, 39(2-3):103–134. 

B. Pang, L. Lee and S. Vaithyanathan. 2002. Thumbs 
up? sentiment classification using machine learn-
ing techniques. In Proceedings of EMNLP-02. 

B. Pang and L. Lee. 2004. A sentimental education: 
sentiment analysis using subjectivity summariza-
tion based on minimum cuts. In Proceedings of 
ACL-04. 

J. Read. 2005. Using emoticons to reduce dependency 
in machine learning techniques for sentiment clas-
sification. In Proceedings of ACL-05. 

L. Rigutini, M. Maggini and B. Liu. 2005. An EM 
based training algorithm for cross-language text 
categorization. In Proceedings of WI-05.  

A. Sarkar. 2001. Applying cotraining methods to sta-
tistical parsing. In Proceedings of NAACL-2001. 

I. Titov and R. McDonald. 2008. A joint model of text 
and aspect ratings for sentiment summarization. In 
Proceedings of ACL-08:HLT. 

B. K. Y. Tsou, R. W. M. Yuen, O. Y. Kwong, T. B. Y. 
La and W. L. Wong. 2005. Polarity classification 
of celebrity coverage in the Chinese press. In Pro-
ceedings of International Conference on Intelli-
gence Analysis. 

P. Turney. 2002. Thumbs up or thumbs down? seman-
tic orientation applied to unsupervised classifica-
tion of reviews. In Proceedings of ACL-2002. 

X. Wan. 2008. Using bilingual knowledge and en-
semble techniques for unsupervised Chinese sen-
timent analysis. In Proceedings of EMNLP-2008. 

T. Wilson, J. Wiebe and P. Hoffmann. 2005. Recog-
nizing Contextual Polarity in Phrase-Level Senti-
ment Analysis. In Proceedings of HLT/EMNLP-05. 

G.-R. Xue, W. Dai, Q. Yang, Y. Yu. 2008. Topic-
bridged PLSA for cross-domain text classification. 
In Proceedings of SIGIR-08. 

Q. Ye, W. Shi and Y. Li. 2006. Sentiment classifica-
tion for movie reviews in Chinese by improved 
semantic oriented approach. In Proceedings of 39th 
Hawaii International Conference on System Sci-
ences, 2006. 

243


