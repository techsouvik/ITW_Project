













































Improving Tree-to-Tree Translation with Packed Forests


Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 558–566,
Suntec, Singapore, 2-7 August 2009. c©2009 ACL and AFNLP

Improving Tree-to-Tree Translation with Packed Forests

Yang Liu and Yajuan Lü and Qun Liu
Key Laboratory of Intelligent Information Processing

Institute of Computing Technology
Chinese Academy of Sciences

P.O. Box 2704, Beijing 100190, China
{yliu,lvyajuan,liuqun}@ict.ac.cn

Abstract

Current tree-to-tree models suffer from
parsing errors as they usually use only 1-
best parses for rule extraction and decod-
ing. We instead propose a forest-based
tree-to-tree model that uses packed forests.
The model is based on a probabilis-
tic synchronous tree substitution gram-
mar (STSG), which can be learned from
aligned forest pairs automatically. The de-
coder finds ways of decomposing trees in
the source forest into elementary trees us-
ing the source projection of STSG while
building target forest in parallel. Compa-
rable to the state-of-the-art phrase-based
system Moses, using packed forests in
tree-to-tree translation results in a signif-
icant absolute improvement of 3.6 BLEU
points over using 1-best trees.

1 Introduction

Approaches to syntax-based statistical machine
translation make use of parallel data with syntactic
annotations, either in the form of phrase structure
trees or dependency trees. They can be roughly
divided into three categories: string-to-tree mod-
els (e.g., (Galley et al., 2006; Marcu et al., 2006;
Shen et al., 2008)), tree-to-string models (e.g.,
(Liu et al., 2006; Huang et al., 2006)), and tree-to-
tree models (e.g., (Eisner, 2003; Ding and Palmer,
2005; Cowan et al., 2006; Zhang et al., 2008)).
By modeling the syntax of both source and tar-
get languages, tree-to-tree approaches have the po-
tential benefit of providing rules linguistically bet-
ter motivated. However, while string-to-tree and
tree-to-string models demonstrate promising re-
sults in empirical evaluations, tree-to-tree models
have still been underachieving.

We believe that tree-to-tree models face two
major challenges. First, tree-to-tree models are
more vulnerable to parsing errors. Obtaining
syntactic annotations in quantity usually entails
running automatic parsers on a parallel corpus.
As the amount and domain of the data used to
train parsers are relatively limited, parsers will
inevitably output ill-formed trees when handling
real-world text. Guided by such noisy syntactic in-
formation, syntax-based models that rely on 1-best
parses are prone to learn noisy translation rules
in training phase and produce degenerate trans-
lations in decoding phase (Quirk and Corston-
Oliver, 2006). This situation aggravates for tree-
to-tree models that use syntax on both sides.

Second, tree-to-tree rules provide poorer rule
coverage. As a tree-to-tree rule requires that there
must be trees on both sides, tree-to-tree mod-
els lose a larger amount of linguistically unmoti-
vated mappings. Studies reveal that the absence of
such non-syntactic mappings will impair transla-
tion quality dramatically (Marcu et al., 2006; Liu
et al., 2007; DeNeefe et al., 2007; Zhang et al.,
2008).

Compactly encoding exponentially many
parses, packed forests prove to be an excellent
fit for alleviating the above two problems (Mi et
al., 2008; Mi and Huang, 2008). In this paper,
we propose a forest-based tree-to-tree model. To
learn STSG rules from aligned forest pairs, we in-
troduce a series of notions for identifying minimal
tree-to-tree rules. Our decoder first converts the
source forest to a translation forest and then finds
the best derivation that has the source yield of one
source tree in the forest. Comparable to Moses,
our forest-based tree-to-tree model achieves an
absolute improvement of 3.6 BLEU points over
conventional tree-based model.

558



IP1

NP2 VP3

PP4 VP-B5

NP-B6 NP-B7 NP-B8

NR9 CC10P11 NR12 VV13 AS14 NN15

bushi yu shalong juxing le huitan

Bush held a talk with Sharon

NNP16 VBD17 DT18 NN19 IN20 NNP21

NP22 NP23 NP24

NP25 PP26

NP27

VP28

S 29

Figure 1: An aligned packed forest pair. Each
node is assigned a unique identity for reference.
The solid lines denote hyperedges and the dashed
lines denote word alignments. Shaded nodes are
frontier nodes.

2 Model

Figure 1 shows an aligned forest pair for a Chinese
sentence and an English sentence. The solid lines
denote hyperedges and the dashed lines denote
word alignments between the two forests. Each
node is assigned a unique identity for reference.
Each hyperedge is associated with a probability,
which we omit in Figure 1 for clarity. In a forest,
a node usually has multiple incoming hyperedges.
We use IN(v) to denote the set of incoming hy-
peredges of node v. For example, the source node
“IP1” has following two incoming hyperedges: 1

e1 = 〈(NP-B
6, VP3), IP1〉

e2 = 〈(NP
2, VP-B5), IP1〉

1As there are both source and target forests, it might be
confusing by just using a span to refer to a node. In addition,
some nodes will often have the same labels and spans. There-
fore, it is more convenient to use an identity for referring to a
node. The notation “IP1” denotes the node that has a label of
“IP” and has an identity of “1”.

Formally, a packed parse forest is a compact
representation of all the derivations (i.e., parse
trees) for a given sentence under a context-free
grammar. Huang and Chiang (2005) define a for-
est as a tuple 〈V,E, v̄,R〉, where V is a finite set
of nodes, E is a finite set of hyperedges, v̄ ∈ V is
a distinguished node that denotes the goal item in
parsing, and R is the set of weights. For a given
sentence w1:l = w1 . . . wl, each node v ∈ V is in
the form of Xi,j , which denotes the recognition of
non-terminal X spanning the substring from posi-
tions i through j (that is, wi+1 . . . wj). Each hy-
peredge e ∈ E is a triple e = 〈T (e), h(e), f(e)〉,
where h(e) ∈ V is its head, T (e) ∈ V ∗ is a vector
of tail nodes, and f(e) is a weight function from
R

|T (e)| to R.
Our forest-based tree-to-tree model is based on

a probabilistic STSG (Eisner, 2003). Formally,
an STSG can be defined as a quintuple G =
〈Fs,Ft,Ss,St, P 〉, where

• Fs andFt are the source and target alphabets,
respectively,

• Ss and St are the source and target start sym-
bols, and

• P is a set of production rules. A rule r is a
triple 〈ts, tt,∼〉 that describes the correspon-
dence ∼ between a source tree ts and a target
tree tt.

To integrate packed forests into tree-to-tree
translation, we model the process of synchronous
generation of a source forest Fs and a target forest
Ft using a probabilistic STSG grammar:

Pr(Fs, Ft) =
∑

Ts∈Fs

∑

Tt∈Ft

Pr(Ts, Tt)

=
∑

Ts∈Fs

∑

Tt∈Ft

∑

d∈D

Pr(d)

=
∑

Ts∈Fs

∑

Tt∈Ft

∑

d∈D

∏

r∈d

p(r) (1)

where Ts is a source tree, Tt is a target tree, D is
the set of all possible derivations that transform Ts
into Tt, d is one such derivation, and r is a tree-to-
tree rule.

Table 1 shows a derivation of the forest pair in
Figure 1. A derivation is a sequence of tree-to-tree
rules. Note that we use x to represent a nontermi-
nal.

559



(1) IP(x1:NP-B, x2:VP)→ S(x1:NP, x2:VP)
(2) NP-B(x1:NR)→ NP(x1:NNP)
(3) NR(bushi)→ NNP(Bush)
(4) VP(x1:PP, VP-B(x2:VV, AS(le), x3:NP-B))→ VP(x2:VBD, NP(DT(a), x3:NP), x1:PP)
(5) PP(x1:P, x2:NP-B)→ PP(x1:IN, x2:NP)
(6) P(yu)→ IN(with)
(7) NP-B(x1:NR)→ NP(x1:NP)
(8) NR(shalong)→ NNP(Sharon)
(9) VV(juxing)→ VBD(held)

(10) NP-B(x1:NN)→ NP(x1:NN)
(11) NN(huitan)→ NN(talk)

Table 1: A minimal derivation of the forest pair in Figure 1.

id span cspan complement consistent frontier counterparts
1 1-6 1-2, 4-6 1 1 29
2 1-3 1, 5-6 2, 4 0 0
3 2-6 2, 4-6 1 1 1 28
4 2-3 5-6 1-2, 4 1 1 25, 26
5 4-6 2, 4 1, 5-6 1 0
6 1-1 1 2, 4-6 1 1 16, 22
7 3-3 6 1-2, 4-5 1 1 21, 24
8 6-6 4 1-2, 5-6 1 1 19, 23
9 1-1 1 2, 4-6 1 1 16, 22
10 2-2 5 1-2, 4, 6 1 1 20
11 2-2 5 1-2, 4, 6 1 1 20
12 3-3 6 1-2, 4-5 1 1 21, 24
13 4-4 2 1, 4-6 1 1 17
14 5-5 1-2, 4-6 1 0
15 6-6 4 1-2, 5-6 1 1 19, 23
16 1-1 1 2-4, 6 1 1 6, 9
17 2-2 4 1-3, 6 1 1 13
18 3-3 1-4, 6 1 0
19 4-4 6 1-4 1 1 8, 15
20 5-5 2 1, 3-4, 6 1 1 10, 11
21 6-6 3 1-2, 4, 6 1 1 7, 12
22 1-1 1 2-4, 6 1 1 6, 9
23 3-4 6 1-4 1 1 8, 15
24 6-6 3 1-2, 4, 6 1 1 7, 12
25 5-6 2-3 1, 4, 6 1 1 4
26 5-6 2-3 1, 4, 6 1 1 4
27 3-6 2-3, 6 1, 4 0 0
28 2-6 2-4, 6 1 1 1 3
29 1-6 1-4, 6 1 1 1

Table 2: Node attributes of the example forest pair.

3 Rule Extraction

Given an aligned forest pair as shown in Figure
1, how to extract all valid tree-to-tree rules that
explain its synchronous generation process? By
constructing a theory that gives formal seman-
tics to word alignments, Galley et al. (2004)
give principled answers to these questions for ex-
tracting tree-to-string rules. Their GHKM proce-
dure draws connections among word alignments,
derivations, and rules. They first identify the
tree nodes that subsume tree-string pairs consis-
tent with word alignments and then extract rules
from these nodes. By this means, GHKM proves
to be able to extract all valid tree-to-string rules
from training instances. Although originally de-
veloped for the tree-to-string case, it is possible to
extend GHKM to extract all valid tree-to-tree rules
from aligned packed forests.

In this section, we introduce our tree-to-tree rule
extraction method adapted from GHKM, which
involves four steps: (1) identifying the correspon-

dence between the nodes in forest pairs, (2) iden-
tifying minimum rules, (3) inferring composed
rules, and (4) estimating rule probabilities.

3.1 Identifying Correspondence Between
Nodes

To learn tree-to-tree rules, we need to find aligned
tree pairs in the forest pairs. To do this, the start-
ing point is to identify the correspondence be-
tween nodes. We propose a number of attributes
for nodes, most of which derive from GHKM, to
facilitate the identification.

Definition 1 Given a node v, its span σ(v) is an
index set of the words it covers.

For example, the span of the source node
“VP-B5” is {4, 5, 6} as it covers three source
words: “juxing”, “le”, and “huitan”. For conve-
nience, we use {4-6} to denotes a contiguous span
{4, 5, 6}.

Definition 2 Given a node v, its corresponding
span γ(v) is the index set of aligned words on an-
other side.

For example, the corresponding span of the
source node “VP-B5” is {2, 4}, corresponding to
the target words “held” and “talk”.

Definition 3 Given a node v, its complement span
δ(v) is the union of corresponding spans of nodes
that are neither antecedents nor descendants of v.

For example, the complement span of the source
node “VP-B5” is {1, 5-6}, corresponding to target
words “Bush”, “with”, and “Sharon”.

Definition 4 A node v is said to be consistent with
alignment if and only if closure(γ(v))∩δ(v) = ∅.

For example, the closure of the corresponding
span of the source node “VP-B5” is {2-4} and
its complement span is {1, 5-6}. As the intersec-
tion of the closure and the complement span is an
empty set, the source node “VP-B5” is consistent
with the alignment.

560



PP
4

NP-B
7

P
11

NR
12

PP
4

P
11

NP-B
7

PP
4

NP-B
7

P
11

NR
12

PP
26

IN
20

NP
24

NNP
21

PP
4

P
11

NP-B
7

PP
26

IN
20

NP
24

(a) (b) (c) (d)

Figure 2: (a) A frontier tree; (b) a minimal frontier tree; (c) a frontier tree pair; (d) a minimal frontier
tree pair. All trees are taken from the example forest pair in Figure 1. Shaded nodes are frontier nodes.
Each node is assigned an identity for reference.

Definition 5 A node v is said to be a frontier node
if and only if:

1. v is consistent;

2. There exists at least one consistent node v′ on
another side satisfying:

• closure(γ(v′)) ⊆ σ(v);

• closure(γ(v)) ⊆ σ(v′).

v′ is said to be a counterpart of v. We use τ(v) to
denote the set of counterparts of v.

A frontier node often has multiple counter-
parts on another side due to the usage of unary
rules in parsers. For example, the source node
“NP-B6” has two counterparts on the target side:
“NNP16” and “NP22”. Conversely, the target node
“NNP16” also has two counterparts counterparts
on the source side: “NR9” and “NP-B6”.

The node attributes of the example forest pair
are listed in Table 2. We use identities to refer to
nodes. “cspan” denotes corresponding span and
“complement” denotes complement span. In Fig-
ure 1, there are 12 frontier nodes (highlighted by
shading) on the source side and 12 frontier nodes
on the target side. Note that while a consistent
node is equal to a frontier node in GHKM, this is
not the case in our method because we have a tree
on the target side. Frontier nodes play a critical
role in forest-based rule extraction because they
indicate where to cut the forest pairs to obtain tree-
to-tree rules.

3.2 Identifying Minimum Rules

Given the frontier nodes, the next step is to iden-
tify aligned tree pairs, from which tree-to-tree
rules derive. Following Galley et al. (2006), we
distinguish between minimal and composed rules.
As a composed rule can be decomposed as a se-
quence of minimal rules, we are particularly inter-
ested in how to extract minimal rules. Also, we in-
troduce a number of notions to help identify mini-
mal rules.

Definition 6 A frontier tree is a subtree in a forest
satisfying:

1. Its root is a frontier node;

2. If the tree contains only one node, it must be
a lexicalized frontier node;

3. If the tree contains more than one nodes,
its leaves are either non-lexicalized frontier
nodes or lexicalized non-frontier nodes.

For example, Figure 2(a) shows a frontier tree
in which all nodes are frontier nodes.

Definition 7 A minimal frontier tree is a frontier
tree such that all nodes other than the root and
leaves are non-frontier nodes.

For example, Figure 2(b) shows a minimal fron-
tier tree.

Definition 8 A frontier tree pair is a triple
〈ts, tt,∼〉 satisfying:

1. ts is a source frontier tree;

561



2. tt is a target frontier tree;

3. The root of ts is a counterpart of that of tt;

4. There is a one-to-one correspondence ∼ be-
tween the frontier leaves of ts and tt.

For example, Figure 2(c) shows a frontier tree
pair.

Definition 9 A frontier tree pair 〈ts, tt,∼〉 is said
to be a subgraph of another frontier tree pair
〈ts

′, tt
′,∼′〉 if and only if:

1. root(ts) = root(ts
′);

2. root(tt) = root(tt
′);

3. ts is a subgraph of ts
′;

4. tt is a subgraph of tt
′.

For example, the frontier tree pair shown in Fig-
ure 2(d) is a subgraph of that in Figure 2(c).

Definition 10 A frontier tree pair is said to be
minimal if and only if it is not a subgraph of any
other frontier tree pair that shares with the same
root.

For example, Figure 2(d) shows a minimal fron-
tier tree pair.

Our goal is to find the minimal frontier tree
pairs, which correspond to minimal tree-to-tree
rules. For example, the tree pair shown in Figure
2(d) denotes a minimal rule as follows:

PP(x1:P,x2:NP-B)→ PP(x1:IN, x2:NP)

Figure 3 shows the algorithm for identifying
minimal frontier tree pairs. The input is a source
forest Fs, a target forest Ft, and a source frontier
node v (line 1). We use a set P to store collected
minimal frontier tree pairs (line 2). We first call
the procedure FINDTREES(Fs , v) to identify a set
of frontier trees rooted at v in Fs (line 3). For ex-
ample, for the source frontier node “PP4” in Figure
1, we obtain two frontier trees:

(PP4(P11)(NP-B7))

(PP4(P11)(NP-B7(NR12)))

Then, we try to find the set of corresponding
target frontier trees (i.e., Tt). For each counter-
part v′ of v (line 5), we call the procedure FIND-
TREES(Ft, v

′) to identify a set of frontier trees
rooted at v′ in Ft (line 6). For example, the source

1: procedure FINDTREEPAIRS(Fs , Ft, v)
2: P = ∅
3: Ts ← FINDTREES(Fs , v)
4: Tt ← ∅
5: for v′ ∈ τ(v) do
6: Tt ← Tt∪ FINDTREES(Ft , v

′)
7: end for
8: for 〈ts, tt〉 ∈ Ts × Tt do
9: if ts ∼ tt then

10: P ← P ∪ {〈ts, tt,∼〉}
11: end if
12: end for
13: for 〈ts, tt,∼〉 ∈ P do
14: if ∃〈ts′, tt′,∼′〉 ∈ P : 〈ts′, tt′,∼′〉 ⊆
〈ts, tt,∼〉 then

15: P ← P − {〈ts, tt,∼〉}
16: end if
17: end for
18: end procedure

Figure 3: Algorithm for identifying minimal fron-
tier tree pairs.

frontier node “PP4” has two counterparts on the
target side: “NP25” and “PP26”. There are four
target frontier trees rooted at the two nodes:

(NP25(IN20)(NP24))

(NP25(IN20)(NP24(NNP21)))

(PP26(IN20)(NP24))

(PP26(IN20)(NP24(NNP21)))

Therefore, there are 2 × 4 = 8 pairs of trees.
We examine each tree pair 〈ts, tt〉 (line 8) to see
whether it is a frontier tree pair (line 9) and then
update P (line 10). In the above example, all the
eight tree pairs are frontier tree pairs.

Finally, we keep only minimal frontier tree pairs
in P (lines 13-15). As a result, we obtain the
following two minimal frontier tree pairs for the
source frontier node “PP4”:

(PP4(P11)(NP-B7))↔ (NP25(IN20)(NP24))

(PP4(P11)(NP-B7))↔ (PP26(IN20)(NP24))

To maintain a reasonable rule table size, we re-
strict that the number of nodes in a tree of an STSG
rule is no greater than n, which we refer to as max-
imal node count.

It seems more efficient to let the procedure
FINDTREES(F, v) to search for minimal frontier

562



trees rather than frontier trees. However, a min-
imal frontier tree pair is not necessarily a pair of
minimal frontier trees. On our Chinese-English
corpus, we find that 38% of minimal frontier tree
pairs are not pairs of minimal frontier trees. As a
result, we have to first collect all frontier tree pairs
and then decide on the minimal ones.

Table 1 shows some minimal rules extracted
from the forest pair shown in Figure 1.

3.3 Inferring Composed Rules

After minimal rules are learned, composed rules
can be obtained by composing two or more min-
imal rules. For example, the composition of the
second rule and the third rule in Table 1 produces
a new rule:

NP-B(NR(shalong))→ NP(NNP(Sharon))

While minimal rules derive from minimal fron-
tier tree pairs, composed rules correspond to non-
minimal frontier tree pairs.

3.4 Estimating Rule Probabilities

We follow Mi and Huang (2008) to estimate the
fractional count of a rule extracted from an aligned
forest pair. Intuitively, the relative frequency of a
subtree that occurs in a forest is the sum of all the
trees that traverse the subtree divided by the sum
of all trees in the forest. Instead of enumerating
all trees explicitly and computing the sum of tree
probabilities, we resort to inside and outside prob-
abilities for efficient calculation:

c(r) =
p(ts)× α(root(ts))×

∏

v∈leaves(ts)
β(v)

β(v̄s)

×
p(tt)× α(root(tt))×

∏

v∈leaves(tt)
β(v)

β(v̄t)

where c(r) is the fractional count of a rule, ts is the
source tree in r, tt is the target tree in r, root(·) a
function that gets tree root, leaves(·) is a function
that gets tree leaves, and α(v) and β(v) are outside
and inside probabilities, respectively.

4 Decoding

Given a source packed forest Fs, our decoder finds
the target yield of the single best derivation d that
has source yield of Ts(d) ∈ Fs:

ê = e

(

argmax
d s.t. Ts(d)∈Fs

p(d)

)

(2)

We extend the model in Eq. 1 to a log-linear
model (Och and Ney, 2002) that uses the follow-
ing eight features: relative frequencies in two di-
rections, lexical weights in two directions, num-
ber of rules used, language model score, number
of target words produced, and the probability of
matched source tree (Mi et al., 2008).

Given a source parse forest and an STSG gram-
mar G, we first apply the conversion algorithm
proposed by Mi et al. (2008) to produce a trans-
lation forest. The translation forest has a simi-
lar hypergraph structure. While the nodes are the
same as those of the parse forest, each hyperedge
is associated with an STSG rule. Then, the de-
coder runs on the translation forest. We use the
cube pruning method (Chiang, 2007) to approxi-
mately intersect the translation forest with the lan-
guage model. Traversing the translation forest in
a bottom-up order, the decoder tries to build tar-
get parses at each node. After the first pass, we
use lazy Algorithm 3 (Huang and Chiang, 2005)
to generate k-best translations for minimum error
rate training.

5 Experiments

5.1 Data Preparation

We evaluated our model on Chinese-to-English
translation. The training corpus contains 840K
Chinese words and 950K English words. A tri-
gram language model was trained on the English
sentences of the training corpus. We used the 2002
NIST MT Evaluation test set as our development
set, and used the 2005 NIST MT Evaluation test
set as our test set. We evaluated the translation
quality using the BLEU metric, as calculated by
mteval-v11b.pl with its default setting except that
we used case-insensitive matching of n-grams.

To obtain packed forests, we used the Chinese
parser (Xiong et al., 2005) modified by Haitao
Mi and the English parser (Charniak and Johnson,
2005) modified by Liang Huang to produce en-
tire parse forests. Then, we ran the Python scripts
(Huang, 2008) provided by Liang Huang to out-
put packed forests. To prune the packed forests,
Huang (2008) uses inside and outside probabili-
ties to compute the distance of the best derivation
that traverses a hyperedge away from the glob-
ally best derivation. A hyperedge will be pruned
away if the difference is greater than a threshold
p. Nodes with all incoming hyperedges pruned
are also pruned. The greater the threshold p is,

563



p avg trees # of rules BLEU
0 1 73, 614 0.2021 ± 0.0089
2 238.94 105, 214 0.2165 ± 0.0081
5 5.78 × 106 347, 526 0.2336 ± 0.0078
8 6.59 × 107 573, 738 0.2373 ± 0.0082
10 1.05 × 108 743, 211 0.2385 ± 0.0084

Table 3: Comparison of BLEU scores for tree-
based and forest-based tree-to-tree models.

0.04

0.05

0.06

0.07

0.08

0.09

0.10

 0  1  2  3  4  5  6  7  8  9  10  11

c
o

v
e
ra

g
e

maximal node count

p=0
p=2
p=5
p=8

p=10

Figure 4: Coverage of lexicalized STSG rules on
bilingual phrases.

the more parses are encoded in a packed forest.
We obtained word alignments of the training

data by first running GIZA++ (Och and Ney, 2003)
and then applying the refinement rule “grow-diag-
final-and” (Koehn et al., 2003).

5.2 Forests Vs. 1-best Trees

Table 3 shows the BLEU scores of tree-based and
forest-based tree-to-tree models achieved on the
test set over different pruning thresholds. p is the
threshold for pruning packed forests, “avg trees”
is the average number of trees encoded in one for-
est on the test set, and “# of rules” is the number
of STSG rules used on the test set. We restrict that
both source and target trees in a tree-to-tree rule
can contain at most 10 nodes (i.e., the maximal
node count n = 10). The 95% confidence inter-
vals were computed using Zhang ’s significance
tester (Zhang et al., 2004).

We chose five different pruning thresholds in
our experiments: p = 0, 2, 5, 8, 10. The forests
pruned by p = 0 contained only 1-best tree per
sentence. With the increase of p, the average num-
ber of trees encoded in one forest rose dramati-
cally. When p was set to 10, there were over 100M
parses encoded in one forest on average.

p extraction decoding
0 1.26 6.76
2 2.35 8.52
5 6.34 14.87
8 8.51 19.78
10 10.21 25.81

Table 4: Comparison of rule extraction time (sec-
onds/1000 sentence pairs) and decoding time (sec-
ond/sentence)

Moreover, the more trees are encoded in packed
forests, the more rules are made available to
forest-based models. The number of rules when
p = 10 was almost 10 times of p = 0. With the
increase of the number of rules used, the BLEU
score increased accordingly. This suggests that
packed forests enable tree-to-tree model to learn
more useful rules on the training data. However,
when a pack forest encodes over 1M parses per
sentence, the improvements are less significant,
which echoes the results in (Mi et al., 2008).

The forest-based tree-to-tree model outper-
forms the original model that uses 1-best trees
dramatically. The absolute improvement of 3.6
BLEU points (from 0.2021 to 0.2385) is statis-
tically significant at p < 0.01 using the sign-
test as described by Collins et al. (2005), with
700(+1), 360(-1), and 15(0). We also ran Moses
(Koehn et al., 2007) with its default setting us-
ing the same data and obtained a BLEU score of
0.2366, slightly lower than our best result (i.e.,
0.2385). But this difference is not statistically sig-
nificant.

5.3 Effect on Rule Coverage

Figure 4 demonstrates the effect of pruning thresh-
old and maximal node count on rule coverage.
We extracted phrase pairs from the training data
to investigate how many phrase pairs can be cap-
tured by lexicalized tree-to-tree rules that con-
tain only terminals. We set the maximal length
of phrase pairs to 10. For tree-based tree-to-tree
model, the coverage was below 8% even the max-
imal node count was set to 10. This suggests that
conventional tree-to-tree models lose over 92%
linguistically unmotivated mappings due to hard
syntactic constraints. The absence of such non-
syntactic mappings prevents tree-based tree-to-
tree models from achieving comparable results to
phrase-based models. With more parses included

564



0.09

0.10

0.11

0.12

0.13

0.14

0.15

0.16

0.17

0.18

0.19

0.20

 0  1  2  3  4  5  6  7  8  9  10  11

B
L

E
U

maximal node count

Figure 5: Effect of maximal node count on BLEU
scores.

in packed forests, the rule coverage increased ac-
cordingly. When p = 10 and n = 10, the cov-
erage was 9.7%, higher than that of p = 0. As
a result, packed forests enable tree-to-tree models
to capture more useful source-target mappings and
therefore improve translation quality. 2

5.4 Training and Decoding Time

Table 4 gives the rule extraction time (sec-
onds/1000 sentence pairs) and decoding time (sec-
ond/sentence) with varying pruning thresholds.
We found that the extraction time grew faster than
decoding time with the increase of p. One possi-
ble reason is that the number of frontier tree pairs
(see Figure 3) rose dramatically when more parses
were included in packed forests.

5.5 Effect of Maximal Node Count

Figure 5 shows the effect of maximal node count
on BLEU scores. With the increase of maximal
node count, the BLEU score increased dramati-
cally. This implies that allowing tree-to-tree rules
to capture larger contexts will strengthen the ex-
pressive power of tree-to-tree model.

5.6 Results on Larger Data

We also conducted an experiment on larger data
to further examine the effectiveness of our ap-
proach. We concatenated the small corpus we
used above and the FBIS corpus. After remov-
ing the sentences that we failed to obtain forests,

2Note that even we used packed forests, the rule coverage
was still very low. One reason is that we set the maximal
phrase length to 10 words, while an STSG rule with 10 nodes
in each tree usually cannot subsume 10 words.

the new training corpus contained about 260K sen-
tence pairs with 7.39M Chinese words and 9.41M
English words. We set the forest pruning threshold
p = 5. Moses obtained a BLEU score of 0.3043
and our forest-based tree-to-tree system achieved
a BLEU score of 0.3059. The difference is still not
significant statistically.

6 Related Work

In machine translation, the concept of packed for-
est is first used by Huang and Chiang (2007) to
characterize the search space of decoding with lan-
guage models. The first direct use of packed for-
est is proposed by Mi et al. (2008). They replace
1-best trees with packed forests both in training
and decoding and show superior translation qual-
ity over the state-of-the-art hierarchical phrase-
based system. We follow the same direction and
apply packed forests to tree-to-tree translation.

Zhang et al. (2008) present a tree-to-tree model
that uses STSG. To capture non-syntactic phrases,
they apply tree-sequence rules (Liu et al., 2007)
to tree-to-tree models. Their extraction algorithm
first identifies initial rules and then obtains abstract
rules. While this method works for 1-best tree
pairs, it cannot be applied to packed forest pairs
because it is impractical to enumerate all tree pairs
over a phrase pair.

While Galley (2004) describes extracting tree-
to-string rules from 1-best trees, Mi and Huang et
al. (2008) go further by proposing a method for
extracting tree-to-string rules from aligned forest-
string pairs. We follow their work and focus on
identifying tree-tree pairs in a forest pair, which is
more difficult than the tree-to-string case.

7 Conclusion

We have shown how to improve tree-to-tree trans-
lation with packed forests, which compactly en-
code exponentially many parses. To learn STSG
rules from aligned forest pairs, we first identify
minimal rules and then get composed rules. The
decoder finds the best derivation that have the
source yield of one source tree in the forest. Ex-
periments show that using packed forests in tree-
to-tree translation results in dramatic improve-
ments over using 1-best trees. Our system also
achieves comparable performance with the state-
of-the-art phrase-based system Moses.

565



Acknowledgement

The authors were supported by National Natural
Science Foundation of China, Contracts 60603095
and 60736014, and 863 State Key Project No.
2006AA010108. Part of this work was done
while Yang Liu was visiting the SMT group led
by Stephan Vogel at CMU. We thank the anony-
mous reviewers for their insightful comments.
Many thanks go to Liang Huang, Haitao Mi, and
Hao Xiong for their invaluable help in producing
packed forests. We are also grateful to Andreas
Zollmann, Vamshi Ambati, and Kevin Gimpel for
their helpful feedback.

References

Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proc. of ACL 2005.

David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2).

Brooke Cowan, Ivona Kuc̆erová, and Michael Collins.
2006. A discriminative model for tree-to-tree trans-
lation. In Proc. of EMNLP 2006.

Steve DeNeefe, Kevin Knight, Wei Wang, and Daniel
Marcu. 2007. What can syntax-based MT learn
from phrase-based MT? In Proc. of EMNLP 2007.

Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency
insertion grammars. In Proc. of ACL 2005.

Jason Eisner. 2003. Learning non-isomorphic tree
mappings for machine translation. In Proc. of ACL
2003 (Companion Volume).

Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What’s in a translation rule?
In Proc. of NAACL/HLT 2004.

Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc.
of COLING/ACL 2006.

Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proc. of IWPT 2005.

Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In Proc. of ACL 2007.

Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proc. of AMTA 2006.

Liang Huang. 2008. Forest reranking: Discrimina-
tive parsing with non-local features. In Proc. of
ACL/HLT 2008.

Phillip Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proc. of
NAACL 2003.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proc. of ACL 2007 (demonstration session).

Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proc. of COLING/ACL 2006.

Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin.
2007. Forest-to-string statistical translation rules. In
Proc. of ACL 2007.

Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. Spmt: Statistical machine
translation with syntactified target language phrases.
In Proc. of EMNLP 2006.

Haitao Mi and Liang Huang. 2008. Forest-based trans-
lation rule extraction. In Proc. of EMNLP 2008.

Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proc. of ACL/HLT 2008.

Franz J. Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
machine translation. In Proc. of ACL 2002.

Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1).

Chris Quirk and Simon Corston-Oliver. 2006. The
impact of parsing quality on syntactically-informed
statistical machine translation. In Proc. of EMNLP
2006.

Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proc. of ACL/HLT 2008.

Deyi Xiong, Shuanglong Li, Qun Liu, and Shouxun
Lin. 2005. Parsing the penn chinese treebank with
semantic knowledge. In Proc. of IJCNLP 2005.

Ying Zhang, Stephan Vogel, and Alex Waibel. 2004.
Interpreting bleu/nist scores how much improve-
ment do we need to have a better system? In Proc.
of LREC 2004.

Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,
Chew Lim Tan, and Sheng Li. 2008. A tree
sequence alignment-based tree-to-tree translation
model. In Proc. of ACL/HLT 2008.

566


